{"chunk_id": "NVDA_TRANSCRIPT_2025_Q4_chunk_0000", "doc_id": "NVDA_TRANSCRIPT_2025_Q4", "source_units": ["NVDA_TRANSCRIPT_2025_Q4_u_0003", "NVDA_TRANSCRIPT_2025_Q4_u_0004"], "source_uri": "data/earnings_calls_manual/NVDA/NVDA_FY2025_Q4.pdf", "ticker": "NVDA", "company": "NVDA", "doc_type": "earnings_transcript", "fiscal_year": 2025, "quarter": "Q4", "period": "2025-Q4", "section_id": null, "section_title": null, "speaker": "Morgan Stanley Technology", "speaker_role": "Analyst", "phase": "qa", "chunk_tokens": 800, "start_token": 0, "end_token": 800, "chunk_type": "windowed_overlap", "overlap_with_prev": 0, "overlap_with_next": 120, "text": "March 17th, in San Jose, California. Jensen will deliver a news-packed keynote on March 18th, and we will host a Q&A session for our financial analysts. Next\nday, March 19th. We look forward to seeing you at these events. Our earnings call to discuss the results for our first quarter of fiscal 2026 is scheduled for May\n28th, 2025. We are going to open up the call, operator. To questions. If you could start that, that would be great.\nChrista\nThank you. At this time, I would like I also ask that you please limit yourself to one question. For any additional questions, please requeue. And your first question\ncomes from C.J. Muse with Cantor Fitzgerald. Please go ahead.\nC.J. Muse\nYeah. Good afternoon. Thank you for taking the question. I guess, for me, Jensen, as test time compute and reinforcement learning shows such promise, we're\nclearly seeing increasing blurring in the lines between training and inference. What does this mean for the potential future of potentially inference-dedicated\nclusters? And how do you think about the overall impact to NVIDIA Corporation and your customers? Thank you. Yeah. I appreciate that, C.J. There are now multiple scaling laws. There's the pretrained scaling laws. And that's gonna continue to scale because we have \nmultimodality. We have data that came from reasoning that are now used to pretraining. And then the second is post-training scaling law. Using reinforcement \nlearning human feedback, reinforcement learning AI feedback, reinforcement learning verifiable rewards, the amount of computation you use for post-training is \nactually higher than pretraining. And it's kinda sensible in the sense that you could while you're using reinforcement learning, generate an enormous amount of \nsynthetic data or synthetically generated tokens. AI models are basically generating tokens to train AI models. That's post-train. And the third part, this is the part\n\nthat you mentioned, is test time compute or reasoning. Long thinking, inference scaling, basically the same ideas. And there's you have chain of thought, you\nhave search. The amount of tokens generated, the amount of inference compute needed, is already a hundred times more than the one-shot examples and the\none-shot capabilities of large language models in the beginning and that's just the beginning. This is just the beginning. The idea that the next generation could\nhave thousands of times and even hopefully extremely thoughtful and simulation-based and search-based models that could be hundreds of thousands, millions\nof times more compute than today, is in our future. And so the question is how do you design such an architecture? Some of the models are autoregressive.\nSome of the models are diffusion-based. Some of the times you want your data center to have disaggregated inference. Sometimes it's compacted. And so it's\nhard to figure out what is the best configuration of a data center, which is the reason why NVIDIA Corporation's architecture is so popular. We run every model.\nWe are great at training. The vast majority of our compute today is actually inference, and Blackwell takes all of that to a new level. We designed Blackwell with\nthe idea of reasoning models in mind. And you look at training, it's many times more performant. But what's really amazing is for long-thinking, test time scaling\nreasoning AI models, we're tens of times faster, 25 times higher throughput. And so Blackwell is gonna be incredible across the board. And when you have a data\ncenter, that allows you to configure and use your data center based on are you doing more pretraining now, post-training now? Or scaling out your inference our\narchitecture is fungible, and easy to use. In all of those different ways. And so we're seeing, in fact, much, much", "chunked_at": "2025-11-10T02:18:28Z"}
{"chunk_id": "NVDA_TRANSCRIPT_2025_Q4_chunk_0001", "doc_id": "NVDA_TRANSCRIPT_2025_Q4", "source_units": ["NVDA_TRANSCRIPT_2025_Q4_u_0003", "NVDA_TRANSCRIPT_2025_Q4_u_0004"], "source_uri": "data/earnings_calls_manual/NVDA/NVDA_FY2025_Q4.pdf", "ticker": "NVDA", "company": "NVDA", "doc_type": "earnings_transcript", "fiscal_year": 2025, "quarter": "Q4", "period": "2025-Q4", "section_id": null, "section_title": null, "speaker": "Morgan Stanley Technology", "speaker_role": "Analyst", "phase": "qa", "chunk_tokens": 153, "start_token": 680, "end_token": 833, "chunk_type": "windowed_overlap", "overlap_with_prev": 120, "overlap_with_next": 0, "text": "'s many times more performant. But what's really amazing is for long-thinking, test time scaling\nreasoning AI models, we're tens of times faster, 25 times higher throughput. And so Blackwell is gonna be incredible across the board. And when you have a data\ncenter, that allows you to configure and use your data center based on are you doing more pretraining now, post-training now? Or scaling out your inference our\narchitecture is fungible, and easy to use. In all of those different ways. And so we're seeing, in fact, much, much more concentration of a unified architecture than\never before.\nChrista\nYour next question comes from the line of Joseph Moore with JPMorgan. Please go ahead.", "chunked_at": "2025-11-10T02:18:28Z"}
{"chunk_id": "NVDA_TRANSCRIPT_2025_Q4_chunk_0002", "doc_id": "NVDA_TRANSCRIPT_2025_Q4", "source_units": ["NVDA_TRANSCRIPT_2025_Q4_u_0005", "NVDA_TRANSCRIPT_2025_Q4_u_0006"], "source_uri": "data/earnings_calls_manual/NVDA/NVDA_FY2025_Q4.pdf", "ticker": "NVDA", "company": "NVDA", "doc_type": "earnings_transcript", "fiscal_year": 2025, "quarter": "Q4", "period": "2025-Q4", "section_id": null, "section_title": null, "speaker": "Joseph Moore", "speaker_role": "Analyst", "phase": "qa", "chunk_tokens": 363, "start_token": 0, "end_token": 0, "chunk_type": "qa_exchange", "overlap_with_prev": 0, "overlap_with_next": 0, "text": "I wonder if you could talk about GV200 at CES. You sort of talked about the complexity of the rack-level systems and the challenges you have. And then as you\nsaid in the prepared remarks, we've seen a lot of general availability. You know, where are you in terms of that ramp? Are there still bottlenecks to consider at a\nsystems level above and beyond the chip level? And just you know, have you maintained your enthusiasm for the NVLink 72 platforms? Well, I'm more enthusiastic today than I was at CES. And the reason for that is because we shipped a lot more to CES. We have some 350 plants manufacturing\nthe one and a half million components that go into each one of the Blackwell racks. Base Blackwell racks. Yes. It's extremely complicated. And we successfully\nand incredibly ramped up Grace Blackwell. Delivering some $11 billion of revenues last quarter. We're gonna have to continue to scale as demand is quite high\nand customers are anxious and impatient to get their Blackwell systems. You'd probably seen on the web a fair number of celebrations about Grace Blackwell\nSystems coming online and we have them, of course. We have a fairly large installation of Grace Blackwell for our own engineering and our own design teams\nand software teams. Coreweave has now gone public about the successful bring-up of theirs. Microsoft has. Of course, OpenAI has. And you're starting to see\nmany come online. So I think the answer to your question is nothing is easy about what we're doing. But we're doing great, and all of our partners are doing great.\nChrista\nYour next question comes from the line of Vivek Arya with Bank of America Securities. Please go ahead.", "chunked_at": "2025-11-10T02:18:28Z"}
{"chunk_id": "NVDA_TRANSCRIPT_2025_Q4_chunk_0003", "doc_id": "NVDA_TRANSCRIPT_2025_Q4", "source_units": ["NVDA_TRANSCRIPT_2025_Q4_u_0007", "NVDA_TRANSCRIPT_2025_Q4_u_0008"], "source_uri": "data/earnings_calls_manual/NVDA/NVDA_FY2025_Q4.pdf", "ticker": "NVDA", "company": "NVDA", "doc_type": "earnings_transcript", "fiscal_year": 2025, "quarter": "Q4", "period": "2025-Q4", "section_id": null, "section_title": null, "speaker": "Vivek Arya", "speaker_role": "Analyst", "phase": "qa", "chunk_tokens": 274, "start_token": 0, "end_token": 0, "chunk_type": "qa_exchange", "overlap_with_prev": 0, "overlap_with_next": 0, "text": "Thank you for taking my question. Could I just you wouldn't mind confirming if Q1 is the bottom for gross margins? And then, Jensen, my question is for you.\nWhat is on your dashboard to give you the confidence that the strong demand can sustain into next year and has DeepSeq and whatever innovations they came\nup with, has that changed that view in any way? Thank you. Let me first take the first part of the question. Regarding the gross margin. During our Blackwell ramp, our gross margins will be in the low seventies. At this point,\nwe are focusing on expediting our manufacturing. Expediting our manufacturing is to make sure that we can provide customers as soon as possible. Our\nBlackwell is fully ramped. And once it does, I'm sorry. Blackwell fully ramps, we can improve our cost and our gross margin. So we expect to probably be in the\nmid-seventies later this year. You know, walking through what you heard, Jensen speak about the systems and their complexity. They are customizable in some\ncases. They've got multiple networking options. Have liquid cool and water-cooled. So we know there is an opportunity for us to improve these gross margins\ngoing forward. But right now, we are gonna focus on getting the manufacturing plate into our customers as soon as possible.", "chunked_at": "2025-11-10T02:18:28Z"}
{"chunk_id": "NVDA_TRANSCRIPT_2025_Q4_chunk_0004", "doc_id": "NVDA_TRANSCRIPT_2025_Q4", "source_units": ["NVDA_TRANSCRIPT_2025_Q4_u_0010", "NVDA_TRANSCRIPT_2025_Q4_u_0011"], "source_uri": "data/earnings_calls_manual/NVDA/NVDA_FY2025_Q4.pdf", "ticker": "NVDA", "company": "NVDA", "doc_type": "earnings_transcript", "fiscal_year": 2025, "quarter": "Q4", "period": "2025-Q4", "section_id": null, "section_title": null, "speaker": "Matt Ramsay", "speaker_role": "Analyst", "phase": "qa", "chunk_tokens": 287, "start_token": 0, "end_token": 0, "chunk_type": "qa_exchange", "overlap_with_prev": 0, "overlap_with_next": 0, "text": "Yeah. Good afternoon. Thanks for taking my question. Your next generation Blackwell Ultra is set to launch in the second half of this year. In line with the team's\nannual product cadence. Jensen, can you help us understand the demand dynamics for Ultra given that you'll still be ramping the current generation Blackwell\nsolutions? How do your customers and the supply chain also manage the simultaneous ramps of these two products and is the team still on track to execute\nBlackwell Ultra in the second half of this year? Yes. Blackwell Ultra is second half. As you know, the first Blackwell was have we had a hiccup? That probably cost us a couple of months. We're fully recovered,\nof course. The team did an amazing job recovery. And all of our supply chain partners and just so many people helped us recover at the speed of light. And so\nnow we've successfully ramped production of Blackwell. But that doesn't stop the next train. The next train is you know, it's on an annual rhythm. And, Blackwell\nUltra with, new networking, new memories, and, of course, new processors and all of that is coming online. We've been working with all of our partners and\ncustomers laying this out. They have all of the necessary information. And we'll work with everybody to do the proper transition. This time between Blackwell,", "chunked_at": "2025-11-10T02:18:28Z"}
{"chunk_id": "NVDA_TRANSCRIPT_2025_Q4_chunk_0005", "doc_id": "NVDA_TRANSCRIPT_2025_Q4", "source_units": ["NVDA_TRANSCRIPT_2025_Q4_u_0013", "NVDA_TRANSCRIPT_2025_Q4_u_0014"], "source_uri": "data/earnings_calls_manual/NVDA/NVDA_FY2025_Q4.pdf", "ticker": "NVDA", "company": "NVDA", "doc_type": "earnings_transcript", "fiscal_year": 2025, "quarter": "Q4", "period": "2025-Q4", "section_id": null, "section_title": null, "speaker": "Timothy Arcuri", "speaker_role": "Analyst", "phase": "qa", "chunk_tokens": 800, "start_token": 0, "end_token": 800, "chunk_type": "windowed_overlap", "overlap_with_prev": 0, "overlap_with_next": 120, "text": "Thanks a lot. Jensen, we hear a lot about custom ASICs. Can you kinda speak to the balance between custom ASIC and merchant GPU? We hear about some\nof these heterogeneous super clusters to use both GPU and ASIC. Is that something customers are planning on building or will these infrastructures remain fairly\ndistinct? Thanks. Well, we build very different things than ASICs. In some ways, completely different in some areas we intercept. We're different in several ways. One, NVIDIA\nCorporation's architecture is general. You know, whether you've optimized for autoregressive models or diffusion-based models or vision-based models or\nmultimodal models or text models. We're great in all of it. We're great in all of it because our software stack is so our architecture is responsible. Our software\nstack is ecosystem is so rich that we're the initial target of, you know, most exciting innovations and algorithms. And so by definition, we're much, much more\ngeneral than narrow. We're also really good from the end to end. From data processing, the curation of the training data, to the training of the data, of course, to\nreinforcement learning used in post-training. All the way to inference with test time scaling. So, you know, we're general. We're end to end. And we're\neverywhere. And because we're not in just one cloud, we're in every cloud, we could be on-prem. We could be in, you know, in a robot. Our architecture is much\nmore accessible. And a great target initial target for anybody who's starting up a new company. And so we're everywhere. And then the third thing I would say is\nthat our performance and our rhythm is so incredibly fast. Remember that these data centers are always fixed in size. They're fixed in size or they're fixed in\npower. And if our performance per watt is anywhere from 2x to 4x to 8x, which is not unusual. It translates directly to revenues. And so if you have a\n100-megawatt data center, if the performance or the throughput that 100-megawatt or that gigawatt data center is four times or eight times higher your revenues\nfor that gigawatt data center is eight times higher. And the reason that is so different than data centers of the past is because AI factories are directly monetizable\nthrough its tokens generated. And so the token throughput of our architecture being so incredibly fast is just incredibly valuable to all of the companies that are\nbuilding these things for revenue generation reasons. And capturing the fast ROIs. So I think the third reason is performance. And then the last thing that I would\nsay is the software stack is incredibly hard. Building an ASIC is no different than what we do. We have to build a new architecture. And the ecosystem that sits on\ntop of our architecture is ten times more complex today than it was two years ago. And that's fairly obvious because the amount of software this world building on\ntop of architecture is growing exponentially and AI is advancing very quickly. So bringing that whole ecosystem on top of multiple chips is hard. And so I would\nsay that those four reasons and then finally, I will say this. Just because the chip is designed doesn't mean it gets deployed. And you've seen this over and over\nagain. There are a lot of chips that get built. But when the time comes a business decision has to be made. And that business decision is about deploying a new\nengine, a new processor into a limited AI factory in size and power and find. And our technology is, you know, not only more advanced, more performant, it has\nmuch, much better software capability, and very importantly, our ability to deploy is lightning fast. And so these things are enough for the faint of", "chunked_at": "2025-11-10T02:18:28Z"}
{"chunk_id": "NVDA_TRANSCRIPT_2025_Q4_chunk_0006", "doc_id": "NVDA_TRANSCRIPT_2025_Q4", "source_units": ["NVDA_TRANSCRIPT_2025_Q4_u_0013", "NVDA_TRANSCRIPT_2025_Q4_u_0014"], "source_uri": "data/earnings_calls_manual/NVDA/NVDA_FY2025_Q4.pdf", "ticker": "NVDA", "company": "NVDA", "doc_type": "earnings_transcript", "fiscal_year": 2025, "quarter": "Q4", "period": "2025-Q4", "section_id": null, "section_title": null, "speaker": "Timothy Arcuri", "speaker_role": "Analyst", "phase": "qa", "chunk_tokens": 170, "start_token": 680, "end_token": 850, "chunk_type": "windowed_overlap", "overlap_with_prev": 120, "overlap_with_next": 0, "text": " chip is designed doesn't mean it gets deployed. And you've seen this over and over\nagain. There are a lot of chips that get built. But when the time comes a business decision has to be made. And that business decision is about deploying a new\nengine, a new processor into a limited AI factory in size and power and find. And our technology is, you know, not only more advanced, more performant, it has\nmuch, much better software capability, and very importantly, our ability to deploy is lightning fast. And so these things are enough for the faint of heart as\neverybody knows now. And so there's a lot of different reasons why we do well. Why we win.\nChrista\nYour next question comes from the line of Ben Reitzes with Melius Research. Please go ahead.", "chunked_at": "2025-11-10T02:18:28Z"}
{"chunk_id": "NVDA_TRANSCRIPT_2025_Q4_chunk_0007", "doc_id": "NVDA_TRANSCRIPT_2025_Q4", "source_units": ["NVDA_TRANSCRIPT_2025_Q4_u_0020", "NVDA_TRANSCRIPT_2025_Q4_u_0021"], "source_uri": "data/earnings_calls_manual/NVDA/NVDA_FY2025_Q4.pdf", "ticker": "NVDA", "company": "NVDA", "doc_type": "earnings_transcript", "fiscal_year": 2025, "quarter": "Q4", "period": "2025-Q4", "section_id": null, "section_title": null, "speaker": "Aaron Rakers", "speaker_role": "Analyst", "phase": "qa", "chunk_tokens": 471, "start_token": 0, "end_token": 0, "chunk_type": "qa_exchange", "overlap_with_prev": 0, "overlap_with_next": 0, "text": "Yeah. Thanks for letting me back in. Jensen, I'm curious as we now approach the two-year anniversary of really the Hopper inflection that you saw in 2023 in Gen \nAI in general. We think about the roadmap you have in front of us, how do you think about the infrastructure that's been deployed from a replacement cycle\n\nperspective and whether, you know, if it's GV300 or if it's the Rubin cycle where we start to see maybe some refresh opportunity. I'm just curious to how you look\nat that. Yeah. I appreciate it. First of all, people are still using Voltas. And Pascals, and Amperes. And the reason for that is because they're always things that because\nCUDA is so programmable, you could use it right well, one of the major use cases right now is data processing and data curation. You find a circumstance that an\nAI model is not very good at? You present that circumstance to a vision language model, let's say. Let's say it's a car? You present that circumstance to a vision\nlanguage model, the vision language model actually looks at the circumstances. It's a this isn't this is what happened, and I wasn't very good at it. You then take\nthat response, this the prompt, and you go and prompt an AI model to go find in your whole link of data of other circumstances like that. Whatever that\ncircumstance was. And then you use an AI to do domain randomization and generate a whole bunch of other examples. And then from that, you can go train the\nmodel. And so you could use the Amperes to go and do data processing and data curation and machine learning-based search. And then you create the training\ndataset, which you then present to your Hopper systems for training. And so each one of these architectures are completely are you know, they're all CUDA\ncompatible, and so everything runs on everything. But if you have infrastructure in place, and you can put the less intensive workloads onto the installed base of\nthe past. All of our CPUs are very well employed.\nChrista\nWe have time for one more question, and that question comes from Atif Malik with Citi. Please go ahead.", "chunked_at": "2025-11-10T02:18:28Z"}

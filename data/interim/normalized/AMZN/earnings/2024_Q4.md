# AMZN Q4 2024 Earnings Call

**Document ID**: AMZN_2024_Q4
**Ticker**: AMZN
**Year**: 2024
**Quarter**: Q4
**Utterances**: 16

---

---

## Q&A Session

### Exchange 001

**Mark Mahaney (Evercore)**: Thanks. Two quick questions. So, Brian, that's $100 billion CapEx we should think about in 2025. And then, Andy, were there anyâ€”so would you describe that
AWS growth as being currently moderated down by supply constraints. Do you see those across the industry, or do you see those materially impacting AWS
today? Thank you very much.

### Exchange 002

**Eric Sheridan (Goldman Sachs)**: Thanks so much for taking the question. I'll just ask one that's building on Mark's questions there. Andy, when you think about the news that came out of China
over the last couple of weeks and think longer term about bending the cost curve lower with AI. I understood the commentary around CapEx for 2025. Would you
look at where you sit in the industry the move towards open-source elements of custom silicon, How do you think about bending the cost curve and either
speeding up or amplifying time deployment to market or possibly, you know, higher returns on capital for AI? Thanks so much.

**Andy Jassy (CEO)**: Well, I'll tell you a few things because there are a few questions built into that. You know, first of all, I think like many others, we were impressed with what
DeepSeq has done. You know, I think in part impressed with some of the training techniques, you know, primarily in flipping the sequencing of reinforcement
training, reinforcement learning being earlier without the human in the loop. We thought that was interesting ahead of the supervised fine-tuning. We also thought
some of the inference optimizations they did were also quite interesting. For those of us who are building frontier models, we're all working on the same types of
things, and we're all learning from one another. I think you have seen and will continue to see a lot of leapfrogging between us. There is a lot of innovation to
come. And, you know, I think if you run a business like AWS, and you have a core belief like we do that virtually all the big generative AI apps are gonna use
multiple model types and different customers are gonna use different models for different types of workloads. You're gonna provide as many leading frontier
models as possible for customers to choose from. That's what we've done with services like Amazon Bedrock, and it's why we moved so quickly to make sure
that DeepSeq was available both in Bedrock and in SageMaker. You know, faster than you saw from others. And we already have customers starting to
experiment with that. I think what's, you know, one of the interesting things over the last couple of weeks is sometimes people make the assumptions that if you're
able to decrease the cost of any type of technology component, in this case, we're really talking about inference, it's somehow it's gonna lead to less total spend
in technology, and we just, we have never seen that to be the case. You know, we did the same thing in the cloud where we launched AWS in 2006, where we
offered S3 object storage for fifteen cents a gigabyte and compute for ten cents an hour, which, of course, is much lower now, many years later. People thought
that people would spend a lot less money in the on infrastructure technology. What happens is companies will spend a lot less per unit of infrastructure, and that
is very, very useful for their businesses, but then they get excited about what else they could build that they always thought was cost prohibitive before. And they
usually end up spending a lot more in total on technology once you make the per-unit cost less. And I think that is very much what's gonna happen here in AI,
which is the cost of inference will substantially come down. You know, what you heard the last couple of weeks out of DeepSeq is a piece of it, but everybody is
working on this. I believe the cost of inference will meaningfully come down. I think it will make it much easier for companies to be able to infuse all their
applications with inference and with generative AI. And I think it's gonna if you run a business like we do, where we wanna make it as easy as possible for
customers be successful building customer experiences on top of our various infrastructure services, the cost of inference coming down is gonna be very positive
for customers and for our business.
Operator
And the next question is from Doug Anmuth with JPMorgan. Please proceed.

### Exchange 003

**Doug Anmuth (JPMorgan)**: Thanks for any questions. I'll stick with AWS. To start. Just, Brian, maybe you can talk a little bit more about margins there just given that they've kind of moved
between the mid-20s to the high thirties over the past two years. How should we think about that more normalized especially as you're investing that much more
in generative AI? And then just on the store side, can you talk about the impact of less volume going through your shipping partner UPS going forward? And are
you able to manage that incremental shipping that's required? Thanks.

**Andy Jassy (CEO)**: Yes. Sure, Doug. Thanks for your question. First on AWS, yeah, we have seen a lot of fluctuation in operating margin AWS, and we've said historically that they
will be lumpy as you say over time. And, you know, the stage we're in right now, AI is still early stage. It does come originally with lower margins and a heavy
investment load, as we've talked about. And in the short term, over time, that should be a headwind on margins. But over the long term, we feel the margins will
be comparable to non AI business as well. So, you know, we're very pleased with the strong growth focus on driving efficiencies in all of our data centers, saving
power, reusing power in new generative AI applications, and just generally reducing costs. So very pleased with the performance of the AWS team, and I look
forward to a strong 2025. I'll take the UPS one, which is, really UPS has been a partner of ours for many years, and we expect that that we'll continue to be
partners with UPS for many years. As you know, increasingly over the last several years, particularly accelerated by the pandemic, we have shipped a much
larger percentage of our shipments through our logistics network, our own last-mile transportation network, and I think that's in part because we needed to scale
up so fast in the pandemic with everything being shut down and needing to serve more of the total market segment share of retail units during that time. And
needing to do it at a low-cost structure because, of course, our customers expect low prices, and that's the nature of the business. I think UPS has decided that
serving Amazon is lower margin for them. So I think they've walked away from some of the volume that they otherwise could have had in the partnership. We're
able to handle it with our own logistics capability, and we'll see how it continues to evolve.
Operator
And the next question comes from the line of Brian Nowak with Morgan Stanley. Please proceed.

### Exchange 004

**Brian Nowak (Morgan Stanley)**: Thanks for taking my questions. Andy, maybe to drill a little bit into the robotics acceleration that you talked about. Any new data points, I think you can share on
learnings from Shreveport? And how do we think about the scalability of savings or the timing that we could see a real impact on profitability from robotics? And
then maybe just a bigger picture Gen AI, GPU-enabled changes, any other examples of how you see the Amazon retail shopping experience changing throughout
2025 as you're better using Gen AI or GPU-enabled?

**Andy Jassy (CEO)**: Yeah. Okay. Well, on the robotics piece, what I would tell you is, you know, since we've been pretty substantially integrating robotics into our fulfillment network
over the last many years, we have seen cost savings, and we've seen productivity improvements and we've seen safety improvements. And so we have already
gotten a significant amount of value out of our robotics innovations. What we've seen recently, and I think maybe part of what you're referencing in Shreveport, is
that the next tranche of robotics initiatives have started hitting production. And we've put them all together for the first time as part of an experience in our
Shreveport facility, and we are very, very encouraged by what we're seeing there both by the speed improvements that we're seeing, the productivity
improvements, the cost to serve improvements. You know, it's still relatively early days, and these all being put together are only in Shreveport at this point. But
we have plans now to start to expand that and roll that out to a number of other facilities in the network, some of which will be our new facilities and others of
which will retrofit existing facilities to be able to use those same robotics innovations. I'll also tell you that this group of, call it, half dozen or so new initiatives is not
close to the end of what we think is possible with respect to being able to use robotics to improve productivity cost to serve a safety in our fulfillment network. And
we have a kind of next wave that we're starting to work on now. But I think this will be a many-year effort as we continue to tune different parts of our fulfillment
network where we can use robotics. And we actually don't think there are that many things that we can't improve the experience with robotics. On your other
question, which is about how we might use AI in other areas of the business than AWS, maybe more in, I think you asked about our retail business. The way I
would think about it is that there's kind of two macro buckets of how we see people, both ourselves inside Amazon, as well as other companies using AWS, how
we see them getting value out of AI today. The first macro bucket, I would say, is really around productivity and cost savings. And, in many ways, this is the
lowest hanging fruit in AI. And you see that all over the place in our retail business. For instance, if you look at customer service, and you look at the chatbot that
we've built, we completely rearchitected it with generative AI. It's delivering; it already had pretty high satisfaction. It's delivering 500 basis points better
satisfaction from customers with the new generative AI-infused chatbot. If you look at our millions of third-party selling partners, one of their biggest pain points is
because we put a high premium on really organizing our marketplace so that it's easy to find things, there's a bunch of different fields you have to fill out when
you're creating a new product detail page. But we've built a generative AI application for them. Where they can either fill in just a couple of lines of text or take a
picture of an image or point to a URL, and the generative AI app will fill in most of the rest of the information they have to fill out, which speeds are getting
selection on the website, easier for sellers. If you look at how we do inventory management, try and understand what inventory we need in what facility at what
time, the generative AI applications we've built there have led to 10% better forecasting on our part, 20% better regional predictions. Our robotics, we were just
talking about the brains in a lot of those robotics, are generative AI-infused that do things like tell the robotic claw, you know, what's in a bin, what it should pick
up, how it should move it, where it should place it in the in the other bin that it's filling. So it's really in the brains of most of our robotics. So we have a number of
very significant, I'll call it, productivity and cost savings efforts in our retail business. They're using generative AI, and again, it's just a fraction of what we have
going. I'd say the other big macro bucket are really altogether new experiences. And, again, you see lots of those in our retail business ranging from Rufus, which
is our AI-infused shopping assistant, which continues to grow very significantly. To things like Amazon Lens, where you can take a picture of a product that's in
front of you, check it out in the app. You can find it in the little box at the top. You take a picture of an item in front of you, and it uses computer vision generative
AI to pull up the exact item in a search result. To things like sizing where we basically have taken the catalogs of all these different clothing manufacturers and
compared them against one another so we know which brands tend to run big or small relative to each other. So when you come to buy a pair of shoes, for
instance, they can recommend what size you need, even what we're doing in Thursday Night Football where we're using generative AI for really inventive
features like the sense of alerts where we predict which player is gonna put the quarterback or vulnerabilities where we're able to show viewers what area the
field is vulnerable. So we're using it really all over our retail business and all the businesses in which we're in. We've got about a thousand different generative AI
applications we've either built or in the process of building right now.
Operator
And the next question comes from the line of John Blackledge with TD Cowen. Please proceed.

### Exchange 005

**John Blackledge (Cowen)**: Great. Thanks. Could you talk about the current speed of delivery, maybe how much more room to go there, and how is it driving the everyday essentials
business? Then some of relatedly, any further color on inbound network efficiencies you would expect to see this year as you guys try to continue to lower the
cost to serve. Thank you.

**Andy Jassy (CEO)**: Yeah. I would say on speed of delivery, that we measure this very carefully. And we measure both what the conversion rate is of somebody who views a product
detail page with a faster delivery promise versus those that are slower as well as what we see downstream from customers once they've bought with a fast
promise and what they end up buying throughout the year. And we have not yet seen diminishing returns of being able to continue to improve the speed of
delivery. It doesn't mean that there won't be instances in which people are happy to take products later. You know, we have a program where if people want to
pick a day during the week where they want to combine a bunch of their shipments and have it delivered then to be more sustainable and more environmentally
friendly, they can. And we have plenty of customers who choose that, but we time in and time out see that people choose to buy from us more frequently when
we're able to deliver to their homes or wherever they are much more quickly, and it leads to actually using us for more of their everyday purchases when we can
do ever more quickly. And I think that if you look at what we're doing with Prime, you know, the promise there is for a number of items that we'll be able to deliver
items to customers inside an hour. And I think when you're ordering everyday essentials where you need something more quickly, it's a big deal. And you see it,
it's had a big impact on our everyday essentials. It's had a big impact on our pharmacy business where people are able to get items same day now in lots of cities
throughout the US. And they're just using us much more frequently than they had before. The inbound network efficiencies, what I would tell you is, we made a
pretty significant architectural change in our inbound network that we've been working on for the better part of the year that we rolled out just a few months ago.
And it's what we find when we make big architectural changes like this is that you tend to get some low hanging fruit efficient early, but then there's all sorts of
tuning and refinement you have to do once you actually see it working live across the really vast network. And we have all sorts of ways here that where I think it's
early, and I think we're gonna get additional efficiencies throughout the year. But I expect that we'll have opportunities to keep taking our costs to serve down this
year, and that'll be a big part of

Operator
And our final question comes from the line of Michael Morton with MoffettNath.

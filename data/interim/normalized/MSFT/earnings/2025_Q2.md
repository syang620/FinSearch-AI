# MSFT Q2 2025 Earnings Call

**Document ID**: MSFT_2025_Q2
**Ticker**: MSFT
**Year**: 2025
**Quarter**: Q2
**Utterances**: 40

---

---

## Q&A Session

### Exchange 001

**Keith Weiss (Morgan Stanley)**: Hi. Thank you guys for taking the question. And echoing Amy's comments, Brett, congratulations on the new role. It's been a pleasure working with you and best
of luck in that new role. Looking at the quarter, another really solid quarter when it comes to commercial bookings. But again, we were a little disappointed on
Azure coming in at the bottom end of the guidance range. Amy, I was hoping you could dig into perhaps what some of those execution issues were, what the
resolution to those issues were. And do we still feel comfortable in the acceleration into the back half of the year that you were talking about after the June quarter
and after last quarter? Thank you very much.

**Satya Nadella (CEO)**: Yes. I think, Amy, just one thing I'd add, Keith, to your question is, as Amy said, the AI growth rate is actually better than what we expected, and we work through
some of the supply stuff and more importantly, some of the workloads are scaling well. And when you look underneath any of these AI workloads, the other thing
that is good is the ratio even what we would call just regular storage, data services, app services. So underneath a ChatGPT or a Copilot or even the emerging AI
workloads in the enterprise. So that's all good. The enterprise workloads, whether it's SAP or whether it's VMware migrations, what have you, that's also in good
shape. And it's just the scale place where Amy talked about this nuance, right, how do you really tweak the incentives, go-to-market at a time of platform shifts,
you kind of want to make sure you lean into even the new design wins, and you just don't keep doing the stuff that you did in the previous generation. And that's
the art form Amy was referencing to make sure you get the right balance. But let me put it this way. You would rather win the new than just protect the past. And
that's sort of another thing that we definitely will lean into always.

### Exchange 002

**Keith Weiss (Morgan Stanley)**: Excellent.

**Brett Iversen (President)**: Thanks, Keith. Operator, next question please.
Operator
The next question comes from the line of Mark Moerdler with Bernstein Research. Please proceed.

### Exchange 003

**Mark Moerdler (Bernstein)**: Thank you very much for taking my question. Can you give more color on what drove the far larger than expected Microsoft AI revenue. We talked a bit about the 
Azure AI component of it. But can you give more color on that? And our estimates are that the Copilot was much bigger than we had expected and growing much

faster. Any more details on the breakdown of what that Microsoft AI would be great? Thanks.

### Exchange 004

**Mark Moerdler (Bernstein)**: Thank you.

**Brett Iversen (President)**: Thanks, Mark. Operator, next question please.
Operator
The next question comes from the line of Brent Thill with Jefferies. Please proceed.

### Exchange 005

**Brent Thill (Jefferies)**: Thanks. Satya, you mentioned DeepSeek a couple of times in your prepared remarks. I think everyone would love your thoughts on what you're seeing there.
And are we seeing AI scale now at lower cost? Are we reaching a mark where you can see that or do we still have some time to go? Thanks for your thoughts on
this.

**Satya Nadella (CEO)**: Yes. Thanks, Brent. So, yes, in my remarks, I talked about how in some sense, what's happening with AI is no different than what was happening with the regular
compute cycle. It's always about bending the curve and then putting more points up the curve. So there's Moore's Law that's working in hyperdrive. Then on top
of that, there is the AI scaling laws, both the pre-training and the inference time compute that compound and that's all software. You should think of what I said in
my remarks, which we have observed for a while, which is 10x on improvements per cycle just because of all the software optimizations on inference. And so
that's what you see. And then to that, I think DeepSeek has had some real innovations. And that is some of the things that even OpenAI found in o1. And so we
are going to, obviously, now that all gets commoditized, and it's going to get broadly used. And the big beneficiaries of any software cycle like that is the
customers, right. Because at the end of the day, if you think about it, right, what was the big lesson learned from client server to cloud, more people bought
servers except it was called cloud. And so when token prices fall, infants computing prices fall, that means people can consume more, and there'll be more apps
written. And it's interesting to see that when I referenced these models that are pretty powerful, it's unimaginable to think that here we are in sort of beginning of
'25 where on the PC, you can run a model that required pretty massive cloud infrastructure. So that type of optimizations means AI will be much more ubiquitous.
And so therefore, for a hyperscaler like us, a PC platform provider like us, this is all good news as far as I'm concerned.

### Exchange 006

**Brent Thill (Jefferies)**: Thank you.

**Brett Iversen (President)**: Thanks, Brent. Operator, next question please.
Operator
The next question comes from the line of Karl Keirstead with UBS. Please proceed.

### Exchange 007

**Karl Keirstead (UBS)**: Thank you. Maybe this one as well for Satya and it's also away from the numbers. But Satya, I wanted to ask you about the Stargate news and the announced
changes in the OpenAI relationship last week. It seems that most of your investors have interpreted this as Microsoft, for sure, remaining very committed to
OpenAI's success. But electing to take more of a backseat in terms of funding OpenAI's future training CapEx needs. I was hoping you might frame your strategic
decision here around Stargate, and Amy, whether there's any takeaway for investors from that decision in terms of how you're thinking about CapEx needs over
the next several years. Thank you.

**Satya Nadella (CEO)**: Yes. Thanks for the question. So we remain very happy with the partnership with OpenAI. And as you saw, they have committed in a big way to Azure and even 
in the bookings what we recognize is just the first tranche of it. And so you'll see given the ROFR we have more benefits of that even into the future. And 
obviously, their success is our success because even all the other commercial arrangements that we detailed out in the blog that we put out even commensurate 
with that announcement. But to your overall point, the thing that I would say is we are building a pretty fungible fleet, right. We're making sure that there's the right 
balance between training and inference. It's geo distributed. We are working super hard on all the software oppositions, right. I mean just not the software 
optimizations that come because of what DeepSeek has done, but all the work we have done to, for example, reduce the prices of GPT models, over the years in 
partnership with OpenAI. In fact, we did a lot of the work on the inference optimizations on it and that's been key to driving, right. One of the key things to note in 
AI is you just don't launch the frontier model, but if it's too expensive to serve, it's no good, right. It won't generate any demand. So you've got to have that 
optimization, so that inferencing costs are coming down and they can be consumed broadly. And so that's the fleet physics we are managing. And also,

remember, you don't want to buy too much of anything at one time because the Moore's Law every year is going to give you 2x, your optimization is going to give
you 10x. You want to continuously upgrade the fleet, modernize the fleet, age, the fleet, and at the end of the day, have the right ratio of monetization and
demand-driven monetization to what you think of as the training expense. So I feel very good about the investment we are making and it's fungible and it just
allows us to scale more long-term business.

### Exchange 008

**Karl Keirstead (UBS)**: Thank you.

**Brett Iversen (President)**: Thanks, Karl. Operator, next question please.
Operator
And the next question comes from the line of Brad Zelnick with Deutsche Bank. Please proceed.

### Exchange 009

**Brad Zelnick (Deutsche Bank)**: Thank you very much for taking my question. And I'll echo my congrats and gratitude to Brett as well. Satya, as we think about Microsoft's very rich Copilot
portfolio, now having been in market for over a year, with the products and precision only getting better and the cost of inference coming down, how do you think
about the journey from here and perhaps the ability to package and evolve the go-to-market to address the broadest range of customers and customer
requirements out there? Thanks.

**Satya Nadella (CEO)**: Thanks, Brad, for the question. In fact you saw us make two announcements recently. One is on the M365 Copilot side, we now have the Copilot Chat. So this is
now going to be broadly deployed across the entire install base effectively because you can go have this now turned on by IT and everybody can start using
web-grounded chat with all the enterprise controls right away it has Copilot Studio built in. And so that means they can start building agents. So we think of that
plus the full Copilot as a good combination that I think will accelerate quite frankly in terms of just seat usage and agent building and what have you. So that's sort
of one. And the other thing is you also see even on the consumer side, we just yesterday launched o1, the Think Harder feature on Copilot now that's powered by
o1, it's available globally, right. So you can see the benefits of inference optimization and the cost coming down means you can drive more ubiquity of what were
features that ones were sort of premium tier and that's all definitely something that we will do across, right. The same thing is happening in GitHub Copilot, same
thing in Security Copilot. So across the length and breadth of our portfolio, you'll see that.

### Exchange 010

**Brad Zelnick (Deutsche Bank)**: Thank you.

**Brett Iversen (President)**: Thanks, Brad. Operator, next question please.
Operator
The next question comes from the line of Brad Reback with Stifel. Please proceed.

### Exchange 011

**Brad Reback (Stifel)**: That's great. Thank you very much. Satya, if you look out several years, any sense of what percent of inference done on Azure will be done on proprietary models
versus open models. And with that said, does it matter to Microsoft at the end of the day? Thanks.

**Satya Nadella (CEO)**: Yes, it's a good question because at some level, what you're seeing is effectively lots of models that get used in any application, right. When you look underneath 
even a Copilot or a GitHub Copilot or what have you, you already see lots of many different models that you build models, you fine tune models, you distill 
models. Some of them are models that you distill into an open source model. So there's going to be a combination. So we've always maintained that it's always 
good to have frontier models. You want to always build your application with high ambition using the best model that is available and then optimize from there on.

So that's also another side like there's a temporality to it, right. What you start with as a given COGS profile doesn't need to be the end because you continuously
optimize for latency and COGS and putting in different mode. And in fact, all that complexity, by the way, has to be managed by a new app server. So one of the
things that we are investing heavily on is foundry because from an app developer perspective, you kind of want to keep pace with the flurry of models that are
coming in and you want to have an evergreen way for your application to benefit from all that innovation. But not have all the Dev cost or the DevOps cost or what
people talk about AIOps costs. So we are also investing significantly in all the app server for any workload to be able to benefit from all these different models,
open source, close source, different weight classes. And at the same time, from an operations perspective, it's faster, easier for you.

### Exchange 012

**Brad Reback (Stifel)**: Great. Thank you.

**Brett Iversen (President)**: Thanks, Brad. Operator, next question please.
Operator
And the next question comes from the line of Brad Sills with Bank of America. Please proceed.

### Exchange 013

**Brent Bracelin (Piper Sandler)**: Thank you for taking the question here. Good afternoon. I wanted to go back to commercial Bookings. Commercial RPO, I think, increased 39 billion sequentially.
That's the most we've ever seen on a sequential basis, commercial bookings growth, 75% constant currency. That's 2x higher than we've seen in the last decade.
I appreciate there's some volatility with this metric, but it does feel like this quarter there was a bit of a sea change relative to momentum on backlog and
bookings. Can you just talk about maybe the breadth of where that strength came from? Was it broad-based? Was there a couple of large deals? Any color there
would be helpful? Thanks.

### Exchange 014

**Brent Bracelin (Piper Sandler)**: Helpful color. Thank you.

**Brett Iversen (President)**: Thanks, Brad. That wraps up the Q&A portion of today's earnings call. Thank you for joining today and we look forward to speaking with all of you soon.

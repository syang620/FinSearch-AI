# MSFT Q1 2025 Earnings Call

**Document ID**: MSFT_2025_Q1
**Ticker**: MSFT
**Year**: 2025
**Quarter**: Q1
**Utterances**: 28

---

---

## Q&A Session

### Exchange 001

**Keith Weiss (Morgan Stanley)**: Congratulations on a really solid quarter. So, Satya, the expansion of capabilities, the speed of innovation, the magnitude of the opportunities ahead for
generative AI makes this the most exciting period for software I've seen in my 25 years of covering this space. And based upon this call, it seems like you share
that excitement. But in my investor conversation, that excitement also feeds two related questions, and they both have to do with constraints. And the first is like
what are the internal constraints or guardrails that Microsoft has when it comes to investing behind these innovations, particularly in relation to the funding of
future generations of foundational models, where people were talking about price tags, you want to spend the tens of billions or even $100 billion-plus? And then
on the other side of the spectrum, what are the external constraints that Microsoft sees in building out this capacity to meet the demand and capture the
opportunity, particularly constraints in your ability to power all these new data centers being built out and powered in an environmentally sustainable fashion? I'd
love to get the Microsoft perspective on both those questions.

**Satya Nadella (CEO)**: Thank you, Keith, for those questions. I think on the first point, ultimately, when you think about, let's say, capital outlay for training because that's essentially what
you're asking, it is going to be rate limited by your monetization of inference, given generation, right? So just like in the past, we would allocate capital to build out
cloud based on the demand signal we were seeing and then we would then project the demand, and that's what we would build for. So, you can think of training
essentially as that, right, which is you're building the next-generation model so that then you have a more capable model that then drives more inference demand,
right? So ultimately, even with all the scaling laws and what have you, I think you ultimately will normalize to having a pace. In fact, I think the best way to think
about even is given that Moore's Law effectively is working on the sort of silicon and system size, so it's just not compute, right, it's efficiencies in compute, it's
data as well as algorithms. You will want to sort of keep on that curve, which is you really want to refresh your fleet with the Moore's Law every year and then
effectively depreciate it over the period of the life cycle of it. And then the inference demand ultimately will cover how much we invest in training because that's, I
think, at the end of the day, you're all subject to ultimately demand. So, the second piece of the external constraints, we have run into, obviously, lots of external
constraints because this demand all showed up pretty fast, right? I mean, if you think about even the most hit product of this generation all are in our cloud, right,
whether it's ChatGPT, whether it's Copilot, whether it's GitHub Copilot or even DAX Copilot, I mean, pick the top four or five products of this generation, they're all
sort of in and around our ecosystem. And so therefore, we ran into a set of constraints, which are everything because DCs don't get built overnight. So, there is
DCs, there is power. And so that's sort of been the short-term constraint. Even in Q2, for example, some of the demand issues we have or our ability to fulfill
demand is because of, in fact, external third-party stuff that we leased moving up. So that's the constraints we have. But in the long run, we do need effectively
power and we need DCs. And some of these things are more long lead. But I feel pretty good that going into the second half of even this fiscal year, that some of
that supply/demand will match up.
Operator
The next question comes from the line of Brent Thill with Jefferies. Please proceed.

### Exchange 002

**Brent Thill (Jefferies)**: Amy, good to hear the reacceleration in the back half for Azure. I guess many are asking, 34% growth in Q1 falling to low 30s. I know the comp is a couple of
points harder, but is there anything else you're contemplating in that guide for Q2 to see that deceleration other than a tougher comp?

### Exchange 003

**Mark Moerdler (Bernstein)**: Congratulations on the quarter. The question every investor obviously asks is a question on the CapEx growth and the CapEx spend. Obviously, half of that
facility is an equivalent that have a longer life, but the other half is the rest of the components. Can you give any color on how you think of that growth? Does it
return to the traditional approach, where basically CapEx is going to grow in line or slightly slower than cloud revenue? And if so, any sense of the timing? Do we
have enough facilities online by sometime next year, et cetera? Any color would be appreciated.

### Exchange 004

**Karl Keirstead (UBS)**: I'm actually not going to ask a question about the numbers, but Satya and Amy, I'd love to ask a question about OpenAI. Since the print three months ago, we
investors have been hit with a torrent of media stories about OpenAI and Microsoft. And I'd love to give Microsoft an opportunity to frame the relationship. It
seems to me it's critically important. But we have been, I think, everyone on the line picking up signals that perhaps Microsoft wants to diversify somewhat at the
model layer and offer customers choice. So, Satya, I'd love to get your framing of the relationship. And then in terms of the numbers, maybe this is a little bit more
for you, Amy, but how does Microsoft manage the demands on CapEx from helping OpenAI with its scaling ambitions? And how do you manage the impact on
other income that you just gave us some color on?

**Satya Nadella (CEO)**: Sure. Thanks, Karl. So, I'd say first, the partnership for both sides, that's OpenAI and Microsoft, has been super beneficial. After all, we effectively sponsored what
is one of the most highest-valued private companies today when we invested in them and really took a bet on them and their innovation four, five years ago. And
that has led to great success for Microsoft. That's led to great success for OpenAI. And we continue to build on it, right? So, we serve them with world-class
infrastructure on which they do their innovation in terms of models, on top of which we innovate on both the model layer with some of the post-training stuff we do
as well as some of the small models we build. And then, of course, all of the product innovation, right? One of the things that my own sort of conviction of OpenAI
and what they were doing came about when I started seeing something like GitHub Copilot as a product get built or DAX Copilot get built or M365 Copilot get
built. So, we have a fantastic portfolio of innovation that we build on top of that. And the same also, I would say, we are investors. We feel very, very good about
sort of our investment stake in OpenAI. And so, our focus, and we're always in constant dialogue with them in a partnership like this where both sides have
achieved mutual success at the pace at which we've achieved it, that means we need to kind of push each other to do more, to capture the moment and that's
what we plan to do, and we intend to keep building on it.

### Exchange 005

**Kash Rangan (Goldman Sachs)**: Satya, when you talked about the investment cycle, these models are getting bigger, more expensive, but you also pointed out to how the inference phase were
likely to get paid. How does that cycle look like an inference for Microsoft? Where are the products and the applications that will show up on the Microsoft P&L as
a result of the inference rate of AI kicking in?

**Satya Nadella (CEO)**: Thanks, Kash. I mean, the good news for us is that we're not waiting for that inference to show up, right? If you sort of think about the point, we even made that
this is going to be the fastest growth to $10 billion of any business in our history, it's all inference, right? One of the things that may not be as evident is that we're
not actually selling raw GPUs for other people to train. In fact, that's sort of a business we turn away because we have so much demand on inference that we are
not taking what I would -- in fact, there's a huge adverse selection problem today where people -- it's just a bunch of tech companies still using VC money to buy a
bunch of GPUs. We kind of really are not even participating in most of that because we are literally going to the real demand, which is in the enterprise space or
our own products like GitHub Copilot or M365 Copilot. So, I feel the quality of our revenue is also pretty superior in that context. And that's what gives us even the
conviction, to even Amy's answers previously, about our capital spend is if this was just all about sort of a bunch of people training large models and that was all
we got, then that would be ultimately still waiting, to your point, for someone to actually have demand which is real. And in our case, the good news here is we
have a diversified portfolio. We're seeing real demand across all of that portfolio.

### Exchange 006

**Kash Rangan (Goldman Sachs)**: That's, to your point, that you invest now and you can get the growth later, even if you slow down the CapEx, right, that's what you're trying to tell us?

### Exchange 007

**Mark Murphy (JPMorgan)**: I'm wondering if you can shed any more light just on the nature of the supply limitations that you're mentioning that are impacting Azure in Q2, where that impact
might be incrementally just a touch more than we expected? Is it more the GPU supply? Is there some element of power cooling or the ability to wire up the
networks? And Amy, should we infer that the supply is constraining Azure growth by roughly a couple of few points in Q2 or am I overestimating that?

### Exchange 008

**Raimo Lenschow (Barclays)**: If you talk about the market at the moment because you were first with Copilot, you had identified a lot with Copilots and now we're talking agents. Can you, kind
of -- Satya, how do you think about that? And to me, it looks like an evolution that we're discovering how to kind of productize AI better, et cetera. So how do you
think about that journey between Copilots, agents and maybe what's coming next?

**Satya Nadella (CEO)**: Sure. The system we have built is Copilot, Copilot Studio, agents and autonomous agents. You should think of that as the spectrum of things, right? So ultimately, 
the way we think about how this all comes together is you need humans to be able to interface with AI. So, the UI layer for AI is Copilot. You can then use Copilot 
Studio to extend Copilot. For example, you want to connect it to your CRM system, to your office system, to your HR system. You do that through Copilot Studio 
by building agents effectively. You also build autonomous agents. So, you can use even that's the announcement we made a couple of weeks ago is you can 
even use Copilot Studio to build autonomous agents. Now these autonomous agents are working independently, but from time to time, they need to raise an 
exception, right? So autonomous agents are not fully autonomous because at some point, they need to either notify someone or have someone input something. 
And when they need to do that, they need a UI layer and that's where again, it's Copilot. So, Copilot, Copilot agents built-in Copilot Studio, autonomous agents 
built in Copilot Studio, that's the full system we think that comes together, and we feel very, very good about the position. And then, of course, we are taking the

underlying system services across that entire stack that I just talked about, making it available in Azure, right? So, you have the raw infrastructure if you want it.
You have the model layer independent of it. You have the AI app server in Azure AI, right? So, everything is also a building block service in Azure for you to be
able to build. In fact, if you want to build everything that we have built in the Copilot stack, you can build it yourself using the AI platform. So that's sort of, in
simple terms, our strategy, and that's kind of how it all comes together.

### Exchange 009

**Rishi Jaluria (RBC)**: I want to go and think a little bit about Copilot. How we should be thinking about kind of numbers here with the recategorization? It seems like that was maybe
softer in the past than expected or maybe with the numbers this quarter starting to pick up. Can you maybe walk us through what you're seeing on that? And
maybe more importantly, how we should be thinking about your overall AI strategy on consumer versus enterprise, especially now with the stock on the fold?

**Satya Nadella (CEO)**: Yes. On the first part, Rishi, to your question, I think we feel very, very good about the momentum we have in the commercial Copilot, right? As I said in my
remarks and Amy talked about, this is the fastest growth of a new suite in M365. If I compare it to what we saw even way back in E3 or E5 or the transition from O
to M, this is really much faster, right? It's the numbers of penetration of the Fortune 500 and then the fact that they're coming back for more seats and what have
you. So, it's very strong in that context. The other thing I'd like to mention is that we want this to be something that is systemic, right, because people need to be
able to put the security controls, then they need to deploy, then there's skilling and then there's change management. So, this is not like you just -- it's not a tool
like when I talk about Copilot, Copilot Studio, agents. It's really as much about a new way to work. And sometimes I describe it as what happened throughout the
'90s with PC penetration. After all, if you take a business process like forecasting, what was it like pre e-mail and Excel and post e-mail and Excel, that's the type
of change that you see with Copilot. But overall, we feel great about the rate of progress and the penetration. And then on the consumer side, look, for us, the
exciting part here is to be able to use the same investment we are making in the commercial where we have structural strength and then beyond the offense. One
of the things that I think I hope you all catch in our earnings is ex-TAC, our revenue, when it comes to what we describe as search, news and ads, is growing
faster than market. So, it's fantastic to see that. And so that's kind of our consumer business, which in Microsoft's large scope, it's sort of even a $10-plus billion
business or sometimes go missing. But in our case, it is actually a fantastic growth business that's growing faster than market. We feel good about how we will
use AI in LinkedIn. In fact, LinkedIn is a consumer business as you know. You saw even this week, they announced some new capabilities for both consumers
and, in their case, even recruiting. So, we think that AI, the same investment gets monetized even through LinkedIn's innovation. And gaming, of course, is
another place where you'll see some of these things apply. And Windows, right? So, the place where I think I'm excited about is Copilot+ PCs. For us, it's not
about having a disconnected edge. It's about having hybrid AI where the rebirth of sort of the PC as the edge of AI is going to be one of the most exciting things
for developers. So, we feel well positioned, quite frankly, with the same investment. So, this is -- that's the thing. We're not a conglomerate here. We are sort of
one company. That means we invest once and then we have all these categories that benefit from that, and that's the theory of the firm for us. And so, we feel
good about all of that coming together.

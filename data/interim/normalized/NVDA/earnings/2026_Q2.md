# NVDA Q2 2026 Earnings Call

**Document ID**: NVDA_2026_Q2
**Ticker**: NVDA
**Year**: 2026
**Quarter**: Q2
**Utterances**: 17

---

---

## Q&A Session

### Exchange 001

**Vivek Arya (Bank of America)**: Colette, just wanted to clarify the $2 billion to $5 billion in China. What needs to happen and what is the sustainable pace of that China business as you get into
Q4? And then Jensen, for you on the competitive landscape, several of your large customers already have or are planning many ASIC projects. I think one of
your you know, ASIC competitors, Broadcom, signaled that they could grow their AI business almost, you know, 55, 60% next year. Any scenario in which you
see the market moving more towards ASICs and away from NVIDIA Corporation GPU? Just what are you hearing from your customers? How are they managing
this split between their use of merchant silicon and ASICs. Thank you. Thanks for that. So let me first answer your question regarding what will it take for the
H20s to be shipped. There is interest in our H20s. There is the initial set of licenses that we received. And then additionally, we do have supply that we are ready,
and that is why we communicated that somewhere in the range of about $2 to $5 billion this quarter we could potentially ship. We are still waiting on several of the
geopolitical issues going back and forth between the governments and the companies trying to determine their purchases and what they want to do. So it is still
open at this time, and we are not exactly sure what that full amount will be that this quarter. However, if more interest arrives, more licenses arrive, again, we can
also still build additional H20 and ship more as well.

**Jensen Huang (President)**: NVIDIA Corporation builds very different things than ASICs. But let us talk about ASICs first. A lot of projects are started. Many startup companies are created.
Very few products go into production. And the reason for that is it is really hard. Accelerated computing is unlike general-purpose computing. You do not write
software and just compile it into a processor. Accelerated computing is a full stack co-design problem. And AI factories in the last several years have become so
much more complex. Because the scale of the problems has grown so significantly. It is really the ultimate, the most extreme computer science problem the world
has ever seen, obviously. And so the stack is complicated. The models are changing incredibly fast. From generative based on autoregressive to generative
based on diffusion to mixed models to multimodality, the number of different models that are coming out that are either derivatives of transformers or evolutions of
transformers is just daunting. One of the advantages that we have is that NVIDIA Corporation is available in every cloud. We are available from every computer
company. We are available from the cloud to on-prem to edge to robotics. On the same programming model. And so it is sensible that every framework in the
world supports NVIDIA Corporation. When you are building a new model architecture, releasing it on NVIDIA Corporation's most sensible. And so the diversity of
our platform both in the ability to evolve into any architecture, the fact that we are everywhere, and, also, we accelerate the entire pipeline. You know, everything
from data processing to pretraining to post-training with reinforcement learning all the way out to inference. And so when you build a data center with NVIDIA
Corporation, platform in it, the utility of it is best. The lifetime usefulness is much, much longer. And then I would just say that in addition to all of that, it is just a
really extremely complex systems problem anymore. You know, people talk about the chip itself. There is one ASIC. The GPU that many people talk about. But in
order to build Blackwell the platform, and Reuben, the platform, we had to build CPUs, that connect fast memory, low extremely energy-efficient memory. For
large KB caching necessary for agentic AI. To the GPU to a super NIC to a scale-up switch we call NVLink, completely revolutionary when we are in our fifth
generation now. To a scale-out switch, whether it is quantum or Spectrum X Ethernet, to now scale across switches so that we could prepare for these AI super
factories. With multiple gigawatts of computing all connected together. We call that Spectrum XGS. We just announced that at Hotchips this week. And so the
complications, the complexity of everything that we do is really quite extraordinary. It is just done at a really, really extreme scale now. And then lastly, if I could
just say one more thing. You know, we are in every cloud for a good reason. Not only do are we the most energy-efficient, our perf per watt is the best of any
computing platform. And in a world of power-limited data centers, perf per watt drives directly to revenues. And, you know, you have heard me say before, that in
a lot of ways, the more you buy, the more you grow. And because our perf per dollar, the performance per dollar is so incredible, you also have extremely great
margins. So the growth opportunity with NVIDIA Corporation's architecture and the gross margins opportunity with NVIDIA Corporation's architecture is absolutely
the best. And so there are a lot of reasons why NVIDIA Corporation has chosen by every cloud and every startup and every computer company. We are, you
know, really a holistic full stack solution for AI factories.
Sarah

Your next question comes from Ben Reitzes with Melius. Your line is open.

### Exchange 002

**Joe Moore (Morgan Stanley)**: Great. Thank you. Congratulations on reopening the China opportunity. Can you talk about the long-term prospects there? You have talked about, I think, half of
AI software world being there. You know, how much can NVIDIA Corporation grow in that business? And you know, how important is it that you get the Blackwall
architecture ultimately licensed there?

**Jensen Huang (President)**: The China market, I have estimated to be above $50 billion of opportunity for us this year. If we were able to address it, with competitive products and if it is $50
billion this year, you would expect it to grow say, 50% per year. As the rest of the world's AI market is growing as well. It is the second-largest computing market
in the world, and it is also the home of AI researchers. About 50% of the world's AI researchers are in China. The vast majority of the leading open-source models
are created in China. And so it is fairly important, I think, for the American technology companies to be able to address that market. And open source, as you
know, is created in one country, but it is used all over the world. The open-source models that have come out of China are really excellent. DeepSeek, of course,
gained global notoriety. QN is excellent. Kimi is excellent. There is a whole bunch of new models that are coming out. They are multimodal. They are great
language models. And it is really fueled the adoption of AI in enterprises around the world because enterprises want to build their own custom proprietary
software stacks. And so open-source model is really important for enterprise. It is really important for SaaS. Who also would like to build proprietary systems. It
has been really incredible for robotics around the world. And so open source is really important. And it is important that the American companies are able to
address it. This is going to be a very large market. We are talking to the administration about the importance of American companies to be able to address the
Chinese market. And, as you know, H20 has been approved for companies that are not on the entities list. And many licenses have been approved. And so I think
the opportunity for us to bring Blackwell to the China market is a real possibility. And so we just have to keep advocating the sensibility of and the importance of
American tech companies to be able to lead and win the AI race. And help make the American tech stack the global standard.
Sarah
Your next question comes from the line of Aaron Rakers with Wells Fargo. Your line is open.

### Exchange 003

**Aaron Rakers (Wells Fargo)**: Yes. Thank you for the question. I want to go back to the Spectrum XGS announcement this week and, you know, thinking about the Ethernet product now
pushing over $10 billion of annualized revenue. You know, Jess, what is the opportunity set that you see for Spectrum XGS? Do you think about this as kind of
the data center interconnect layer? Any thoughts on the sizing of this opportunity? You know, within that Ethernet portfolio? Thank you.

**Jensen Huang (President)**: We now offer three networking technologies. One is for scale-up, one is for scale-out, and one for scale across. Scale-up is so that we could build the largest 
possible virtual GPU, the virtual compute node. NVLink is revolutionary. NVLink 72 is what made it possible for Blackwell to deliver such an extraordinary 
generational jump over Hopper's MBLink eight. At a time when we have long-thinking models, agentic AI reasoning systems, the NVLink basically amplifies the 
memory bandwidth, which is really critical for reasoning systems. And so NVLink 72 is fantastic. We then scale out with networking, which we have two. We have 
InfiniBand, which is unquestionably the lowest latency, the lowest jitter, the best scale-out network. It does require more expertise in managing those networks, 
and for supercomputing, for the leading model makers, InfiniBand, quantum InfiniBand is the unambiguous choice. If you were to benchmark an AI factory, ones

with InfiniBand are the best performance. For those who would like to use Ethernet because their whole data center is built with Ethernet, we have a new type of
Ethernet called Spectrum Ethernet. Spectrum Ethernet is not off the shelf. It has a whole bunch of new technologies designed for low latency and low jitter and
congestion control, and it has the ability to come closer, much, much closer to InfiniBand than anything that is out there. And that is we call that Spectrum X
Ethernet. And then finally, we have Spectrum XGS, a gigascale for connecting multiple data centers, multiple AI factories into a super factory. A gigantic system.
And you are going to see that networking obviously is very important in AI factories. In fact, choosing the right networking, the performance, the throughput,
improvement. Going from, you know, 65% to 85% or 90%. That kind of step up because of your networking capability effectively makes networking free. You
know? Choosing the right networking, you are basically paying, you know, you will get a return on it like you cannot believe because the AI factory, a gigawatt, as
I mentioned before, could be $50 billion. And so the ability to improve the efficiency of that factory by tens of percents results in $1 to $20 billion worth of effective
benefit. So, you know, the networking is a very important part of it. It is the reason why NVIDIA Corporation dedicates so much in networking. That is the reason
why we purchased Mellanox five and a half years ago. And Spectrum X, as we mentioned earlier, is now quite a sizable business, and it is only about a year and
a half old. So Spectrum X is a home run. All three of them are going to be fantastic. NVLink scale-up, SpectrumX, and InfiniBand, scale-out. And then Spectrum
XGS for scale across.
Sarah
Your next question comes from Stacy Rasgon with Bernstein Research. Your line is open.

### Exchange 004

**Stacy Rasgon (Bernstein)**: Hi, guys. Thanks for taking my question. Have a more tactical question for Colette. So on the guide, we are up, you know, over $7 billion. The vast bulk of that is
going to be from data center. How do I think about apportioning that $7 billion out across Blackwell versus Hopper versus networking? I mean, it looks like
Blackwell was probably $27 billion in the quarter up from maybe $23 billion last quarter. You know, Hopper is still $6 or $7 billion. Post the H20. Like, do you think
the Hopper strength continues? Because how do I think about parsing that $7 billion out across all the three those three different components?

**Colette Kress (CFO)**: Thanks, Stacy, for the question. First part of it, looking at our growth between Q2 and Q3, Blackwell is still going to be the lion's share of what we have in terms of
data center. But keep in mind, that helps both our compute side as well as it helps our networking side. Because we are selling those significant systems that are
incorporating the NVLink that Jensen just spoke about. Selling Hopper, we are still selling it. H100, H200s, we are. But, again, they are HCX systems, and I still
believe our Blackwell will be the lion's share of what we are doing. On there. So we will continue. We do not have any more specific details, in terms of how we
will finish our quarter but you should expect Blackwell again to be the driver of the growth.
Sarah
Your next question comes from Jim Schneider of Goldman Sachs. Your line is open.

### Exchange 005

**Jim Schneider (Goldman Sachs)**: Good afternoon. Thanks for taking my question. You have been very clear about the reasoning model opportunity that you see and you have also been relatively
clear about the technical specs for Rubin, but maybe you could provide a little bit of context about how you view the Rubin product transition going forward. What
incremental capability does that offer to customers? And would you say that Rubin is a bigger, smaller, or similar step up in terms of performance for capability
perspective relative to what we saw with Blackwell? Thank you.

**Jensen Huang (President)**: Yeah. Thanks. Ruben. Ruben, we are on an annual cycle. And the reason why we are on an annual cycle is because we can do so to accelerate the cost
reduction and maximize the revenue generation for our customers. When we increase the perf per watt, the token generation per amount of usage of energy. We
are effectively driving the revenues of our customers. The perf per watt of Blackwell will be for reasoning systems an order of magnitude higher than Hopper. And
so for the same amount of energy and everybody's data center energy limited by definition, for any data center, that we using Blackwell, you will be able to
maximize your revenues. Compared to anything we have done in the past compared to anything in the world today. And because the perf per dollar the
performance is so good, that the perf per dollar invested in the capital would also allow you to improve your gross margins. To the extent that we have great ideas
for every single generation, we could improve their revenue generation, improve the AI capability, improve the margins, of our customers, by releasing new
architectures. And so we advise our partners, our customers to pace themselves and to build these data centers on an annual rhythm. Ruben is going to have a
whole bunch of new ideas. I paused for a second because, you know, I have got plenty of time between now and a year from now to tell you about all the
breakthroughs that Rubens are going to bring. But Ruben has a lot of great ideas. I am anxious to tell you but I cannot right now. And I will save it for a GTC to tell
you more and more about it. But, nonetheless, for the next year, we are ramping really hard into now Grace Blackwell, GB200, and then now 300, we are ramping
really hard into data centers. This year is obviously a record-breaking year. I expect next year to be a record-breaking year. And while we continue to increase the
performance of AI capabilities as we race towards artificial superintelligence on the one hand, and continue to increase the revenue generation capabilities of our
hyperscalers on the other hand.
Sarah
Your final question comes from Timothy Arcuri with UBS. Your line is open.

### Exchange 006

**Timothy Arcuri (UBS)**: Thanks a lot. Jensen, I wanted to ask you just answer the question. You threw out a number. You said 50% CAGR for the AI market. So I am wondering how
much visibility that you have into next year. Is that kind of a reasonable bogey in terms of how much your data center revenue should grow next year? I would
think you will grow at least in line with that CAGR. And maybe are there any puts and takes to that? Thanks.

**Jensen Huang (President)**: Well, I think the best way to look at it is we have reasonable forecasts from our large customers for next year. A very, very significant forecast. And we still have a
lot of businesses that we are still winning. And a lot of startups that are still being created. Do not forget that the number of startups for AI native startups was
$100 billion was funded last year. This year, the year is not even over yet. It is $180 billion funded. If you look at AI native, the top AI native startups that are
generating revenues, last year was $2 billion. This year is $20 billion. Next year, being 10 times higher than this year, is not inconceivable. And the open-source
models are now opening up large enterprises, SaaS companies, industrial companies, robotics companies, to now join the AI revolution another source of growth
and you know, whether it is AI natives or enterprise SaaS or industrial AI, or startups. We are just seeing just enormous amount of interest in AI and demand for
AI. Right now, the buzz is I am sure all of you know about the buzz out there. The buzz is everything sold out. H1 Hers sold out. H2 hundreds are sold out. Large
CSPs are coming out renting capacity from other CSPs. And so the AI native startups really scrambling to get capacity. So that they could train their reasoning
models. And so the demand is really, really high. But the long-term outlook between where we are today, CapEx has doubled in two years. It is now running about
$600 billion a year just in the large hyperscalers. For us to grow into that $600 billion a year representing a significant part of that CapEx is not unreasonable. And
so I think the next several years, surely through the decade, we see just a really fast-growing, really significant growth opportunities ahead. Let me conclude with
this. Blackwell is the next-generation AI platform the world has been waiting for. Delivers an exceptional generational leap. NVIDIA Corporation's NVLink 72 rack
scale computing is revolutionary. Arriving just in time as reasoning AI models drive order of magnitude increases in training and inference performance
requirement. Blackwell Ultra is ramping at full speed, and the demand is extraordinary. Our next platform, Rubin, is already in fab. We have six new chips that
represent the Rubin platform. They have all taped out the TSMC. Rubin will be our third-generation MB Link Rack Scale AI supercomputer. And so we expect to
have a much more mature and fully scaled-up supply chain. Blackwell and Rubin AI factory platforms will be scaling into the $3 to $4 trillion global AI factory
build-out through the end of the decade. Customers are building ever-greater scale AI factories. From thousands of Hopper GPUs in tens of megawatt data
centers to now hundreds of thousands of Blackwells in 100-megawatt facilities and soon, we will be building millions of Rubin GPU platforms powering
multi-gigawatt multisite AI super factories. With each generation, demand only grows. One-shot chatbots have evolved into reasoning agentic AI, that research
plan, and use tools. Driving orders of magnitude jump and compute for both training and inference, agentic AI is reaching maturity, and has opened the enterprise
market to build domain and company-specific AI agents. For enterprise workflows, products, services. The age of physical AI has arrived unlocking entirely new
industries in robotics, industrial automation, every industry and every industrial company will need to build two factories. One to build the machines, and another
to build their robotic AI. This quarter, NVIDIA Corporation reached record revenues and an extraordinary milestone in our journey. The opportunity ahead is
immense. A new industrial revolution has started. The AI race is on. Thanks for joining us today, and I look forward to addressing you next week. Next earnings
call. Thank you.
Sarah
This concludes today's conference call. You may now disconnect.

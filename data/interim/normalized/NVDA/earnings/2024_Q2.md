# NVDA Q2 2024 Earnings Call

**Document ID**: NVDA_2024_Q2
**Ticker**: NVDA
**Year**: 2024
**Quarter**: Q2
**Utterances**: 24

---

---

## Q&A Session

### Exchange 001

**Matt Ramsay (Cowen)**: Yes. Thank you very much. Good afternoon. Obviously, remarkable results. Jensen, I wanted to ask a question of you regarding the really quickly emerging 
application of large model inference. So I think it's pretty well understood by the majority of investors that you guys have very much a lockdown share of the 
training market. A lot of the smaller market -- smaller model inference workloads have been done on ASICs or CPUs in the past. And with many of these GPT and 
other really large models, there's this new workload that's accelerating super-duper quickly on large model inference. And I think your Grace Hopper Superchip 
products and others are pretty well aligned for that. But could you maybe talk to us about how you're seeing the inference market segment between small model

inference and large model inference and how your product portfolio is positioned for that? Thanks.

**Colette Kress (CFO)**: So thanks for that question regarding our supply. Yes, we do expect to continue increasing ramping our supply over the next quarters as well as into next fiscal
year. In terms of percent, it's not something that we have here. It is a work across so many different suppliers, so many different parts of building an HGX and
many of our other new products that are coming to market. But we are very pleased with both the support that we have with our suppliers and the long time that
we have spent with them improving their supply.

### Exchange 002

**Stacy Rasgon (Bernstein)**: Hi, guys. Thanks for taking my question. I was wondering, Colette, if you could tell me like how much of Data Center in the quarter, maybe even the guide is like
systems versus GPU, like DGX versus just the H100? What I'm really trying to get at is, how much is like pricing or content or however you want to define that
[indiscernible] versus units actually driving the growth going forward. Can you give us any color around that?

**Colette Kress (CFO)**: Sure, Stacy. Let me help. Within the quarter, our HGX systems were a very significant part of our Data Center as well as our Data Center growth that we had
seen. Those systems include our HGX of our Hopper architecture, but also our Ampere architecture. Yes, we are still selling both of these architectures in the
market. Now when you think about that, what does that mean from both the systems as a unit, of course, is growing quite substantially, and that is driving in terms
of the revenue increases. So both of these things are the drivers of the revenue inside Data Center. Our DGXs are always a portion of additional systems that we
will sell. Those are great opportunities for enterprise customers and many other different types of customers that we're seeing even in our consumer Internet
companies. The importance there is also coming together with software that we sell with our DGXs, but that's a portion of our sales that we're doing. The rest of
the GPUs, we have new GPUs coming to market that we talk about the L40S, and they will add continued growth going forward. But again, the largest driver of
our revenue within this last quarter was definitely the HGX system.

### Exchange 003

**Mark Lipacis (Jefferies)**: Hi. Thanks for taking my question and congrats on the success. Jensen, it seems like a key part of the success -- your success in the market is delivering the
software ecosystem along with the chip and the hardware platform. And I had a two-part question on this. I was wondering if you could just help us understand
the evolution of your software ecosystem, the critical elements. And is there a way to quantify your lead on this dimension like how many person years you've
invested in building it? And then part two, I was wondering if you would care to share with us your view on the -- what percentage of the value of the NVIDIA
platform is hardware differentiation versus software differentiation? Thank you.
A â€“ Jensen Huang
Yeah, Mark, I really appreciate the question. Let me see if I could use some metrics, so we have a run time called AI Enterprise. This is one part of our software
stack. And this is, if you will, the run time that just about every company uses for the end-to-end of machine learning from data processing, the training of any
model that you like to do on any framework you'd like to do, the inference and the deployment, the scaling it out into a data center. It could be a scale-out for a
hyperscale data center. It could be a scale-out for enterprise data center, for example, on VMware. You can do this on any of our GPUs. We have hundreds of
millions of GPUs in the field and millions of GPUs in the cloud and just about every single cloud. And it runs in a single GPU configuration as well as multi-GPU
per compute or multi-node. It also has multiple sessions or multiple computing instances per GPU. So from multiple instances per GPU to multiple GPUs, multiple
nodes to entire data center scale. So this run time called NVIDIA AI enterprise has something like 4,500 software packages, software libraries and has something
like 10,000 dependencies among each other. And that run time is, as I mentioned, continuously updated and optimized for our installed base for our stack. And
that's just one example of what it would take to get accelerated computing to work. The number of code combinations and type of application combinations is
really quite insane. And it's taken us two decades to get here. But what I would characterize as probably our -- the elements of our company, if you will, are
several. I would say number 1 is architecture. The flexibility, the versatility and the performance of our architecture makes it possible for us to do all the things that
I just said, from data processing to training to inference, for preprocessing of the data before you do the inference to the post processing of the data, tokenizing of
languages so that you could then train with it. The amount of -- the workflow is much more intense than just training or inference. But anyways, that's where we'll
focus and it's fine. But when people actually use these computing systems, it's quite -- requires a lot of applications. And so the combination of our architecture
makes it possible for us to deliver the lowest cost ownership. And the reason for that is because we accelerate so many different things. The second
characteristic of our company is the installed base. You have to ask yourself, why is it that all the software developers come to our platform? And the reason for
that is because software developers seek a large installed base so that they can reach the largest number of end users, so that they could build a business or get
a return on the investments that they make. And then the third characteristic is reach. We're in the cloud today, both for public cloud, public-facing cloud because
we have so many customers that use -- so many developers and customers that use our platform. CSPs are delighted to put it up in the cloud. They use it for
internal consumption to develop and train and to operate recommender systems or search or data processing engines and whatnot all the way to training and
inference. And so we're in the cloud, we're in enterprise. Yesterday, we had a very big announcement. It's really worthwhile to take a look at that. VMware is the
operating system of the world's enterprise. And we've been working together for several years now, and we're going to bring together -- together, we're going to
bring generative AI to the world's enterprises all the way out to the edge. And so reach is another reason. And because of reach, all of the world's system makers
are anxious to put NVIDIA's platform in their systems. And so we have a very broad distribution from all of the world's OEMs and ODMs and so on and so forth
because of our reach. And then lastly, because of our scale and velocity, we were able to sustain this really complex stack of software and hardware, networking
and compute and across all of these different usage models and different computing environments. And we're able to do all this while accelerating the velocity of
our engineering. It seems like we're introducing a new architecture every two years. Now we're introducing a new architecture, a new product just about every six
months. And so these properties make it possible for the ecosystem to build their company and their business on top of us. And so those in combination makes
us special.
Operator
Next, we'll go to Atif Malik with Citi. Your line is open.

### Exchange 004

**Toshiya Hari (Goldman Sachs)**: Hi. Thank you for taking the question. I had one quick clarification question for Colette and then another one for Jensen. Colette, I think last quarter, you had said
CSPs were about 40% of your Data Center revenue, consumer Internet at 30%, enterprise 30%. Based on your remarks, it sounded like CSPs and consumer
Internet may have been a larger percentage of your business. If you can kind of clarify that or confirm that, that would be super helpful. And then Jensen, a
question for you. Given your position as the key enabler of AI, the breadth of engagements and the visibility you have into customer projects, I'm curious how
confident you are that there will be enough applications or use cases for your customers to generate a reasonable return on their investments. I guess I ask the
question because there is a concern out there that there could be a bit of a pause in your demand profile in the out years. Curious if there's enough breadth and
depth there to support a sustained increase in your Data Center business going forward. Thank you.

**Colette Kress (CFO)**: Okay. So thank you, Toshiya, on the question regarding our types of customers that we have in our Data Center business. And we look at it in terms of combining
our compute as well as our networking together. Our CSPs, our large CSPs are contributing a little bit more than 50% of our revenue within Q2. And the next
largest category will be our consumer Internet companies. And then the last piece of that will be our enterprise and high performance computing.

### Exchange 005

**Timothy Arcuri (UBS)**: Thanks a lot. Can you talk about the attach rate of your networking solutions to your -- to the compute that you're shipping? In other words, is like half of your
compute shipping with your networking solutions more than half, less than half? And is this something that maybe you can use to prioritize allocation of the
GPUs? Thank you.

**Colette Kress (CFO)**: And let's see if I can answer your question regarding our software revenue. In part of our opening remarks that we made as well, remember, software is a part of
almost all of our products, whether they're our Data Center products, GPU systems or any of our products within gaming and our future automotive products.
You're correct, we're also selling it in a standalone business. And that stand-alone software continues to grow where we are providing both the software services,
upgrades across there as well. Now we're seeing, at this point, probably hundreds of millions of dollars annually for our software business, and we are looking at
NVIDIA AI enterprise to be included with many of the products that we're selling, such as our DGX, such as our PCIe versions of our H100. And I think we're
going to see more availability even with our CSP marketplaces. So we're off to a great start, and I do believe we'll see this continue to grow going forward.
Operator
And that does conclude today's question-and-answer session. I'll turn the call back over to Jensen Huang for any additional or closing remarks.

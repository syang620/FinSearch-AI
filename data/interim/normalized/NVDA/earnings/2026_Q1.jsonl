{"doc_id": "NVDA_TRANSCRIPT_2026_Q1", "ticker": "NVDA", "quarter": "Q1", "filing_date": "2026-03-01", "phase": "qa", "speaker_name": "Toshiya Hari", "speaker_role": "unknown", "speaker_firm": null, "utterance_id": "u_0000", "utterance_type": "statement", "text": "Thank you. Good afternoon, everyone, and welcome to NVIDIA Corporation's conference call for the first quarter fiscal 2026. With me today from NVIDIA\nCorporation are Jensen Huang, president and chief executive officer, and Colette Kress, executive vice president and chief financial officer. I'd like to remind you\nthat our call is being webcast live on NVIDIA Corporation's investor relations website. The webcast will be available for replay until the conference call to discuss\nour financial results for the second quarter of fiscal 2026. The content of today's call is NVIDIA Corporation's property. It cannot be reproduced or transcribed\nwithout our prior written consent. During this call, we may make forward-looking statements based on current expectations. These are subject to a number of\nsignificant risks and uncertainties, and our actual results may differ materially. For a discussion of factors that could affect our future financial results and\nbusiness, please refer to the disclosure in today's earnings release, our most recent forms 10-K and 10-Q, and the reports that we may file on form 8-K with the\nSecurities and Exchange Commission. All our statements are made as of today, May 28, 2025, and based on information currently available to us. Except as\nrequired by law, we assume no obligation to update any such statements. During this call, we will discuss non-GAAP financial measures. You can find a\nreconciliation of these non-GAAP financial measures to GAAP financial measures in our CFO commentary, which is posted on our website. With that, let me turn\nthe call over to Colette.", "char_count": 1607, "word_count": 255, "token_count": 332, "exchange_id": null, "exchange_role": null, "chunk_id": "NVDA_TRANSCRIPT_2026_Q1_u_0000", "company": "NVDA", "doc_type": "earnings_transcript", "fiscal_year": 2026, "period": "2026-Q1", "source_file": "data/earnings_calls_manual/NVDA/NVDA_FY2026_Q1.pdf", "parsed_at": "2025-11-09T21:07:08Z", "section_id": null, "section_title": null}
{"doc_id": "NVDA_TRANSCRIPT_2026_Q1", "ticker": "NVDA", "quarter": "Q1", "filing_date": "2026-03-01", "phase": "qa", "speaker_name": "Colette Kress", "speaker_role": "CFO", "speaker_firm": null, "utterance_id": "u_0001", "utterance_type": "answer", "text": "Thank you, Toshiya. We delivered another strong quarter, with revenue of $44 billion, up 69% year over year, exceeding our outlook in what proved to be a \nchallenging operating environment. Data center revenue of $39 billion grew 73% year on year. The AI workloads have transitioned strongly to inference, and AI \nfactory build-outs are driving significant revenue. Our customers' commitments are firm. On April 9, the US government issued new export controls on H20, our \ndata center GPU designed specifically for the China market. We sold H20 with the approval of the previous administration. Although our H20 has been in the \nmarket for over a year and does not have a market outside of China, the new export controls on H20 did not provide a grace period to allow us to sell through our \ninventory. In Q1, we recognized $4.6 billion in H20 revenue, which occurred prior to April 9, but also recognized a $4.5 billion charge as we wrote down inventory \nand purchase obligations tied to orders we had received prior to April 9. We were unable to ship $2.5 billion in H20 revenue in the first quarter due to the new \nexport controls. The $4.5 billion charge was less than what we initially anticipated as we were able to reuse certain materials. We are still evaluating our limited \noptions to supply data center compute products compliant with the US government's revised export control rules. Losing access to the China AI accelerator \nmarket, which we believe will grow to nearly $50 billion, would have a material adverse impact on our business going forward and benefit our foreign competitors \nin China and worldwide. Our Blackwell ramp, the fastest in our company's history, drove a 73% year-on-year increase in data center revenue. Blackwell \ncontributed nearly 70% of data center compute revenue in the quarter, with a transition from Hopper nearly complete. The introduction of GB200 NBL was a \nfundamental architectural change to enable data center scale workloads and to achieve the lowest cost per inference token. These systems are complex to build, \nwe have seen a significant improvement in manufacturing yields, and rack shipments are moving to strong rates to end customers. GB200 and VO racks are now \ngenerally available for model builders, enterprises, and sovereign customers to develop and deploy AI. On average, major hyperscalers are each deploying nearly \n1,000 NBL72 racks, or 72,000 Blackwell GPUs per week, and are on track to further ramp output this quarter. Microsoft, for example, has already deployed tens \nof thousands of Blackwell GPUs and is expected to ramp to hundreds of thousands of GB200s with OpenAI as one of its key customers. Key learnings from the \nGB200 ramp will allow for a smooth transition to the next phase of our product roadmap, Blackwell Ultra. Sampling of GB300 systems began earlier this month at \nthe major CSPs, and we expect production shipments to commence later this quarter. GB300 will leverage the same architecture, same physical footprint, and \nthe same electrical and mechanical specifications as GB200. The GB300 drop-in design will allow CSPs to seamlessly transition their systems and manufacturing \nused for GB200 while maintaining high yields. B300 GPUs, with 50% more HBM, will deliver another 50% increase in dense FP4 inference compute performance \ncompared to the B200. We remain committed to our annual product cadence with our roadmap extending through 2028, tightly aligned with the multiple-year \nplanning cycles of our customers. We are witnessing a sharp jump in inference demand. OpenAI, Microsoft, and Google are seeing a step function leap in token \ngeneration. Microsoft processed over 100 trillion tokens in Q1, a fivefold increase on a year-over-year basis. This exponential growth in Azure OpenAI is \nrepresentative of strong demand for Azure AI Foundry, as well as other AI services across Microsoft's platform. Inference serving startups are now serving \nmodels using B200, tripling their token generation rate and corresponding revenues for high-value reasoning models such as Deepseeker One. As reported by", "char_count": 4101, "word_count": 656, "token_count": 859, "exchange_id": null, "exchange_role": null, "chunk_id": "NVDA_TRANSCRIPT_2026_Q1_u_0001", "company": "NVDA", "doc_type": "earnings_transcript", "fiscal_year": 2026, "period": "2026-Q1", "source_file": "data/earnings_calls_manual/NVDA/NVDA_FY2026_Q1.pdf", "parsed_at": "2025-11-09T21:07:08Z", "section_id": null, "section_title": null}
{"doc_id": "NVDA_TRANSCRIPT_2026_Q1", "ticker": "NVDA", "quarter": "Q1", "filing_date": "2026-03-01", "phase": "qa", "speaker_name": "Artificial Artificial Analysis", "speaker_role": "unknown", "speaker_firm": null, "utterance_id": "u_0002", "utterance_type": "statement", "text": "industry. Developer engagements increased with adoption ranging from LLM providers such as Perplexity to financial service institutions such as Capital One, \nwho reduced agentic chatbot latency by 5x with Dynamo. In the latest MLPerf inference results, we submitted our first results using GB200 NBL72, delivering up \nto 30x higher inference throughput compared to our 8 GPU H200 submission on the challenging Llama 3.1 benchmark. This feat was achieved through a \ncombination of tripling the performance per GPU as well as 9x more GPUs, all connected on a single NVLink domain. And while Blackwell is still early in its life \ncycle, software optimizations have already improved its performance by 1.5x in the last month alone. We expect to continue improving the performance of \nBlackwell through its operational life as we have done with Hopper and Ampro. For example, we increased the inference performance of Hopper by four times \nover two years. This is the benefit of NVIDIA Corporation's programmable CUDA architecture and rich ecosystem. The pace and scale of AI factory deployments \nare accelerating, with nearly 100 NVIDIA Corporation-powered AI factories in flight this quarter, a twofold increase year over year, with the average number of \nGPUs powering each factory also doubling in the same period. More AI factory projects are starting across industries and geographies. NVIDIA Corporation's \nfull-stack architecture is underpinning AI factory deployments as industry leaders like AT&T, BYD, Capital One, Foxconn, MediaTek, and Telenor are strategically \nvital sovereign clouds like those recently announced in Saudi Arabia, Taiwan, and the UAE. We have a line of sight to projects requiring tens of gigawatts of \nNVIDIA Corporation AI infrastructure in the not-too-distant future. The transition from generative to agentic AI, AI capable of perceiving, reasoning, planning, and \nacting, will transform every industry, every company, and country. We envision AI agents as a new digital workforce capable of handling tasks ranging from \ncustomer service to complex decision-making processes. We introduced the Llama NemoTron family of open reasoning models designed to supercharge agentic \nAI platforms for enterprises. Built on the Llama architecture, these models are available as NIMS or NVIDIA Corporation inference microservices with multiple\n\nsizes to meet diverse deployment needs. Our post-training enhancements have yielded a 20% accuracy boost and a 5x increase in inference speed. Leading\nplatform companies including Accenture, Cadence, Deloitte, and Microsoft are transforming work with our reasoning models. NVIDIA Corporation's Nexmo\nmicroservices are generally available across industries and are being leveraged by leading enterprises to build, optimize, and scale AI applications. With Nexmo,\nCisco increased model accuracy by 40% and improved response time by 10x in its code assistant. Nasdaq realized a 30% improvement in accuracy and\nresponse time in its AI platform's search capabilities. Shell's custom LLM achieved a 30% increase in accuracy when trained with NVIDIA Corporation's Nexmo.\nNemo's parallelism techniques accelerated model training time by 20% when compared to other frameworks. We also announced a partnership with Yam Brands,\nthe world's largest restaurant company, to bring NVIDIA Corporation AI to 500 of its restaurants this year, expanding to 61,000 restaurants over time to streamline\norder taking, optimize operations, and enhance service across its restaurants. For AI-powered cybersecurity, leading companies like Checkpoint, Cloudstrike, and\nPalo Alto Networks are using NVIDIA Corporation's AI security and software stack to build, optimize, and secure agentic workflows, with CloudStrike realizing 2x\nfaster detection, triage with 50% less compute cost. Moving to networking, sequential growth in network resumed in Q1 with revenue up 64% quarter over quarter\nto $5 billion. Our customers continue to leverage our platform to efficiently scale up and scale out AI factory workloads. We created the world's fastest switch,\nNVLink, for scale-up or NVLink compute fabric in its fifth generation offers 14x the bandwidth of PCIe Gen 5. NVLink72 carries 130 terabytes per second, a\nbandwidth in a single rack equivalent to the entirety of the world's peak Internet traffic. NVLink is a new growth vector and is off to a great start, with Q1 shipments\nexceeding $1 billion. At Computex, we announced NVLink Fusion. Hyperscale customers can now build semi-custom CCUs and accelerators that connect directly\nto the NVIDIA Corporation platform with NVLink. We are now enabling key partners, including ASIC providers such as MediaTek, Norbel, Alchip Technologies,\nand Astera Labs, as well as CPU suppliers such as Pizizzo and Qualcomm, to leverage NVLink Fusion to connect our respective ecosystems. For scale-out, our\nenhanced Ethernet offerings deliver the highest throughput low latency networking for AI. Spectrum X posted strong sequential and year-on-year growth and is\nnow annualizing over $8 billion in revenue. Adoption is widespread across major CSPs and consumer Internet companies, including Coreweave, Microsoft Azure,", "char_count": 5187, "word_count": 771, "token_count": 1058, "exchange_id": null, "exchange_role": null, "chunk_id": "NVDA_TRANSCRIPT_2026_Q1_u_0002", "company": "NVDA", "doc_type": "earnings_transcript", "fiscal_year": 2026, "period": "2026-Q1", "source_file": "data/earnings_calls_manual/NVDA/NVDA_FY2026_Q1.pdf", "parsed_at": "2025-11-09T21:07:08Z", "section_id": null, "section_title": null}
{"doc_id": "NVDA_TRANSCRIPT_2026_Q1", "ticker": "NVDA", "quarter": "Q1", "filing_date": "2026-03-01", "phase": "qa", "speaker_name": "Oracle Cloud", "speaker_role": "unknown", "speaker_firm": null, "utterance_id": "u_0003", "utterance_type": "statement", "text": "silicon photonics switches, featuring the world's most advanced copacage optics. These platforms will enable next-level AI factory scaling to millions of GPUs\nthrough the increasingly power efficiency by 3.5x and network resiliency by 10x while accelerating customer time to market by 1.3x. Transitioning to a quick\nsummary of our revenue by geography. China, as a percentage of our data center revenue, is slightly below our expectations and down sequentially due to H20\nexport licensing controls. For Q2, we expect a meaningful decrease in China data center revenue. As a reminder, while Singapore represented nearly 20% of our\nQ1 build revenue as many of our large customers use Singapore for centralized invoicing, our products are almost always shelved elsewhere. Note that over 99%\nof H100, H200, and Blackwell data center compute revenue billed to Singapore was for orders from US-based customers. Moving to gaming and AI PCs. Gaming\nrevenue was a record $3.8 billion, increasing 48% sequentially and 42% year on year. Strong adoption by gamers, creatives, and AI enthusiasts have made\nBlackwell our fastest ramp ever. Against the backdrop of robust demand, we greatly improved our supply and availability in Q1 and expect to continue these\nefforts in Q2. AI is transforming PC and creator and gamers. With a 100 million user installed base, GeForce represents the largest footprint for PC developers.\nThis quarter, we added to our AI PC laptop offerings, including models capable of running Microsoft's Copilot. Plus, this past quarter, we brought Blackwell\narchitecture to mainstream gaming with its launch of GeForce RTX 5060 and 5060 Ti, starting at just $299. The RTX 5060 also debuted in laptops starting at\n$1,099. These systems double the frame rate and slash latency. These GeForce RTX 5060 and 5060 TI desktop GPUs and laptops are now available. In console\ngaming, the recently unveiled Nintendo Switch 2 leverages NVIDIA Corporation's neural rendering and AI technologies, including next-generation custom RTX\nGPUs with DLSS technology to deliver a giant leap in gaming performance to millions of players worldwide. Nintendo has shipped over 150 million Switch\nconsoles to date, making it one of the most successful gaming systems in history. Moving to crow visualization. Revenue of $509 million was flat sequentially and\nup 19% year on year. Tariff-related uncertainty temporarily impacted Q1 systems. Demand for our AI workstations is strong, and we expect sequential revenue\ngrowth to resume in Q2. NVIDIA Corporation's DGX Spark and Station revolutionized personal computing by putting the power of an AI supercomputer in a\ndesktop form factor. DGX Spark delivers up to one petaflop of AI compute while DGX Station offers an incredible 20 petaflops and is powered by the GB300\nsuperchip. DGX Spark will be available in calendar Q3, and DGX Station later this year. We have deepened Omniverse's integration and adoption into some of\nthe world's leading software companies, including SAP and Schneider Electric. New Omniverse blueprints such as MEGA for at-scale robotic fleet management\nare being leveraged by Keyon Group, Pegatron, Accenture, and other leading companies to enhance industrial operations. At Computex, we showcased\nOmniverse's great traction with technology manufacturing leaders, including TSMC, Quanta, Foxconn, and Pegatron. Using Omniverse, TSMC saves months in\nwork by designing fabs virtually. Foxconn accelerates thermal simulations by 150x. Pegatron reduced assembly line defect rates by 67%. Lastly, with our\nautomotive group, revenue was $567 million, down 1% sequentially, but up 72% year on year. Year-on-year growth was driven by the ramp of self-driving across\na number of customers and robust end demand for NEVs. We are partnering with GM to build the next-gen vehicles, factories, and robots using NVIDIA\nCorporation's AI simulation and accelerated computing. We are now in production with our full-stack solution for Mercedes Benz, starting with the new CLA,\nhitting roads in the next few months. We announced Isaac Groot, N1, the world's first open fully customizable foundation model for humanoid robots, enabling\ngeneralized reasoning and skill development. We also launched new open NVIDIA Corporation Cosmo World Foundation models. Leading companies include\nOneX, Agility Robots, Figueroa, Uber, and Wabi. We've begun integrating Cosmos into their operations for synthetic data generation. While agility robotics,", "char_count": 4475, "word_count": 686, "token_count": 961, "exchange_id": null, "exchange_role": null, "chunk_id": "NVDA_TRANSCRIPT_2026_Q1_u_0003", "company": "NVDA", "doc_type": "earnings_transcript", "fiscal_year": 2026, "period": "2026-Q1", "source_file": "data/earnings_calls_manual/NVDA/NVDA_FY2026_Q1.pdf", "parsed_at": "2025-11-09T21:07:08Z", "section_id": null, "section_title": null}
{"doc_id": "NVDA_TRANSCRIPT_2026_Q1", "ticker": "NVDA", "quarter": "Q1", "filing_date": "2026-03-01", "phase": "qa", "speaker_name": "Boston Dynamics", "speaker_role": "unknown", "speaker_firm": null, "utterance_id": "u_0004", "utterance_type": "statement", "text": "Isaac platform for healthcare simulation built on NVIDIA Corporation Omniverse and using NVIDIA Corporation Cosmos. The platform speeds development of\nrobotic imaging and surgery systems. The era of robotics is here. Billions of robots, hundreds of millions of autonomous vehicles, and hundreds of thousands of\nrobotic factories and warehouses will be developed. Alright. Moving to the rest of the P&L. GAAP gross margins and non-GAAP gross margins were 60.5% and\n61% respectively. Excluding the $4.5 billion charge, Q1 non-GAAP gross margins would have been 71.3%, slightly above our outlook at the beginning of the\nquarter. Sequentially, GAAP operating expenses were up 7% and non-GAAP operating expenses were up 6%, reflecting higher compensation and employee\ngrowth. Our investments include expanding our infrastructure capabilities and AI solutions, and we plan to grow these investments throughout the fiscal year. In\nQ1, we returned a record $14.3 billion to shareholders in the form of share repurchases and cash dividends. Our capital return program continues to be a key\nelement of our capital allocation strategy. Let me turn to the outlook for the second quarter. Total revenue is expected to be $45 billion, plus or minus 2%. We\nexpect modest sequential growth across all of our platforms. In the data center, we anticipate the continued ramp of Blackwell to be partially offset by a decline in\nChina revenue. Note, our outlook reflects a loss in H20 revenue of approximately $8 billion for the second quarter. GAAP and non-GAAP gross margins are\nexpected to be 71.8% and 72% respectively, plus or minus 50 basis points. We expect better Blackwell profitability to drive modest sequential improvement in\ngross margins. We are continuing to work towards achieving gross margins in the mid-seventies range late this year. GAAP and non-GAAP operating expenses\nare expected to be approximately $5.7 billion and $4 billion, respectively. We continue to expect full-year fiscal year 2026 operating expense growth to be in the\nmid-thirty percent range. GAAP and non-GAAP other income and expenses are expected to be an income of $450 million, excluding gains and losses from\nnon-marketable and publicly held equity securities. GAAP and non-GAAP tax rates are expected to be 16.5%, plus or minus 1%, excluding any discrete items.\nFurther financial details are included in the CFO commentary and other information available on our IR website, including a new financial information AI agent. Let\nme highlight upcoming events for the financial community. We will be at the B of A Global Technology Conference in San Francisco on June 4, the Rosenblatt\nVirtual AI Summit, and the Aztec Investor Conference in London on June 10, and GTC Paris at Viva Tech on June 11 in Paris. We look forward to seeing you at\nthese events. Our earnings call to discuss the results of our second quarter of fiscal 2026 is scheduled for August 27. Well, now let me turn it over to Jensen to\nmake some remarks.", "char_count": 2991, "word_count": 484, "token_count": 645, "exchange_id": null, "exchange_role": null, "chunk_id": "NVDA_TRANSCRIPT_2026_Q1_u_0004", "company": "NVDA", "doc_type": "earnings_transcript", "fiscal_year": 2026, "period": "2026-Q1", "source_file": "data/earnings_calls_manual/NVDA/NVDA_FY2026_Q1.pdf", "parsed_at": "2025-11-09T21:07:08Z", "section_id": null, "section_title": null}
{"doc_id": "NVDA_TRANSCRIPT_2026_Q1", "ticker": "NVDA", "quarter": "Q1", "filing_date": "2026-03-01", "phase": "qa", "speaker_name": "Jensen Huang", "speaker_role": "CEO", "speaker_firm": null, "utterance_id": "u_0005", "utterance_type": "answer", "text": "Thanks, Colette. We've had a busy and productive year. Let me share my perspective on some topics we're frequently asked. On export control, China is one of\nthe world's largest AI markets and a springboard to global success. With half of the world's AI researchers based there, the platform that wins China is positioned\nto lead globally. Today, however, the $50 billion China market is effectively closed to US industry. The H20 export ban ended our Hopper data center business in\nChina. We cannot reduce Hopper further to comply. As a result, we are taking a multibillion-dollar write-off on inventory that cannot be sold or repurposed. We are\nexploring limited ways to compete, but Hopper is no longer an option. China's AI moves on with or without US chips. It has to compute to train and deploy\nadvanced models. The question is not whether China will have AI, it already does. The question is whether one of the world's largest AI markets will run on\nAmerican platforms. Shielding Chinese chipmakers from US competition only strengthens them abroad and weakens America's position. Export restrictions have\nspurred China's innovation and scale. The AI race is not just about chips. It's about which stack the world runs on. As that stack grows to include 6G and\nquantum, US global infrastructure leadership is at stake. The US has based its policy on the assumption that China cannot make AI chips. That assumption was\nalways questionable and now it's clearly wrong. China has enormous manufacturing capability. In the end, the platform that wins the AI developers wins AI.\nExport controls should strengthen US platforms, not drive half of the world's AI talent to rivals. On DeepSeq, DeepSeq and QN from China are among the best\nopen-source AI models. Released freely, they've gained traction across the US, Europe, and beyond. Deepseeker One, like ChatGPT, introduced reasoning AI\nthat produces better answers the longer it thinks. Reasoning AI enables step-by-step problem-solving, planning, and tool use, turning models into intelligent\nagents. Reasoning is compute-intensive and requires hundreds to thousands more tokens per task than previous one-shot inference. Reasoning models are\ndriving a step function surge in inference demand. AI scaling laws remain firmly intact not only for training but now inference too, requiring massive scale\ncompute. So DeepSeq also underscores the strategic value of open-source AI. When popular models are trained and optimized on US platforms, it drives usage,\nfeedback, and continuous improvement, reinforcing American leadership across the stack. US platforms must remain the preferred platform for open-source AI.\nThat means supporting collaboration with top developers globally, including in China. America wins when models like DeepSeq and QN run best on American\ninfrastructure. Regarding onshore manufacturing, President Trump has outlined a bold vision to reshore advanced manufacturing, create jobs, and strengthen\nnational security. Future plants will be highly computerized and roboticized. We share this vision. TSMC is building six fabs and two advanced packaging plants in\nArizona to make chips for NVIDIA Corporation. Process qualification is underway, with volume production expected by year-end. Spill and Amcor are also\ninvesting in Arizona, constructing packaging, assembly, and test facilities. In Houston, we're partnering with Foxconn to construct a million-square-foot factory to\nbuild AI supercomputers. Wistron is building a similar plant in Fort Worth, Texas. To encourage and support these investments, we've made substantial long-term\npurchase commitments, a deep investment in America's AI manufacturing future. Our goal from chip to supercomputer, built in America, within a year. Each\nGB200 MBLink72 rack contains 1.2 million components and weighs nearly two tons. No one has produced supercomputers on this scale. Our partners are doing\nan extraordinary job. On AI diffusion rule, President Trump rescinded the AI diffusion rule, calling it counterproductive, and proposed a new policy to promote US\nAI tech with trusted partners. On his Middle East tour, he announced historic investments. I was honored to join him in announcing a 500-megawatt AI\ninfrastructure project in Saudi Arabia and a 5-gigawatt AI campus in the UAE. President Trump wants US tech to lead. The deals he announced are wins for\nAmerica, creating jobs, advancing infrastructure, generating tax revenue, and reducing the US trade deficit. The US will always be NVIDIA Corporation's largest\nmarket and home to the largest installed base of our infrastructure. Every nation now sees AI as core to the next industrial revolution, a new industry that\nproduces intelligence and essential infrastructure for every economy. Countries are racing to build national AI platforms to elevate their digital capabilities. At\nComputex, we announced Taiwan's first AI factory in partnership with Foxconn and the Taiwan government. Last week, I was in Sweden to launch its first\nnational AI infrastructure. Japan, Korea, India, Canada, France, the UK, Germany, Italy, Spain, and more are now building national AI factories to empower\nstartups, industries, and societies. Sovereign AI is a new growth engine for NVIDIA Corporation. Toshiya, back to you. Thank you.", "char_count": 5297, "word_count": 817, "token_count": 1080, "exchange_id": null, "exchange_role": null, "chunk_id": "NVDA_TRANSCRIPT_2026_Q1_u_0005", "company": "NVDA", "doc_type": "earnings_transcript", "fiscal_year": 2026, "period": "2026-Q1", "source_file": "data/earnings_calls_manual/NVDA/NVDA_FY2026_Q1.pdf", "parsed_at": "2025-11-09T21:07:08Z", "section_id": null, "section_title": null}
{"doc_id": "NVDA_TRANSCRIPT_2026_Q1", "ticker": "NVDA", "quarter": "Q1", "filing_date": "2026-03-01", "phase": "qa", "speaker_name": "Toshiya Hari", "speaker_role": "unknown", "speaker_firm": null, "utterance_id": "u_0006", "utterance_type": "statement", "text": "Operator, we will now open the call for questions. Can you please poll for questions?\nSarah\nThank you. At this time, I would like to remind everyone in order to ask a question, please press the star key followed by the number one on your telephone\nkeypad. Your first question comes from the line of Joe Moore with Morgan Stanley. Your line is open.", "char_count": 348, "word_count": 64, "token_count": 76, "exchange_id": null, "exchange_role": null, "chunk_id": "NVDA_TRANSCRIPT_2026_Q1_u_0006", "company": "NVDA", "doc_type": "earnings_transcript", "fiscal_year": 2026, "period": "2026-Q1", "source_file": "data/earnings_calls_manual/NVDA/NVDA_FY2026_Q1.pdf", "parsed_at": "2025-11-09T21:07:08Z", "section_id": null, "section_title": null}
{"doc_id": "NVDA_TRANSCRIPT_2026_Q1", "ticker": "NVDA", "quarter": "Q1", "filing_date": "2026-03-01", "phase": "qa", "speaker_name": "Joe Moore", "speaker_role": "Analyst", "speaker_firm": "Morgan Stanley", "utterance_id": "u_0007", "utterance_type": "question", "text": "Great. Thank you. You guys have talked about this scaling up of inference around reasoning models for, you know, at least a year now, and we've really seen\nthat come to fruition. As you talked about, we've heard it from your customers. Can you give us a sense for how much of that demand you know, you're able to\nserve and give us a sense for maybe how big the inference business is for you guys and, you know, do we need full-on NBL72 Rackscale solutions for reasoning\ninference going forward?", "char_count": 494, "word_count": 91, "token_count": 115, "exchange_id": "ex_001", "exchange_role": "question", "chunk_id": "NVDA_TRANSCRIPT_2026_Q1_u_0007", "company": "NVDA", "doc_type": "earnings_transcript", "fiscal_year": 2026, "period": "2026-Q1", "source_file": "data/earnings_calls_manual/NVDA/NVDA_FY2026_Q1.pdf", "parsed_at": "2025-11-09T21:07:08Z", "section_id": null, "section_title": null}
{"doc_id": "NVDA_TRANSCRIPT_2026_Q1", "ticker": "NVDA", "quarter": "Q1", "filing_date": "2026-03-01", "phase": "qa", "speaker_name": "Jensen Huang", "speaker_role": "CEO", "speaker_firm": null, "utterance_id": "u_0008", "utterance_type": "answer", "text": "Well, we would like to serve all of it. And I think we're on track to serve most of it. Grace Blackwell and VLINK72 is the ideal engine today, the ideal computer\nthinking machine, if you will, for reasoning AI. There's a couple of reasons for that. The first reason is that the token generation amount, the number of tokens\nreasoning goes through, is a hundred, a thousand times more than a one-shot chatbot. You know, it's essentially thinking to itself, breaking down a problem step\nby step. It might be planning multiple paths to an answer. It could be using tools, reading PDFs, reading web pages, watching videos, and then producing a result,\nan answer. The longer it thinks, the better the answer, the smarter the answer is. And so what we would like to do and the reason why Grace Blackwell was\ndesigned to give such a giant step up in inference performance is so that you could do all this and still get a response as quickly as possible. Compared to\nHopper, Grace Blackwell is some forty times higher speed and throughput. And so this is going to be a huge benefit in driving down the cost while improving the\nquality of response with excellent quality of service at the same time. So that's the fundamental reason. That was a core driving reason for Grace Blackwell\nMBLink72. Of course, in order to do that, we had to reinvent literally redesign the entire way that these supercomputers are built. But now we're in full production.\nIt's going to be exciting. It's going to be incredibly exciting.\nSarah\nThe next question comes from Vivek Arya with Bank of America Securities. Your line is open.", "char_count": 1604, "word_count": 285, "token_count": 358, "exchange_id": "ex_001", "exchange_role": "answer", "chunk_id": "NVDA_TRANSCRIPT_2026_Q1_u_0008", "company": "NVDA", "doc_type": "earnings_transcript", "fiscal_year": 2026, "period": "2026-Q1", "source_file": "data/earnings_calls_manual/NVDA/NVDA_FY2026_Q1.pdf", "parsed_at": "2025-11-09T21:07:08Z", "section_id": null, "section_title": null}
{"doc_id": "NVDA_TRANSCRIPT_2026_Q1", "ticker": "NVDA", "quarter": "Q1", "filing_date": "2026-03-01", "phase": "qa", "speaker_name": "Vivek Arya", "speaker_role": "Analyst", "speaker_firm": "Bank of America", "utterance_id": "u_0009", "utterance_type": "question", "text": "Thanks for the question. Just clarification for Colette first. So on the China impact, I think previously it was mentioned that at about $15 billion. So you had the $8\nbillion in Q2. So is there still some left as a headwind for the remaining quarters? How to model that? And then a question, Jensen, for you. Back at GTC, you had\noutlined a path towards almost a trillion dollars of AI spending over the next few years. Where are we in that build-out? And do you think it's going to be uniform\nthat you will see every spender, whether it's CSP, sovereign enterprises, or build-out? Should we expect some periods of digestion in between? Just what are\nyour customer discussions telling you about how to model growth for next year?", "char_count": 730, "word_count": 131, "token_count": 165, "exchange_id": "ex_002", "exchange_role": "question", "chunk_id": "NVDA_TRANSCRIPT_2026_Q1_u_0009", "company": "NVDA", "doc_type": "earnings_transcript", "fiscal_year": 2026, "period": "2026-Q1", "source_file": "data/earnings_calls_manual/NVDA/NVDA_FY2026_Q1.pdf", "parsed_at": "2025-11-09T21:07:08Z", "section_id": null, "section_title": null}
{"doc_id": "NVDA_TRANSCRIPT_2026_Q1", "ticker": "NVDA", "quarter": "Q1", "filing_date": "2026-03-01", "phase": "qa", "speaker_name": "Colette Kress", "speaker_role": "CFO", "speaker_firm": null, "utterance_id": "u_0010", "utterance_type": "answer", "text": "Yes, Vivek. Thanks so much for the question. Regarding H20, yes, we recognized $4.6 billion in H20 in Q1. We were unable to ship $2.5 billion, so the total for\nQ1 should have been $7 billion. When we look at our Q2, our Q2 is going to be meaningfully down in terms of China data center revenue. And we had highlighted\nin terms of the amount of orders that we had planned for H20 in Q2, and that was $8 billion. Now going forward, we did have other orders going forward that we\nwill not be able to fulfill. That is what was incorporated, therefore, in the amount that we wrote down of the $4.5 billion. That write-down was about inventory and\npurchase commitments. And our purchase commitments were about what we expected regarding the orders that we had received. Going forward, though, it's a\nbigger issue regarding the amount of the market that we will not be able to serve. We assess that TAM to be close to about $50 billion in the future, as we don't\nhave a product to enable for China.", "char_count": 991, "word_count": 185, "token_count": 241, "exchange_id": "ex_002", "exchange_role": "answer", "chunk_id": "NVDA_TRANSCRIPT_2026_Q1_u_0010", "company": "NVDA", "doc_type": "earnings_transcript", "fiscal_year": 2026, "period": "2026-Q1", "source_file": "data/earnings_calls_manual/NVDA/NVDA_FY2026_Q1.pdf", "parsed_at": "2025-11-09T21:07:08Z", "section_id": null, "section_title": null}
{"doc_id": "NVDA_TRANSCRIPT_2026_Q1", "ticker": "NVDA", "quarter": "Q1", "filing_date": "2026-03-01", "phase": "qa", "speaker_name": "Jensen Huang", "speaker_role": "CEO", "speaker_firm": null, "utterance_id": "u_0011", "utterance_type": "answer", "text": "In fact, probably the best way to think through it is that AI is several things. Of course, we know that AI is this incredible technology that's going to transform every\nindustry. You know, from, of course, the way we do software to healthcare and financial services to, you know, retail, to, I guess, every industry. Transportation,\nmanufacturing. And we're at the beginning of that. But maybe another way to think about that is where do we need intelligence? Where do we need digital\nintelligence? And it's in every country. It's in every industry. And we know because of that, we recognize that AI is also an infrastructure. It's a way of developing a\ntechnology that requires factories. And these factories produce tokens. And they, as I mentioned, are important to every single industry in every single country.\nAnd so on that basis, we're really at the very beginning of it. Because the adoption of this technology is really kind of in its early stages. Now we've reached an\nextraordinary milestone. With AIs that are reasoning, are thinking, what people call inference time scaling. You know, of course, it created a whole new we've\nentered an era where inference is going to be a significant part of the compute workload. But anyhow, it's going to be a new infrastructure, and we're building it\nout in the cloud. The United States is really the early starter and available in US clouds. And this is our largest market, our largest installed base, and we can\ncontinue to see that happening. But beyond that, we're going to have to see AI go into enterprise, which is on-prem. Because so much of the data is still on-prem,\naccess control is really important. It's really hard to move all of every company's data into the cloud. And so we're going to move AI into the enterprise. And you\nsaw that we announced a couple of really exciting new products. Our RTX Pro enterprise AI server that runs everything enterprise and AI. Our DGX Spark and\nDGX Station, which is designed for developers who want to work on-prem. And so enterprise AI is just taking off. Telcos, today, a lot of the telco infrastructure will\nbe in the future software-defined and built on AI. And so 6G is going to be built on AI. And that infrastructure needs to be built out. And as I said, it's very, very\nearly stages. And then, of course, every factory today that makes things will have an AI factory that sits with it. And the AI factory is going to be creating AI and\noperating AI for the factory itself, but also to power the products and the things that are made by the factory. So it's very clear that every company will have AI\nfactories, and very soon, there'll be robotics companies, robot companies, and those companies will be also building AIs to drive the robots. So we're at the\nbeginning of all of this build-out.\nSarah\nThe next question comes from C.J. Muse with Cantor Fitzgerald. Your line is open.\nC.J. Muse\nYeah. Good afternoon. Thank you for taking the question. There have been many large GPU cluster investments announced in the last month, and you alluded to\na few of them with Saudi Arabia, the UAE. And then also, you know, we've heard from Oracle and XAI, just to name a few. So my question is, are there others\nthat have yet to be announced of the same kind of scale and magnitude? And perhaps more importantly, how are these orders impacting your lead times for\nBlackwell and your current visibility sitting here today, you know, almost halfway through 2025?", "char_count": 3464, "word_count": 614, "token_count": 778, "exchange_id": null, "exchange_role": null, "chunk_id": "NVDA_TRANSCRIPT_2026_Q1_u_0011", "company": "NVDA", "doc_type": "earnings_transcript", "fiscal_year": 2026, "period": "2026-Q1", "source_file": "data/earnings_calls_manual/NVDA/NVDA_FY2026_Q1.pdf", "parsed_at": "2025-11-09T21:07:08Z", "section_id": null, "section_title": null}
{"doc_id": "NVDA_TRANSCRIPT_2026_Q1", "ticker": "NVDA", "quarter": "Q1", "filing_date": "2026-03-01", "phase": "qa", "speaker_name": "Jensen Huang", "speaker_role": "CEO", "speaker_firm": null, "utterance_id": "u_0012", "utterance_type": "answer", "text": "Well, we have more orders today than we did at the last time I spoke about orders at GTC. However, we're also increasing our supply chain and building out our\nsupply chain. They're doing a fantastic job. We're building it here onshore in the United States. But we're going to keep our supply chain quite busy for several\nmany more years coming. And with respect to further announcements, I'm going to be on the road next week through Europe. And it's just about every country\nneeds to build out AI infrastructure, and there are umpteen AI factories being planned. I think in the remarks, Colette mentioned there's some 800 AI factories\nbeing built. There's a whole bunch that haven't been announced. And I think the important concept here, which makes it easier to understand, is that like other\ntechnologies that impact literally every single industry, of course, electricity was one, and it became infrastructure. Of course, the information infrastructure, which\nwe now know as the Internet, affects every single industry, every country, every society. Intelligence is surely one of those things. I don't know any company,\nindustry, country who thinks that intelligence is optional. It's essential infrastructure. And so we've now digitalized intelligence. And so I think we're clearly in the\nbeginning of the build-out of this infrastructure. And every country will have it. I'm certain of that. Every industry will use it. That, I'm certain of. And what's unique\nabout this infrastructure is that it needs factories. You know, it's a little bit like the energy infrastructure. Electricity. It needs factories. We need factories to\nproduce this intelligence. And the intelligence is getting more sophisticated. We were talking about earlier that we had a huge breakthrough in the last couple of\nyears with reasoning AI, and now there are agents that reason. And there are super agents that use a whole bunch of tools, and then there's clusters of super\nagents where agents are working with agents solving problems. So you could just imagine compared to one-shot chatbots, and the agents that are now using AI,\nbuilt on these large language models, much more compute-intensive, they really need to be in R. And so I think we're in the beginning of the build-out. And there\nshould be many, many more announcements in the future.\nSarah\nYour next question comes from Ben Reitzes with Melius. Your line is open.", "char_count": 2409, "word_count": 399, "token_count": 507, "exchange_id": null, "exchange_role": null, "chunk_id": "NVDA_TRANSCRIPT_2026_Q1_u_0012", "company": "NVDA", "doc_type": "earnings_transcript", "fiscal_year": 2026, "period": "2026-Q1", "source_file": "data/earnings_calls_manual/NVDA/NVDA_FY2026_Q1.pdf", "parsed_at": "2025-11-09T21:07:08Z", "section_id": null, "section_title": null}
{"doc_id": "NVDA_TRANSCRIPT_2026_Q1", "ticker": "NVDA", "quarter": "Q1", "filing_date": "2026-03-01", "phase": "qa", "speaker_name": "Ben Reitzes", "speaker_role": "unknown", "speaker_firm": null, "utterance_id": "u_0013", "utterance_type": "statement", "text": "Yeah. Hi. Thanks for the question. I wanted to ask, you know, first to Colette, a little clarification around the guidance and maybe putting it in a different way. The\n$8 billion for H20 just seems like, you know, it's roughly $3 billion more than most people thought with regard to what you'd be foregoing in the second quarter. So\nthat would mean that, you know, with regard to your guidance, the rest of the business, you know, in order to hit $45 billion, is doing $2 to $3 billion or so better.\nSo, you know, I was wondering if that math made sense to you. And then, you know, in terms of the guidance, that would imply the non-China business is doing a\nbit better than the Street expected. So, you know, wondering, you know, what the primary driver was there in your view. And then this second part of my question,\nyou know, Jensen, I know you guide one quarter at a time, but with regard to the AI diffusion rule being lifted and this momentum with sovereign, you know,\nthere's been times in your history where you've said on calls like this where you know, you have more conviction in sequential growth throughout the year,\netcetera. And given the unleashing of demand with AI diffusion being, you know, revoked and the supply chain increasing, you know, does the environment give\nyou more conviction in sequential growth as we go throughout the year? So first one for Colette, and then next one for Jensen. Thanks so much.", "char_count": 1431, "word_count": 259, "token_count": 325, "exchange_id": null, "exchange_role": null, "chunk_id": "NVDA_TRANSCRIPT_2026_Q1_u_0013", "company": "NVDA", "doc_type": "earnings_transcript", "fiscal_year": 2026, "period": "2026-Q1", "source_file": "data/earnings_calls_manual/NVDA/NVDA_FY2026_Q1.pdf", "parsed_at": "2025-11-09T21:07:08Z", "section_id": null, "section_title": null}
{"doc_id": "NVDA_TRANSCRIPT_2026_Q1", "ticker": "NVDA", "quarter": "Q1", "filing_date": "2026-03-01", "phase": "qa", "speaker_name": "Colette Kress", "speaker_role": "CFO", "speaker_firm": null, "utterance_id": "u_0014", "utterance_type": "answer", "text": "Thanks, Ben, for the question. When we look at our Q2 guidance and our commentary that we provided, that had the export controls not occurred, we would have\nhad orders about $8 billion for H20. That's correct. That was a possibility for what we would have had in our outlook for this quarter in Q2. So what we also have\ntalked about here is the growth that we've seen in Blackwell. Blackwell across many of our customers as well as the growth that we continue to have in terms of\nsupply that we need for our customers. So putting those together, that's where we came through with the guidance that we provided. I'm going to turn the rest\nover to Jensen to see how he wants to Thanks. Thanks.", "char_count": 691, "word_count": 129, "token_count": 158, "exchange_id": null, "exchange_role": null, "chunk_id": "NVDA_TRANSCRIPT_2026_Q1_u_0014", "company": "NVDA", "doc_type": "earnings_transcript", "fiscal_year": 2026, "period": "2026-Q1", "source_file": "data/earnings_calls_manual/NVDA/NVDA_FY2026_Q1.pdf", "parsed_at": "2025-11-09T21:07:08Z", "section_id": null, "section_title": null}
{"doc_id": "NVDA_TRANSCRIPT_2026_Q1", "ticker": "NVDA", "quarter": "Q1", "filing_date": "2026-03-01", "phase": "qa", "speaker_name": "Jensen Huang", "speaker_role": "CEO", "speaker_firm": null, "utterance_id": "u_0015", "utterance_type": "answer", "text": "Thanks, Ben. I would say compared to the beginning of the year, compared to GTC time frame, there are four positive surprises. The first positive surprise is the\nstep function demand increase of reasoning AI. I think it is fairly clear now that AI is going through exponential growth. And reasoning AI really busted through.\nConcerns about hallucination or its ability to really solve problems, and I think a lot of people are crossing that barrier and realizing how incredibly effective\nagentic AI is. And reasoning AI is. So number one is inference reasoning. And the exponential growth there. Demand growth. The second one you mentioned AI\ndiffusion. It's really terrific to see that the AI diffusion role was rescinded. President Trump wants America to win. And he also realizes that we're not the only\ncountry in the race. And he wants the United States to win. And recognizes that we have to get the American stack out to the world and have the world build on\ntop of American stacks instead of alternatives. And so AI diffusion happened. The rescinding of it happened at almost precisely the time that the countries around\nthe world are awakening to the importance of AI as an infrastructure, not just as a technology of great curiosity and great importance, but infrastructure for their\nindustries and startups and society. Just as they had to build out infrastructure for electricity and the Internet, you've got to build out infrastructure for AI. I think\nthat's an awakening. And that creates a lot of opportunity. The third is enterprise AI. Agents work. And agents are doing these agents are really quite successful.\nMuch more than generative AI, agentic AI, is game-changing. You know, agents can understand ambiguous and rather implicit instructions and are able to\nproblem solve and use tools and have memory and so on. And so I think enterprise AI is ready to take off, and it's taken us a few years to build a computing\nsystem that is able to integrate, run enterprise AI stacks, run enterprise IT stacks, but add AI to it. And this is the RTX Pro enterprise server that we announced at\nComputex just last week. And just about every major IT company has joined us, and I'm super excited about that. And so computing is one part of it. But\nremember, enterprise IT is really three pillars. It's compute, storage, and networking. And we've now put all three of them together finally, and we're going to\nmarket with that. And then lastly, industrial AI. Remember, one of the implications of the world reordering, if you will, is regions onshoring manufacturing and\nbuilding plants everywhere. In addition to AI factories, of course, there are new electronics manufacturing, chip manufacturing, being built around the world. And\nall of these new plants and these new factories are creating exactly the right time when Omniverse and AI and all the work that we're doing with robotics is\nemerging. And so this fourth pillar is quite important. Every factory will have an AI factory associated with it. And in order to create these physical AI systems, you\nreally have to train a vast amount of data. So back to more data, more training, more AIs to be created, more computers. And so these four drivers are really\nkicking into turbocharge.\nSarah\nYour next question comes from Timothy Arcuri with UBS. Your line is open.", "char_count": 3329, "word_count": 570, "token_count": 699, "exchange_id": null, "exchange_role": null, "chunk_id": "NVDA_TRANSCRIPT_2026_Q1_u_0015", "company": "NVDA", "doc_type": "earnings_transcript", "fiscal_year": 2026, "period": "2026-Q1", "source_file": "data/earnings_calls_manual/NVDA/NVDA_FY2026_Q1.pdf", "parsed_at": "2025-11-09T21:07:08Z", "section_id": null, "section_title": null}
{"doc_id": "NVDA_TRANSCRIPT_2026_Q1", "ticker": "NVDA", "quarter": "Q1", "filing_date": "2026-03-01", "phase": "qa", "speaker_name": "Timothy Arcuri", "speaker_role": "Analyst", "speaker_firm": "UBS", "utterance_id": "u_0016", "utterance_type": "question", "text": "Thanks a lot. Jensen, I wanted to ask about China. It sounds like the July guidance assumes there's no SKU replacement for the H20. But if the president wants\nthe US to win, it seems like you're going to have to be allowed to ship something into China. So I guess I had two points on that. First of all, have you been\napproved to ship a new modified version into China? And are you currently building it, but you just can't ship it in fiscal Q2? And then you were sort of run rating\n$7 to $8 billion a quarter into China. Can we get back to those sorts of quarterly run rates once you get something that you're, you know, allowed to ship back into\nChina? I think we're all trying to figure out how much to add back to our models and when. So, you know, whatever you can say there would be great. Thanks.", "char_count": 803, "word_count": 158, "token_count": 191, "exchange_id": "ex_003", "exchange_role": "question", "chunk_id": "NVDA_TRANSCRIPT_2026_Q1_u_0016", "company": "NVDA", "doc_type": "earnings_transcript", "fiscal_year": 2026, "period": "2026-Q1", "source_file": "data/earnings_calls_manual/NVDA/NVDA_FY2026_Q1.pdf", "parsed_at": "2025-11-09T21:07:08Z", "section_id": null, "section_title": null}
{"doc_id": "NVDA_TRANSCRIPT_2026_Q1", "ticker": "NVDA", "quarter": "Q1", "filing_date": "2026-03-01", "phase": "qa", "speaker_name": "Jensen Huang", "speaker_role": "CEO", "speaker_firm": null, "utterance_id": "u_0017", "utterance_type": "answer", "text": "The president has a plan. He has a vision. I trust him. With respect to our export controls, it's a set of limits. And the new set of limits pretty much makes it\nimpossible for us to reduce Hopper any further, you know, for any productive use. And so the new limits, the new limits, you know, it's kind of the end of the road\nfor Hopper. We have some limited options, and so the key is to understand the limits. The key is to understand the limits and see if we can come up with\ninteresting products that could continue to serve the Chinese market. We don't have anything at the moment. But we're considering it. We're thinking about it.\nObviously, the limits are quite stringent at the moment. And we have nothing to announce today. And when the time comes, you know, we'll engage the\nadministration and discuss that.\nSarah\nYour final question comes from the line of Aaron Rakers with Wells Fargo. Your line is open.\nJake\n\nHi. This is Jake on for Aaron. Thanks for taking the question and congrats on a great quarter. I was wondering if you could give some additional color around the\nstrength you saw within the networking business, particularly around the adoption of your Ethernet solutions at CSPs, as well as any change you're seeing in\nnetwork attach rates.", "char_count": 1264, "word_count": 228, "token_count": 281, "exchange_id": "ex_003", "exchange_role": "answer", "chunk_id": "NVDA_TRANSCRIPT_2026_Q1_u_0017", "company": "NVDA", "doc_type": "earnings_transcript", "fiscal_year": 2026, "period": "2026-Q1", "source_file": "data/earnings_calls_manual/NVDA/NVDA_FY2026_Q1.pdf", "parsed_at": "2025-11-09T21:07:08Z", "section_id": null, "section_title": null}
{"doc_id": "NVDA_TRANSCRIPT_2026_Q1", "ticker": "NVDA", "quarter": "Q1", "filing_date": "2026-03-01", "phase": "qa", "speaker_name": "Jensen Huang", "speaker_role": "CEO", "speaker_firm": null, "utterance_id": "u_0018", "utterance_type": "answer", "text": "Yeah. Thank you for that. We now have three networking platforms. Maybe four. The first one is the scale-up platform. To turn a computer into a much larger\ncomputer. Scaling up is incredibly hard to do. Scaling out is easier to do, but scaling up is hard to do. And that platform is called NVLink. And NVLink comes with\nit chips and switches and enabling spines, and it's really complicated. But anyways, that's our new platform. Scale-up platform. In addition to InfiniBand, we also\nhave SpectrumX. We've been fairly consistent that Ethernet was designed for a lot of traffic that is independent. But in the case of AI, you have a lot of computers\nworking together. And the traffic of AI is insanely bursty. Latency matters a lot because the AI is thinking. And it wants to get worked on as quickly as possible.\nAnd you've got a whole bunch of nodes working together. And so we enhanced Ethernet, added capabilities like extremely low latency, congestion control,\nadaptive routing, the type of technologies that were available only in InfiniBand to Ethernet. And as a result, we improved the utilization of Ethernet in these\nclusters. These clusters are gigantic. From as low as 50% to as high as 85%, 90%. And so the difference is if you had a cluster that's $10 billion, and you\nimproved its effectiveness by 40%, that's worth $4 billion. It's incredible. And so Spectrum X has been really quite frankly a home run in this last quarter. As we\nsaid in the prepared remarks, we added two very significant CSPs to the Spectrum X adoption. And then the last one is BlueField, which is our control plane. And\nso in those four, the control plane and network, which is used for storage, is used for security, and for many of these clusters that want to achieve isolation among\nits users, multi-tenant clusters, and still be able to use and have extremely high performance, bare metal performance, BlueField is ideal for that and is used in a\nlot of the key cases. So we have these four networking platforms. They're all growing. And we're doing really well. I'm very proud of the team. And so over to you.\nSarah\nThat is all the time we have for questions. Jensen, I will turn the call back to you.", "char_count": 2193, "word_count": 390, "token_count": 498, "exchange_id": null, "exchange_role": null, "chunk_id": "NVDA_TRANSCRIPT_2026_Q1_u_0018", "company": "NVDA", "doc_type": "earnings_transcript", "fiscal_year": 2026, "period": "2026-Q1", "source_file": "data/earnings_calls_manual/NVDA/NVDA_FY2026_Q1.pdf", "parsed_at": "2025-11-09T21:07:08Z", "section_id": null, "section_title": null}
{"doc_id": "NVDA_TRANSCRIPT_2026_Q1", "ticker": "NVDA", "quarter": "Q1", "filing_date": "2026-03-01", "phase": "qa", "speaker_name": "Jensen Huang", "speaker_role": "CEO", "speaker_firm": null, "utterance_id": "u_0019", "utterance_type": "answer", "text": "Thank you. This is the start of a powerful new wave of growth. Grace Blackwell is in full production. We're off to the races. We now have multiple significant\ngrowth engines. Inference, once the light of workload, is surging with revenue-generating AI services. AI is growing faster and will be larger than any platform\nshifts before, including the Internet, mobile, and cloud. Blackwell is built to power the full AI lifecycle, from training frontier models to running complex inference\nand reasoning agents at scale. Training demand continues to rise with breakthroughs in post-training and reinforcement learning and synthetic data generation.\nBut inference is exploding. Reasoning AI agents require orders of magnitude more compute. The foundations of our next growth platforms are in place and ready\nto scale. Sovereign AI, nations are investing in AI infrastructure like they once did for electricity and the Internet. Enterprise AI must be deployable on-prem and\nintegrated with existing IT. Our RTX Pro, DGX Spark, and DGX Station Enterprise AI systems are ready to modernize the $500 billion IT infrastructure on-prem or\nin the cloud. Every major IT provider is partnering with us. Industrial AI, from training to digital twin simulation to deployment, NVIDIA Corporation's Omniverse\nand Isaac Groot are powering next-generation factories and humanoid robotic systems worldwide. The age of AI is here. From AI infrastructures, inference at\nscale, sovereign AI, enterprise AI, and industrial AI, NVIDIA Corporation is ready. Join us at GTC Paris. I'll keynote at Viva Tech on June 11, talking about\nquantum GPU computing, robotic factories, and robots, and celebrate our partnerships building AI factories across the region. The NVIDIA Corporation van will\ntour France, the UK, Germany, and Belgium. Thank you for joining us at the earnings call today. See you in Paris.\nSarah\nThis concludes today's conference call. You may now disconnect.", "char_count": 1948, "word_count": 304, "token_count": 398, "exchange_id": null, "exchange_role": null, "chunk_id": "NVDA_TRANSCRIPT_2026_Q1_u_0019", "company": "NVDA", "doc_type": "earnings_transcript", "fiscal_year": 2026, "period": "2026-Q1", "source_file": "data/earnings_calls_manual/NVDA/NVDA_FY2026_Q1.pdf", "parsed_at": "2025-11-09T21:07:08Z", "section_id": null, "section_title": null}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FinSearch-AI: RAG Pipeline Experimentation\n",
    "## Example notebook using the new data science-centric structure\n",
    "\n",
    "This notebook demonstrates how to use the simplified structure for RAG experimentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup and imports\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add src to path\n",
    "sys.path.insert(0, str(Path.cwd() / 'src'))\n",
    "\n",
    "# Import our modules\n",
    "from finsearch.config import settings\n",
    "from finsearch.data import loader, normalizer\n",
    "from finsearch.features import chunker, embedder\n",
    "from finsearch.models import retriever, reranker, generator\n",
    "from finsearch.evaluation import metrics, visualize\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load and Explore Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration\n",
    "config = settings.load_config('configs/default.yaml')\n",
    "\n",
    "# Load raw data\n",
    "data_loader = loader.DataLoader(config.data.raw_path)\n",
    "documents = data_loader.load_company_documents('AAPL')\n",
    "\n",
    "print(f\"Loaded {len(documents)} documents for AAPL\")\n",
    "print(f\"Document types: {set(doc.metadata['doc_type'] for doc in documents)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick data exploration\n",
    "df = pd.DataFrame([doc.metadata for doc in documents])\n",
    "df['year'] = pd.to_datetime(df['date']).dt.year\n",
    "\n",
    "# Visualize document distribution\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "df['doc_type'].value_counts().plot(kind='bar', ax=ax1)\n",
    "ax1.set_title('Documents by Type')\n",
    "\n",
    "df.groupby('year')['doc_type'].count().plot(ax=ax2)\n",
    "ax2.set_title('Documents by Year')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Test Different Chunking Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare chunking strategies\n",
    "strategies = ['semantic', 'window', 'sentence']\n",
    "chunk_results = {}\n",
    "\n",
    "sample_doc = documents[0]  # Take first document as example\n",
    "\n",
    "for strategy in strategies:\n",
    "    chunker_instance = chunker.DocumentChunker(\n",
    "        strategy=strategy,\n",
    "        chunk_size=config.data.chunk_size,\n",
    "        overlap=config.data.chunk_overlap\n",
    "    )\n",
    "    \n",
    "    chunks = chunker_instance.chunk(sample_doc.content)\n",
    "    chunk_results[strategy] = {\n",
    "        'count': len(chunks),\n",
    "        'avg_size': np.mean([len(c.text) for c in chunks]),\n",
    "        'chunks': chunks\n",
    "    }\n",
    "\n",
    "# Display results\n",
    "chunk_df = pd.DataFrame(chunk_results).T\n",
    "print(\"Chunking Strategy Comparison:\")\n",
    "print(chunk_df[['count', 'avg_size']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Embedding Generation and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize embedder\n",
    "embedding_model = embedder.Embedder(model_name=config.embeddings.model)\n",
    "\n",
    "# Generate embeddings for chunks\n",
    "best_strategy = 'semantic'  # Based on above analysis\n",
    "chunks = chunk_results[best_strategy]['chunks']\n",
    "\n",
    "# Batch embedding generation\n",
    "embeddings = embedding_model.embed_batch(\n",
    "    [c.text for c in chunks[:100]],  # First 100 for demo\n",
    "    batch_size=config.embeddings.batch_size\n",
    ")\n",
    "\n",
    "print(f\"Generated {len(embeddings)} embeddings\")\n",
    "print(f\"Embedding dimension: {embeddings[0].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize embedding space (t-SNE)\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "embeddings_2d = tsne.fit_transform(embeddings)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], alpha=0.5)\n",
    "plt.title('Document Chunk Embeddings (t-SNE)')\n",
    "plt.xlabel('Component 1')\n",
    "plt.ylabel('Component 2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Retrieval Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup hybrid retriever\n",
    "hybrid_retriever = retriever.HybridRetriever(\n",
    "    embedder=embedding_model,\n",
    "    use_hybrid=config.retrieval.use_hybrid,\n",
    "    dense_weight=config.retrieval.dense_weight,\n",
    "    sparse_weight=config.retrieval.sparse_weight\n",
    ")\n",
    "\n",
    "# Load or create index\n",
    "hybrid_retriever.build_index(chunks)\n",
    "\n",
    "# Test queries\n",
    "test_queries = [\n",
    "    \"What was Apple's revenue growth in the last quarter?\",\n",
    "    \"Describe the company's AI initiatives\",\n",
    "    \"What are the main risk factors?\"\n",
    "]\n",
    "\n",
    "retrieval_results = {}\n",
    "for query in test_queries:\n",
    "    results = hybrid_retriever.retrieve(query, k=config.retrieval.top_k)\n",
    "    retrieval_results[query] = results\n",
    "    print(f\"\\nQuery: {query}\")\n",
    "    print(f\"Retrieved {len(results)} documents\")\n",
    "    print(f\"Top result: {results[0].text[:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Reranking Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize reranker\n",
    "reranker_model = reranker.Reranker(\n",
    "    model_name=config.reranking.model,\n",
    "    device='cuda' if torch.cuda.is_available() else 'cpu'\n",
    ")\n",
    "\n",
    "# Rerank results for each query\n",
    "reranked_results = {}\n",
    "for query, initial_results in retrieval_results.items():\n",
    "    reranked = reranker_model.rerank(\n",
    "        query=query,\n",
    "        documents=initial_results,\n",
    "        top_k=config.reranking.top_k\n",
    "    )\n",
    "    reranked_results[query] = reranked\n",
    "    \n",
    "    print(f\"\\nQuery: {query}\")\n",
    "    print(f\"Top reranked result: {reranked[0].text[:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load evaluation dataset\n",
    "eval_dataset = pd.read_json('data/processed/benchmark.json', lines=True)\n",
    "\n",
    "# Run evaluation\n",
    "evaluator = metrics.RAGEvaluator(\n",
    "    retriever=hybrid_retriever,\n",
    "    reranker=reranker_model,\n",
    "    metrics=config.evaluation.metrics\n",
    ")\n",
    "\n",
    "results = evaluator.evaluate(\n",
    "    queries=eval_dataset['query'].tolist(),\n",
    "    relevant_docs=eval_dataset['relevant_docs'].tolist(),\n",
    "    k_values=config.evaluation.k_values\n",
    ")\n",
    "\n",
    "# Display results\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\nEvaluation Results:\")\n",
    "print(results_df.groupby('k')[['precision', 'recall', 'mrr', 'ndcg']].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize performance\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "metrics_to_plot = ['precision', 'recall', 'mrr', 'ndcg']\n",
    "for ax, metric in zip(axes.flat, metrics_to_plot):\n",
    "    results_df.groupby('k')[metric].mean().plot(ax=ax, marker='o')\n",
    "    ax.set_title(f'{metric.upper()} @ K')\n",
    "    ax.set_xlabel('K')\n",
    "    ax.set_ylabel(metric.capitalize())\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Generation with RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize generator\n",
    "llm = generator.LLMGenerator(\n",
    "    model_name=config.generation.model,\n",
    "    temperature=config.generation.temperature\n",
    ")\n",
    "\n",
    "# Full RAG pipeline\n",
    "def rag_pipeline(query: str) -> str:\n",
    "    # Retrieve\n",
    "    retrieved = hybrid_retriever.retrieve(query, k=config.retrieval.top_k)\n",
    "    \n",
    "    # Rerank\n",
    "    reranked = reranker_model.rerank(query, retrieved, top_k=config.reranking.top_k)\n",
    "    \n",
    "    # Generate\n",
    "    context = \"\\n\\n\".join([doc.text for doc in reranked])\n",
    "    response = llm.generate(\n",
    "        query=query,\n",
    "        context=context,\n",
    "        max_tokens=config.generation.max_tokens\n",
    "    )\n",
    "    \n",
    "    return response\n",
    "\n",
    "# Test the pipeline\n",
    "test_query = \"What are Apple's main revenue streams and their growth rates?\"\n",
    "response = rag_pipeline(test_query)\n",
    "\n",
    "print(f\"Query: {test_query}\\n\")\n",
    "print(f\"Response:\\n{response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Experiment Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log experiment results\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "experiment_results = {\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    'config': {\n",
    "        'chunking': best_strategy,\n",
    "        'embedding_model': config.embeddings.model,\n",
    "        'retrieval': {\n",
    "            'hybrid': config.retrieval.use_hybrid,\n",
    "            'weights': [config.retrieval.dense_weight, config.retrieval.sparse_weight]\n",
    "        },\n",
    "        'reranking_model': config.reranking.model\n",
    "    },\n",
    "    'metrics': results_df.groupby('k')[metrics_to_plot].mean().to_dict()\n",
    "}\n",
    "\n",
    "# Save to experiments folder\n",
    "exp_file = f\"experiments/results/exp_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "# with open(exp_file, 'w') as f:\n",
    "#     json.dump(experiment_results, f, indent=2)\n",
    "\n",
    "print(\"Experiment logged:\")\n",
    "print(json.dumps(experiment_results, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Compare Multiple Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter sweep\n",
    "configs_to_test = [\n",
    "    {'dense_weight': 0.5, 'sparse_weight': 0.5},\n",
    "    {'dense_weight': 0.7, 'sparse_weight': 0.3},\n",
    "    {'dense_weight': 0.3, 'sparse_weight': 0.7},\n",
    "    {'dense_weight': 1.0, 'sparse_weight': 0.0},  # Dense only\n",
    "    {'dense_weight': 0.0, 'sparse_weight': 1.0},  # Sparse only\n",
    "]\n",
    "\n",
    "sweep_results = []\n",
    "\n",
    "for cfg in configs_to_test:\n",
    "    # Update retriever config\n",
    "    hybrid_retriever.dense_weight = cfg['dense_weight']\n",
    "    hybrid_retriever.sparse_weight = cfg['sparse_weight']\n",
    "    \n",
    "    # Run quick evaluation\n",
    "    # ... evaluation code ...\n",
    "    \n",
    "    print(f\"Config: {cfg} - MRR@5: {0.75 + np.random.random()*0.1:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "This notebook demonstrates the simplified workflow with the new structure:\n",
    "\n",
    "1. **Clear imports** from organized modules\n",
    "2. **Easy experimentation** with different components\n",
    "3. **Integrated evaluation** pipeline\n",
    "4. **Experiment tracking** built-in\n",
    "5. **Visualization** of results\n",
    "\n",
    "The data science-centric structure makes it much easier to:\n",
    "- Test different configurations\n",
    "- Track experiments\n",
    "- Share reproducible research\n",
    "- Iterate quickly on improvements"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
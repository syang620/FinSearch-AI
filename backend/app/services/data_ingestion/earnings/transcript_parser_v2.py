"""
Earnings Call Transcript Parser V2

Parses earnings call transcripts from PDF to normalized JSONL format with:
- Phase detection (prepared remarks vs Q&A)
- Speaker identification and role classification
- Utterance-level units with Q&A pairing
- Token counting for LLM context management
"""

import logging
import re
from pathlib import Path
from typing import List, Dict, Optional, Tuple
from datetime import datetime
import json

try:
    from pypdf import PdfReader
except ImportError:
    from PyPDF2 import PdfReader

try:
    import tiktoken
except ImportError:
    tiktoken = None
    logging.warning("tiktoken not installed - token counts will be estimated")

# Import metadata schema helpers
import sys
import os
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..'))
from metadata_schema import compute_period, compute_chunk_id, get_current_timestamp

logger = logging.getLogger(__name__)


class TranscriptParserV2:
    """Parser for earnings call transcripts with speaker and Q&A detection"""

    # Common analyst firms for role detection
    ANALYST_FIRMS = [
        'Goldman Sachs', 'Morgan Stanley', 'JP Morgan', 'JPMorgan', 'Bank of America',
        'Citigroup', 'Wells Fargo', 'Barclays', 'UBS', 'Credit Suisse', 'Deutsche Bank',
        'Piper Sandler', 'Raymond James', 'Evercore', 'Jefferies', 'Stifel',
        'Oppenheimer', 'Cowen', 'Bernstein', 'RBC', 'BMO', 'Mizuho', 'Needham'
    ]

    # Executive role patterns
    EXECUTIVE_ROLES = {
        'CEO': r'\b(?:CEO|Chief Executive Officer)\b',
        'CFO': r'\b(?:CFO|Chief Financial Officer)\b',
        'COO': r'\b(?:COO|Chief Operating Officer)\b',
        'CTO': r'\b(?:CTO|Chief Technology Officer)\b',
        'President': r'\bPresident\b',
        'Chairman': r'\bChairman\b',
    }

    def __init__(self):
        """Initialize parser"""
        # Initialize tokenizer if available
        if tiktoken:
            self.encoding = tiktoken.get_encoding("cl100k_base")
        else:
            self.encoding = None

    def parse_transcript_to_jsonl(
        self,
        pdf_path: str,
        ticker: str,
        year: int,
        quarter: int
    ) -> Tuple[List[Dict], str]:
        """
        Parse earnings call transcript to JSONL format

        Args:
            pdf_path: Path to PDF file
            ticker: Stock ticker symbol
            year: Fiscal year
            quarter: Fiscal quarter (1-4)

        Returns:
            Tuple of (utterance_units, markdown_export)
        """
        logger.info(f"Parsing transcript: {pdf_path}")

        # Extract text from PDF
        text = self._extract_text_from_pdf(pdf_path)

        if not text:
            logger.error(f"No text extracted from PDF: {pdf_path}")
            return [], ""

        # Clean text
        cleaned_text = self._clean_text(text)

        # Detect phases (prepared remarks vs Q&A)
        phases = self._detect_phases(cleaned_text)

        # Extract and classify speakers
        speakers = self._extract_speakers(cleaned_text)
        speakers_with_roles = self._classify_speaker_roles(speakers, cleaned_text)

        # Parse utterances
        utterances = self._parse_utterances(
            cleaned_text,
            phases,
            speakers_with_roles,
            ticker,
            year,
            quarter,
            pdf_path  # Pass source file path
        )

        # Create Q&A exchanges
        utterances = self._create_qa_exchanges(utterances)

        # Generate markdown export
        markdown_export = self._generate_markdown_export(
            utterances,
            ticker,
            year,
            quarter
        )

        logger.info(
            f"Parsed transcript: {len(utterances)} utterances, "
            f"{len(speakers_with_roles)} speakers, "
            f"{len([u for u in utterances if u.get('exchange_id')])} Q&A exchanges"
        )

        return utterances, markdown_export

    def _extract_text_from_pdf(self, pdf_path: str) -> Optional[str]:
        """Extract all text from PDF"""
        try:
            reader = PdfReader(pdf_path)
            text_parts = []
            for page in reader.pages:
                try:
                    text = page.extract_text()
                    if text:
                        text_parts.append(text)
                except Exception as e:
                    logger.warning(f"Error extracting page: {e}")
                    continue
            return '\n\n'.join(text_parts) if text_parts else None
        except Exception as e:
            logger.error(f"Error reading PDF: {e}")
            return None

    def _clean_text(self, text: str) -> str:
        """Clean and normalize extracted text"""
        # Remove boilerplate headers
        boilerplate_patterns = [
            r'Generated by discountingcashflows\.com\s*\n',
            r'Date:.*?\d{4}\s*\n',
            r'\w+ Earnings Call – FY\d{4} Q\d\s*\n',
        ]
        for pattern in boilerplate_patterns:
            text = re.sub(pattern, '', text, flags=re.IGNORECASE)

        # Remove excessive whitespace
        text = re.sub(r'\n\s*\n', '\n\n', text)

        # Remove page numbers
        text = re.sub(r'\n\d+\n', '\n', text)

        # Normalize line breaks
        text = text.replace('\r\n', '\n').replace('\r', '\n')

        return text.strip()

    def _detect_phases(self, text: str) -> Dict[str, Tuple[int, int]]:
        """Detect presentation vs Q&A section boundaries"""
        phases = {}

        # Q&A section patterns
        qa_patterns = [
            r'(?:Question|Q)[ -]?(?:and|&)[ -]?(?:Answer|A)',
            r'(?:Operator|Moderator)\s*\n',
            r'(?:questions? (?:and )?answers?)',
            r'(?:open(?:ing)? (?:the )?call (?:to|for) questions)',
        ]

        qa_start = None
        for pattern in qa_patterns:
            match = re.search(pattern, text, re.IGNORECASE)
            if match:
                qa_start = match.start()
                break

        if qa_start:
            phases['prepared_remarks'] = (0, qa_start)
            phases['qa'] = (qa_start, len(text))
        else:
            # No Q&A detected, assume entire transcript is prepared remarks
            phases['prepared_remarks'] = (0, len(text))

        return phases

    def _extract_speakers(self, text: str) -> Dict[str, str]:
        """Extract speaker names from transcript"""
        speakers = {}

        # Pattern for speaker names
        speaker_pattern = r'^([A-Z][a-z]+(?: [A-Z][a-z]+)+)(?:,.*?)?\n'

        matches = re.finditer(speaker_pattern, text, re.MULTILINE)

        for match in matches:
            original_name = match.group(1).strip()
            normalized_name = self._normalize_name(original_name)

            if original_name not in speakers:
                speakers[original_name] = normalized_name

        return speakers

    def _normalize_name(self, name: str) -> str:
        """Normalize a person's name"""
        # Remove common titles
        titles = ['Mr.', 'Mrs.', 'Ms.', 'Dr.', 'Prof.']
        normalized = name
        for title in titles:
            normalized = normalized.replace(title, '').strip()

        # Clean up extra spaces
        normalized = re.sub(r'\s+', ' ', normalized)

        return normalized.strip()

    def _classify_speaker_roles(
        self,
        speakers: Dict[str, str],
        text: str
    ) -> Dict[str, Dict]:
        """Classify speaker roles (CEO, CFO, Analyst, Operator)"""
        speakers_with_roles = {}

        for original_name, normalized_name in speakers.items():
            role = None
            firm = None

            # Find context around speaker name
            pattern = re.escape(original_name) + r'[^\n]{0,200}'
            matches = re.finditer(pattern, text, re.IGNORECASE)

            context = ""
            for match in matches:
                context += match.group(0) + " "
                if len(context) > 500:
                    break

            # Check for executive roles
            for role_name, role_pattern in self.EXECUTIVE_ROLES.items():
                if re.search(role_pattern, context, re.IGNORECASE):
                    role = role_name
                    break

            # Check for analyst firms
            if not role:
                for firm_name in self.ANALYST_FIRMS:
                    if firm_name.lower() in context.lower():
                        role = 'Analyst'
                        firm = firm_name
                        break

            # Check for operator/moderator
            if not role and re.search(r'\b(?:Operator|Moderator)\b', context, re.IGNORECASE):
                role = 'Operator'

            # Default role
            if not role:
                role = 'Unknown'

            speakers_with_roles[normalized_name] = {
                'original_name': original_name,
                'normalized_name': normalized_name,
                'role': role,
                'firm': firm
            }

        return speakers_with_roles

    def _parse_utterances(
        self,
        text: str,
        phases: Dict[str, Tuple[int, int]],
        speakers: Dict[str, Dict],
        ticker: str,
        year: int,
        quarter: int,
        source_file: str
    ) -> List[Dict]:
        """Parse utterances from text"""
        utterances = []
        utterance_id = 0

        # Compute standardized fields
        parsed_at = get_current_timestamp()
        fiscal_year = year
        quarter_str = f"Q{quarter}"
        period = compute_period(fiscal_year, quarter_str)
        filing_date = f"{year}-{quarter * 3:02d}-01"  # Approximate
        doc_id = f"{ticker}_TRANSCRIPT_{year}_{quarter_str}"

        # Create speaker lookup by original name
        speaker_lookup = {info['original_name']: name for name, info in speakers.items()}

        # Pattern to find speaker boundaries
        speaker_pattern = r'^([A-Z][a-z]+(?: [A-Z][a-z]+)+)(?:,.*?)?\n'

        # Find all speaker positions
        speaker_matches = list(re.finditer(speaker_pattern, text, re.MULTILINE))

        for i, match in enumerate(speaker_matches):
            speaker_original = match.group(1).strip()
            speaker_normalized = speaker_lookup.get(speaker_original)

            if not speaker_normalized:
                continue

            # Get speaker info
            speaker_info = speakers.get(speaker_normalized, {})

            # Find utterance text (from after speaker name to next speaker or end)
            start = match.end()
            end = speaker_matches[i + 1].start() if i + 1 < len(speaker_matches) else len(text)
            utterance_text = text[start:end].strip()

            if not utterance_text or len(utterance_text) < 10:
                continue

            # Determine phase
            phase = None
            for phase_name, (phase_start, phase_end) in phases.items():
                if phase_start <= match.start() < phase_end:
                    phase = phase_name
                    break

            # Determine utterance type
            utterance_type = 'statement'
            if phase == 'qa':
                if speaker_info.get('role') == 'Analyst':
                    utterance_type = 'question' if '?' in utterance_text else 'question'
                elif speaker_info.get('role') in ['CEO', 'CFO', 'COO', 'CTO', 'President']:
                    utterance_type = 'answer'

            # Count tokens
            token_count = self._count_tokens(utterance_text)

            # Compute chunk_id
            chunk_id = compute_chunk_id(doc_id, utterance_id, 'utterance')
            utt_id = f"u_{utterance_id:04d}"

            # Create utterance unit with unified metadata schema
            utterance_unit = {
                # Core identifiers
                'doc_id': doc_id,
                'chunk_id': chunk_id,

                # Company information
                'ticker': ticker,
                'company': ticker,  # Use ticker as company name

                # Document type and period
                'doc_type': 'earnings_transcript',
                'fiscal_year': fiscal_year,
                'quarter': quarter_str,
                'period': period,
                'filing_date': filing_date,

                # Section/structure (null for transcripts)
                'section_id': None,
                'section_title': None,

                # Chunk information
                'unit_type': 'utterance',
                'unit_index': utterance_id,
                'text': utterance_text,
                'char_count': len(utterance_text),
                'word_count': len(utterance_text.split()),

                # Source tracking
                'source_file': source_file,
                'parsed_at': parsed_at,

                # Transcript-specific fields
                'phase': phase,
                'speaker_name': speaker_normalized,
                'speaker_role': speaker_info.get('role', 'unknown'),
                'speaker_firm': speaker_info.get('firm'),
                'utterance_id': utt_id,
                'utterance_type': utterance_type,
                'token_count': token_count,
                'exchange_id': None,  # Will be populated by _create_qa_exchanges
                'exchange_role': None
            }

            utterances.append(utterance_unit)
            utterance_id += 1

        return utterances

    def _count_tokens(self, text: str) -> int:
        """Count tokens in text using tiktoken"""
        if self.encoding:
            return len(self.encoding.encode(text))
        else:
            # Rough estimate: 1 token ≈ 4 characters
            return len(text) // 4

    def _create_qa_exchanges(self, utterances: List[Dict]) -> List[Dict]:
        """Group Q&A pairs into exchanges"""
        exchange_id = 0
        in_exchange = False
        current_exchange = None

        for utterance in utterances:
            if utterance['phase'] != 'qa':
                continue

            # Start new exchange on question
            if utterance['utterance_type'] == 'question':
                exchange_id += 1
                current_exchange = f"ex_{exchange_id:03d}"
                utterance['exchange_id'] = current_exchange
                utterance['exchange_role'] = 'question'
                in_exchange = True

            # Continue exchange with answer
            elif utterance['utterance_type'] == 'answer' and in_exchange:
                utterance['exchange_id'] = current_exchange
                utterance['exchange_role'] = 'answer'
                in_exchange = False  # Exchange complete

        return utterances

    def _generate_markdown_export(
        self,
        utterances: List[Dict],
        ticker: str,
        year: int,
        quarter: int
    ) -> str:
        """Generate human-readable markdown export"""
        lines = []

        # Header
        lines.append(f"# {ticker} Q{quarter} {year} Earnings Call")
        lines.append("")
        lines.append(f"**Document ID**: {ticker}_{year}_Q{quarter}")
        lines.append(f"**Ticker**: {ticker}")
        lines.append(f"**Year**: {year}")
        lines.append(f"**Quarter**: Q{quarter}")
        lines.append(f"**Utterances**: {len(utterances)}")
        lines.append("")
        lines.append("---")
        lines.append("")

        # Prepared remarks
        prepared_remarks = [u for u in utterances if u['phase'] == 'prepared_remarks']
        if prepared_remarks:
            lines.append("## Prepared Remarks")
            lines.append("")

            current_speaker = None
            for utt in prepared_remarks:
                speaker_name = utt['speaker_name']
                speaker_role = utt['speaker_role']

                if speaker_name != current_speaker:
                    lines.append(f"### {speaker_name} ({speaker_role})")
                    lines.append("")
                    current_speaker = speaker_name

                lines.append(utt['text'])
                lines.append("")

        # Q&A session
        qa_utterances = [u for u in utterances if u['phase'] == 'qa']
        if qa_utterances:
            lines.append("---")
            lines.append("")
            lines.append("## Q&A Session")
            lines.append("")

            # Group by exchange
            exchanges = {}
            for utt in qa_utterances:
                exchange_id = utt.get('exchange_id')
                if exchange_id:
                    if exchange_id not in exchanges:
                        exchanges[exchange_id] = []
                    exchanges[exchange_id].append(utt)

            # Write exchanges
            for exchange_id in sorted(exchanges.keys()):
                exchange_utts = exchanges[exchange_id]

                lines.append(f"### {exchange_id.replace('ex_', 'Exchange ')}")
                lines.append("")

                for utt in exchange_utts:
                    speaker_name = utt['speaker_name']
                    speaker_role = utt['speaker_role']
                    speaker_firm = utt.get('speaker_firm')

                    if speaker_firm:
                        speaker_label = f"{speaker_name} ({speaker_firm})"
                    else:
                        speaker_label = f"{speaker_name} ({speaker_role})"

                    lines.append(f"**{speaker_label}**: {utt['text']}")
                    lines.append("")

        return '\n'.join(lines)


# Global parser instance
transcript_parser_v2 = TranscriptParserV2()

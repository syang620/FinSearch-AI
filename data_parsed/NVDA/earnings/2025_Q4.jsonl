{"doc_id": "NVDA_TRANSCRIPT_2025_Q4", "ticker": "NVDA", "quarter": "Q4", "filing_date": "2025-12-01", "phase": "qa", "speaker_name": "Stewart Stecker", "speaker_role": "unknown", "speaker_firm": null, "utterance_id": "u_0000", "utterance_type": "statement", "text": "Good afternoon, everyone, and welcome to NVIDIA Corporation's conference call for the fourth quarter of fiscal 2025. With me today from NVIDIA Corporation\nare Jensen Huang, president and chief executive officer, and Colette Kress, executive vice president and chief financial officer. I'd like to remind you that our call\nis being webcast live on NVIDIA Corporation's Investor website. Webcast will be available for replay until the conference call discuss our financial results, the first\nquarter of fiscal 2026. The content of today's call is NVIDIA Corporation's property. It can't be reproduced or transcribed without prior written consent. During this\ncall, we may make forward-looking statements based on current expectations, these are subject to a number of significant risks and uncertainties and our actual\nresults may differ materially. A discussion of factors that could affect our future financial results and business, please refer to the disclosure in today's earnings\nrelease. Our most recent forms 10-K and 10-Q, and the reports that we may file on form 8-K with the Securities and Exchange Commission. All our statements\nare made as of today, February 26, 2025, based on information currently available to us. Except as required by law, we assume no obligation to update any such\nstatements. During this call, we will discuss non-GAAP financial measures. Confined and reconciliation of these non-GAAP financial measures GAAP financial\nmeasures in our CFO commentary, which is posted on our website. With that, let me turn the call over to Colette.", "char_count": 1565, "word_count": 244, "token_count": 318, "exchange_id": null, "exchange_role": null, "chunk_id": "NVDA_TRANSCRIPT_2025_Q4_u_0000", "company": "NVDA", "doc_type": "earnings_transcript", "fiscal_year": 2025, "period": "2025-Q4", "source_file": "data/earnings_calls_manual/NVDA/NVDA_FY2025_Q4.pdf", "parsed_at": "2025-11-09T21:07:08Z", "section_id": null, "section_title": null}
{"doc_id": "NVDA_TRANSCRIPT_2025_Q4", "ticker": "NVDA", "quarter": "Q4", "filing_date": "2025-12-01", "phase": "qa", "speaker_name": "Colette Kress", "speaker_role": "CFO", "speaker_firm": null, "utterance_id": "u_0001", "utterance_type": "answer", "text": "Thanks, Stewart. Q4 was another record quarter. Revenue of $39.3 billion was up 12% sequentially and up 78% year on year. And above our outlook, of $37.5 \nbillion. For fiscal 2025, revenue was $130.5 billion. Up 114% in the prior year. Let's start with data center. Data center revenue for fiscal 2025 was $115.2 billion. \nMore than doubling from the prior year. In the fourth quarter, it is in a revenue of $35.6 billion was a record, up 16% sequentially and 93% year on year. As the \nBlackwell ramp commenced, and Hopper 200 continued to contribute growth, In Q4, Blackwell sales exceeded our expectations. We delivered $11 billion of \nBlackwell revenue to meet strong demand. This is the fastest product ramp in our company's history. Unprecedented in its speed and scale. Blackwell production \nis in full gear across multiple configurations, and we are increasing supply quickly. Expanding customer adoption. Our Q4 data center compute revenue jumped \n18% sequentially and over 2x year on year. Customers are racing to scale infrastructure to train the next generation of cutting-edge models and unlock the next \nlevel of AI capabilities. With Blackwell, it will be common for these clusters to start with 100,000 GPUs or more. Shipments have already started for multiple \ninfrastructures of this size. Post-training and model customization are fueling demand for NVIDIA Corporation infrastructure and software as developers and \nenterprises leverage techniques such as fine-tuning, reinforcement learning, and distillation to tailor models for domain-specific use cases. Hugging Face alone \nhosts over 90,000 derivatives traded from the Llama Foundation model. The scale of post-training and model customization is massive and can collectively \ndemand orders of magnitude more compute than pretraining. Our inference demand is accelerating. Driven by test time scaling and new reasoning models. Like \nOpenAI's O3, DeepSeq R1, and Grok 3. Long-thinking reasoning AI can require 100x more compute per task compared to one-shot inferences. Blackwell was \narchitected for reasoning AI inference. Blackwell supercharges reasoning AI models with up to 25x higher token throughput and 20x lower cost versus Hopper \n100. It is revolutionary. Transformer engine is built for LLM. And mixer of experts inference. And its NVLink domain delivers 14x the throughput of PCIe Gen 5. \nEnsuring the response time, throughput, and cost efficiency needed to tackle the growing complexity of inferences scale. Companies across industries are \ntapping into NVIDIA Corporation's full-stack inference platform to boost performance and slash cost. Now tripled inference throughput and cut cost by 66% using \nNVIDIA Corporation TensorRT for its screenshot feature. Perplexity sees 435 million monthly queries and reduced its inference costs 3x with NVIDIA Corporation \nTriton inference server and TensorRT LLM. Microsoft Bing achieved a 5x speedup at major TCO savings for visual search across billions of images with NVIDIA \nCorporation, TensorRT, and acceleration libraries. Blackwell has great demand for inference. Many of the early GV200 deployments are earmarked for inference. \nA first for a new architecture. Blackwell addresses the entire AI market from pretraining, post-training, to inference across clouds, to on-premise, to enterprise. Its \nprogrammable architecture accelerates every AI model and over 4,400 applications ensuring large infrastructure investments against obsolescence in rapidly \nevolving markets. Our performance and pace of innovation are unmatched. We're driven to a 200% reduction in inference cost in just the last two years. We \ndelivered the lowest TCO and the highest ROI. And full-stack optimizations for NVIDIA Corporation and our large ecosystem including 5.9 million developers \ncontinuously improve our customers' economics. In Q4, large CSPs represented about half of our data center revenue. And these sales increased nearly 2x year \non year. Large CSPs were some of the first to stand up Blackwell, with Azure, GCP, AWS, and OCI, bringing GV200 systems to cloud regions around the world to \nmeet surging customer demand for AI. Regional cloud hosting NVIDIA Corporation GPUs increased as a percentage of data center revenue. Reflecting continued \nAI factory build-outs globally and rapidly rising demand for AI reasoning models and agents. Coreweave launched a 100,000 GV200 cluster-based instance with \nNVLink switch and Quantum-2 InfiniBand. Consumer Internet revenue grew 3x year on year. Driven by an expanding set of generative AI and deep learning use \ncases. These include recommender systems, vision language understanding, synthetic data generation search, and agentic AI. For example, XAI is adopting the \nGV200 to train and inference its next generation of Grok AI models. Meta's cutting-edge Andromeda, advertising engine runs on NVIDIA Corporation's Grace \nHopper Superchip. Serving vast quantities of ads across Instagram, Facebook applications. Andromeda harnesses Grace Hopper's fast interconnect and large \nmemory to boost inference, throughput by 3x. Enhanced ad personalization, and deliver meaningful jumps in monetization and ROI. Enterprise revenue increased \nnearly 2x year on accelerating demand model fine-tuning. Agentic AI workflows. And GPU-accelerated data processing. We introduced NVIDIA Corporation \nLlama Numitron model family nodes to help developers create and deploy AI agents across a range of applications, including customer support, fraud detection, \nand product supply chain and inventory management. Leading AI agent platform providers, including SAP and ServiceNow, are among the first to use new \nmodels. Health care leaders, IQVIA, and Lumenon. And Mayo Clinic as well as ARC and Institute are using NVIDIA Corporation AI to speed drug discovery \nenhance genomic research, and pioneer advanced health care services with generative and agentic AI. As AI expands beyond the digital world, NVIDIA \nCorporation infrastructure and software platforms are increasingly being adopted to power robotics and physical AI development. One of the early and largest \nrobotics applications and autonomous vehicles were virtually every AV company is developing on NVIDIA Corporation, in the data center. NVIDIA Corporation's \nautomotive vertical revenue is expected to grow to approximately $5 billion this fiscal year. At CES, Hyundai Motor Group announced it is adopting NVIDIA \nCorporation Technologies to accelerate AV and robotics development and smart factory initiatives. Vision transformers, self-supervised learning, multimodal \nsensor fusion, and high-fidelity simulation are driving breakthroughs in AV development and will require 10x more compute. At TDX, we announced the NVIDIA\n\nCorporation Cosmos World foundation model platform. Just as language foundation models have revolutionized language AI, Cosmos is a physical AI to\nrevolutionize robotics. The robotics and automotive companies, including ride-sharing giant Uber, are among the first to adopt the platform. From a geographic\nperspective, sequential growth in our data center revenue was strongest in the US, driven by the initial ramp of Blackwell. Countries across the globe are building\ntheir AI ecosystems and demand for compute infrastructure is surging. France's \u20ac200 billion AI investment and the EU's \u20ac200 billion Invest AI initiative offer a\nglimpse into the build-out that will redefine global AI infrastructure in the coming years. Now as a percentage of total data center revenue, data center sales in\nChina remained well below levels seen onset of export controls. China shipments absent any change in regulations, we believe that will remain roughly at the\ncurrent percentage. The market in China for data center solutions remained very competitive. We will continue to comply with export controls while serving our\ncustomers. Networking revenue declined 3% sequentially. Our networking attached to GPU compute systems is robust at over 75%. We are transitioning from\nsmall NVLink 8 with InfiniBand to large NVLink 72. The Spectrum X. Spectrum X and NVLink switch revenue increased and represents a major new growth\nsector. We expect networking a return to growth in Q1. AI requires a new class of networking. NVIDIA Corporation offers NVLink switch systems for scale of\ncompute. For scale out, we offer Quantum InfiniBand for HPC supercomputers, and SpectrumX for Ethernet environments. Spectrum X enhances the Ethernet for\nAI computing and has been a huge success. Microsoft Azure OCI, Fortease, and others are building large AI factories with SpectrumX. The first Stargate data\ncenters will use Spectrum X. Yesterday, Cisco announced integrating Spectrum X into their networking portfolio to help enterprises build AI infrastructure. With its\nlarge enterprise footprint and global reach, Cisco will bring NVIDIA Corporation Ethernet to every industry. Now moving to gaming and AR PCs. Gaming revenue\nof $2.5 billion decreased 22% sequentially and 11% year on year. Full year revenue of $11.4 billion increased 9% year on year. And demand remains strong\nthroughout the holiday. However, Q4 shipments were impacted by supply constraints. We expect strong sequential growth in Q1 as supply increases. The new\nGeForce RTX 50 series desktop and laptop GPUs are here. Built for gamers, creators, and developers, they fuse AI and graphics, redefining visual computing.\nPowered by the Blackwell architecture, fifth-generation tensor cores, and fourth-generation RT cores and featuring up to 3,400 AI TOPS. These GPUs deliver a\n2x performance leap and new AI-driven rendering, including neural shaders, digital human technologies, geometry, and lighting. The new DLSS 4 boosts frame\nrates up to 8x with AI-driven frame generation turning one rendered frame into three. It also features the industry's first real-time application of transformer models\npacking 2x more parameters and 4x to compute unprecedented visual fidelity. We also announced a wave of GeForce Blackwell laptop GPUs with new NVIDIA\nCorporation Max-Q technology that extends battery life, by up to an incredible 40%. These laptops will be available starting in March from the world's top\nmanufacturers. Moving to our professional visualization business. Revenue of $511 million was up 5% sequentially and 10% year on year. Full year revenue of\n$1.9 billion increased 21% year on year. Key industry verticals driving demand include automotive and health care. NVIDIA Corporation Technologies and\ngenerative AI are reshaping design engineering, and simulation workloads. Increasingly, these technologies are being leveraged in leading software platforms.\nFrom ANSYS, Cadence, and Siemens fueling demand for NVIDIA Corporation RTX workstations. Now moving to automotive. Revenue was a record $570 million,\nup 27% sequentially and up 103% year on year. Full year revenue of $1.7 billion increased 55% year on year. Strong growth was driven by the continued ramp in\nautonomous vehicles, including cars and robotaxis. At CES, we announced Toyota the world's largest automaker will build its next generation vehicles on NVIDIA", "char_count": 11124, "word_count": 1659, "token_count": 2291, "exchange_id": null, "exchange_role": null, "chunk_id": "NVDA_TRANSCRIPT_2025_Q4_u_0001", "company": "NVDA", "doc_type": "earnings_transcript", "fiscal_year": 2025, "period": "2025-Q4", "source_file": "data/earnings_calls_manual/NVDA/NVDA_FY2025_Q4.pdf", "parsed_at": "2025-11-09T21:07:08Z", "section_id": null, "section_title": null}
{"doc_id": "NVDA_TRANSCRIPT_2025_Q4", "ticker": "NVDA", "quarter": "Q4", "filing_date": "2025-12-01", "phase": "qa", "speaker_name": "Corporation Oren", "speaker_role": "unknown", "speaker_firm": null, "utterance_id": "u_0002", "utterance_type": "statement", "text": "powered by NVIDIA Corporation Drive 4. Finally, our end-to-end autonomous vehicle platform, NVIDIA Corporation DRIVE Hyperion, has passed industry safety\nassessments by Ryland, two of the industry's foremost authorities, automotive-grade safety and cybersecurity, NVIDIA Corporation is the first AV platform to\nreceive a comprehensive set of third-party assessments. Moving to the rest of the P&L. GAAP gross margins, was 73%. And non-GAAP gross margins were\n73.5%. Down sequentially as expected with our first deliveries of the Blackwell architecture. As discussed last quarter, Blackwell is a customizable AI\ninfrastructure with several different types of NVIDIA Corporation build chips. Multiple networking options, and for air and liquid-cooled data center. We exceeded\nour expectations in Q4, in ramping Blackwell, increasing system availability, providing several configurations to our customers. As Blackwell ramps, we expect\ngross margins to be in the low seventies. We initially, we are focused on expediting the manufacture as they race to build out Blackwell infrastructure. When fully\nramped, we have many opportunities to improve the cost and gross margin. Will improve and return to the mid-seventies. Late this fiscal year. Sequentially,\nGAAP operating expenses were up 9% and non-GAAP operating expenses were 11%, reflecting higher engineering development costs and higher compute and\ninfrastructure costs for new product introductions. In Q4, we returned $8.1 billion to shareholders, the form of share repurchases cash dividends. Let me turn to\nthe outlook in the first quarter. Total revenue is expected to be $43 billion. Plus or minus 2%. Continuing with its strong demand, we expect a significant ramp of\nBlackwell in Q1. We expect sequential growth. In both data center and gaming. Within data center, we expect sequential growth from both. Compute and\nnetworking. GAAP and non-GAAP gross margins are expected to be 70.6%. And 71% respectively. Plus or minus 50 basis points. GAAP and non-GAAP\noperating expenses are expected to be approximately $5.2 billion and $3.6 billion. We expect full year fiscal year 2026 operating expenses grow to grow to be in\nthe mid-thirties. GAAP and non-GAAP other incoming expenses are expected to be an income of approximately $400 million. Excluding gains and losses, from\nnon-marketable and publicly held equity securities. GAAP and non-GAAP tax rates are expected to be 17% plus or minus 1% excluding any discrete items.\nFurther financial details are included in the CFO commentary and other information available on our IR website. Including a new financial information AI agent. In\nclosing, let me highlight upcoming events for the financial community. We will be at the TD Cowen Healthcare Conference in Boston on March 3rd. And at the", "char_count": 2798, "word_count": 429, "token_count": 597, "exchange_id": null, "exchange_role": null, "chunk_id": "NVDA_TRANSCRIPT_2025_Q4_u_0002", "company": "NVDA", "doc_type": "earnings_transcript", "fiscal_year": 2025, "period": "2025-Q4", "source_file": "data/earnings_calls_manual/NVDA/NVDA_FY2025_Q4.pdf", "parsed_at": "2025-11-09T21:07:08Z", "section_id": null, "section_title": null}
{"doc_id": "NVDA_TRANSCRIPT_2025_Q4", "ticker": "NVDA", "quarter": "Q4", "filing_date": "2025-12-01", "phase": "qa", "speaker_name": "Morgan Stanley Technology", "speaker_role": "Analyst", "speaker_firm": "Morgan Stanley", "utterance_id": "u_0003", "utterance_type": "question", "text": "March 17th, in San Jose, California. Jensen will deliver a news-packed keynote on March 18th, and we will host a Q&A session for our financial analysts. Next\nday, March 19th. We look forward to seeing you at these events. Our earnings call to discuss the results for our first quarter of fiscal 2026 is scheduled for May\n28th, 2025. We are going to open up the call, operator. To questions. If you could start that, that would be great.\nChrista\nThank you. At this time, I would like I also ask that you please limit yourself to one question. For any additional questions, please requeue. And your first question\ncomes from C.J. Muse with Cantor Fitzgerald. Please go ahead.\nC.J. Muse\nYeah. Good afternoon. Thank you for taking the question. I guess, for me, Jensen, as test time compute and reinforcement learning shows such promise, we're\nclearly seeing increasing blurring in the lines between training and inference. What does this mean for the potential future of potentially inference-dedicated\nclusters? And how do you think about the overall impact to NVIDIA Corporation and your customers? Thank you.", "char_count": 1108, "word_count": 186, "token_count": 252, "exchange_id": "ex_001", "exchange_role": "question", "chunk_id": "NVDA_TRANSCRIPT_2025_Q4_u_0003", "company": "NVDA", "doc_type": "earnings_transcript", "fiscal_year": 2025, "period": "2025-Q4", "source_file": "data/earnings_calls_manual/NVDA/NVDA_FY2025_Q4.pdf", "parsed_at": "2025-11-09T21:07:08Z", "section_id": null, "section_title": null}
{"doc_id": "NVDA_TRANSCRIPT_2025_Q4", "ticker": "NVDA", "quarter": "Q4", "filing_date": "2025-12-01", "phase": "qa", "speaker_name": "Jensen Huang", "speaker_role": "CEO", "speaker_firm": null, "utterance_id": "u_0004", "utterance_type": "answer", "text": "Yeah. I appreciate that, C.J. There are now multiple scaling laws. There's the pretrained scaling laws. And that's gonna continue to scale because we have \nmultimodality. We have data that came from reasoning that are now used to pretraining. And then the second is post-training scaling law. Using reinforcement \nlearning human feedback, reinforcement learning AI feedback, reinforcement learning verifiable rewards, the amount of computation you use for post-training is \nactually higher than pretraining. And it's kinda sensible in the sense that you could while you're using reinforcement learning, generate an enormous amount of \nsynthetic data or synthetically generated tokens. AI models are basically generating tokens to train AI models. That's post-train. And the third part, this is the part\n\nthat you mentioned, is test time compute or reasoning. Long thinking, inference scaling, basically the same ideas. And there's you have chain of thought, you\nhave search. The amount of tokens generated, the amount of inference compute needed, is already a hundred times more than the one-shot examples and the\none-shot capabilities of large language models in the beginning and that's just the beginning. This is just the beginning. The idea that the next generation could\nhave thousands of times and even hopefully extremely thoughtful and simulation-based and search-based models that could be hundreds of thousands, millions\nof times more compute than today, is in our future. And so the question is how do you design such an architecture? Some of the models are autoregressive.\nSome of the models are diffusion-based. Some of the times you want your data center to have disaggregated inference. Sometimes it's compacted. And so it's\nhard to figure out what is the best configuration of a data center, which is the reason why NVIDIA Corporation's architecture is so popular. We run every model.\nWe are great at training. The vast majority of our compute today is actually inference, and Blackwell takes all of that to a new level. We designed Blackwell with\nthe idea of reasoning models in mind. And you look at training, it's many times more performant. But what's really amazing is for long-thinking, test time scaling\nreasoning AI models, we're tens of times faster, 25 times higher throughput. And so Blackwell is gonna be incredible across the board. And when you have a data\ncenter, that allows you to configure and use your data center based on are you doing more pretraining now, post-training now? Or scaling out your inference our\narchitecture is fungible, and easy to use. In all of those different ways. And so we're seeing, in fact, much, much more concentration of a unified architecture than\never before.\nChrista\nYour next question comes from the line of Joseph Moore with JPMorgan. Please go ahead.", "char_count": 2821, "word_count": 454, "token_count": 581, "exchange_id": "ex_001", "exchange_role": "answer", "chunk_id": "NVDA_TRANSCRIPT_2025_Q4_u_0004", "company": "NVDA", "doc_type": "earnings_transcript", "fiscal_year": 2025, "period": "2025-Q4", "source_file": "data/earnings_calls_manual/NVDA/NVDA_FY2025_Q4.pdf", "parsed_at": "2025-11-09T21:07:08Z", "section_id": null, "section_title": null}
{"doc_id": "NVDA_TRANSCRIPT_2025_Q4", "ticker": "NVDA", "quarter": "Q4", "filing_date": "2025-12-01", "phase": "qa", "speaker_name": "Joseph Moore", "speaker_role": "Analyst", "speaker_firm": "JPMorgan", "utterance_id": "u_0005", "utterance_type": "question", "text": "I wonder if you could talk about GV200 at CES. You sort of talked about the complexity of the rack-level systems and the challenges you have. And then as you\nsaid in the prepared remarks, we've seen a lot of general availability. You know, where are you in terms of that ramp? Are there still bottlenecks to consider at a\nsystems level above and beyond the chip level? And just you know, have you maintained your enthusiasm for the NVLink 72 platforms?", "char_count": 452, "word_count": 82, "token_count": 100, "exchange_id": "ex_002", "exchange_role": "question", "chunk_id": "NVDA_TRANSCRIPT_2025_Q4_u_0005", "company": "NVDA", "doc_type": "earnings_transcript", "fiscal_year": 2025, "period": "2025-Q4", "source_file": "data/earnings_calls_manual/NVDA/NVDA_FY2025_Q4.pdf", "parsed_at": "2025-11-09T21:07:08Z", "section_id": null, "section_title": null}
{"doc_id": "NVDA_TRANSCRIPT_2025_Q4", "ticker": "NVDA", "quarter": "Q4", "filing_date": "2025-12-01", "phase": "qa", "speaker_name": "Jensen Huang", "speaker_role": "CEO", "speaker_firm": null, "utterance_id": "u_0006", "utterance_type": "answer", "text": "Well, I'm more enthusiastic today than I was at CES. And the reason for that is because we shipped a lot more to CES. We have some 350 plants manufacturing\nthe one and a half million components that go into each one of the Blackwell racks. Base Blackwell racks. Yes. It's extremely complicated. And we successfully\nand incredibly ramped up Grace Blackwell. Delivering some $11 billion of revenues last quarter. We're gonna have to continue to scale as demand is quite high\nand customers are anxious and impatient to get their Blackwell systems. You'd probably seen on the web a fair number of celebrations about Grace Blackwell\nSystems coming online and we have them, of course. We have a fairly large installation of Grace Blackwell for our own engineering and our own design teams\nand software teams. Coreweave has now gone public about the successful bring-up of theirs. Microsoft has. Of course, OpenAI has. And you're starting to see\nmany come online. So I think the answer to your question is nothing is easy about what we're doing. But we're doing great, and all of our partners are doing great.\nChrista\nYour next question comes from the line of Vivek Arya with Bank of America Securities. Please go ahead.", "char_count": 1213, "word_count": 209, "token_count": 263, "exchange_id": "ex_002", "exchange_role": "answer", "chunk_id": "NVDA_TRANSCRIPT_2025_Q4_u_0006", "company": "NVDA", "doc_type": "earnings_transcript", "fiscal_year": 2025, "period": "2025-Q4", "source_file": "data/earnings_calls_manual/NVDA/NVDA_FY2025_Q4.pdf", "parsed_at": "2025-11-09T21:07:08Z", "section_id": null, "section_title": null}
{"doc_id": "NVDA_TRANSCRIPT_2025_Q4", "ticker": "NVDA", "quarter": "Q4", "filing_date": "2025-12-01", "phase": "qa", "speaker_name": "Vivek Arya", "speaker_role": "Analyst", "speaker_firm": "Bank of America", "utterance_id": "u_0007", "utterance_type": "question", "text": "Thank you for taking my question. Could I just you wouldn't mind confirming if Q1 is the bottom for gross margins? And then, Jensen, my question is for you.\nWhat is on your dashboard to give you the confidence that the strong demand can sustain into next year and has DeepSeq and whatever innovations they came\nup with, has that changed that view in any way? Thank you.", "char_count": 369, "word_count": 68, "token_count": 80, "exchange_id": "ex_003", "exchange_role": "question", "chunk_id": "NVDA_TRANSCRIPT_2025_Q4_u_0007", "company": "NVDA", "doc_type": "earnings_transcript", "fiscal_year": 2025, "period": "2025-Q4", "source_file": "data/earnings_calls_manual/NVDA/NVDA_FY2025_Q4.pdf", "parsed_at": "2025-11-09T21:07:08Z", "section_id": null, "section_title": null}
{"doc_id": "NVDA_TRANSCRIPT_2025_Q4", "ticker": "NVDA", "quarter": "Q4", "filing_date": "2025-12-01", "phase": "qa", "speaker_name": "Colette Kress", "speaker_role": "CFO", "speaker_firm": null, "utterance_id": "u_0008", "utterance_type": "answer", "text": "Let me first take the first part of the question. Regarding the gross margin. During our Blackwell ramp, our gross margins will be in the low seventies. At this point,\nwe are focusing on expediting our manufacturing. Expediting our manufacturing is to make sure that we can provide customers as soon as possible. Our\nBlackwell is fully ramped. And once it does, I'm sorry. Blackwell fully ramps, we can improve our cost and our gross margin. So we expect to probably be in the\nmid-seventies later this year. You know, walking through what you heard, Jensen speak about the systems and their complexity. They are customizable in some\ncases. They've got multiple networking options. Have liquid cool and water-cooled. So we know there is an opportunity for us to improve these gross margins\ngoing forward. But right now, we are gonna focus on getting the manufacturing plate into our customers as soon as possible.", "char_count": 912, "word_count": 154, "token_count": 194, "exchange_id": "ex_003", "exchange_role": "answer", "chunk_id": "NVDA_TRANSCRIPT_2025_Q4_u_0008", "company": "NVDA", "doc_type": "earnings_transcript", "fiscal_year": 2025, "period": "2025-Q4", "source_file": "data/earnings_calls_manual/NVDA/NVDA_FY2025_Q4.pdf", "parsed_at": "2025-11-09T21:07:08Z", "section_id": null, "section_title": null}
{"doc_id": "NVDA_TRANSCRIPT_2025_Q4", "ticker": "NVDA", "quarter": "Q4", "filing_date": "2025-12-01", "phase": "qa", "speaker_name": "Jensen Huang", "speaker_role": "CEO", "speaker_firm": null, "utterance_id": "u_0009", "utterance_type": "answer", "text": "We know several things, Vivek. We have a fairly good line of sight of the amount of capital investment that data centers are building out towards. We know that\ngoing forward, the vast majority of software is gonna be based on machine learning. And so accelerated computing and generative AI, reasoning AI, are going to\nbe the type of architecture you want in your data center. We have, of course, forecast and plans from our top partners. And we also know that there are many\ninnovative really exciting start-ups that are still coming online. As new opportunities for developing the next breakthroughs in AI, whether it's agentic AIs,\nreasoning AIs, or physical AIs. The number of start-ups are still quite vibrant and each one of them needs a fair amount of computing infrastructure. So I think the\nwhether it's the near-term signals or the mid-term signals. Near-term signals, of course, are, you know, POs and forecasts and things like that. Mid-term signals,\nwould be the level of infrastructure and CapEx scale out compared to previous years. And then the long-term signals it has to do with the fact that we know\nfundamentally software has changed. From hand coding that runs on CPUs through machine learning and AI-based software that runs on GPUs and accelerated\ncomputing systems. So we have a fairly good sense that this is the future of software. And then maybe as you roll it out, another way to think about that is we've\nreally only touched consumer AI and search and some amount of consumer generative AI. Advertising, recommenders, kind of the early days of software. The\nnext wave's coming. Agentic AI for enterprise, physical AI for robotics. And Sovereign AI has different regions build out their AI for their own ecosystems. And so\neach one of these are barely off the ground, and we can see them. We can see them because, you know, obviously, we're in the center of much of this\ndevelopment. And we can see great activity happening in all these different places. And these will happen. So near-term, mid-term, long-term.\nChrista\nYour next question comes from the line of Matt Ramsay with Cowen. Please go ahead.", "char_count": 2131, "word_count": 364, "token_count": 463, "exchange_id": null, "exchange_role": null, "chunk_id": "NVDA_TRANSCRIPT_2025_Q4_u_0009", "company": "NVDA", "doc_type": "earnings_transcript", "fiscal_year": 2025, "period": "2025-Q4", "source_file": "data/earnings_calls_manual/NVDA/NVDA_FY2025_Q4.pdf", "parsed_at": "2025-11-09T21:07:08Z", "section_id": null, "section_title": null}
{"doc_id": "NVDA_TRANSCRIPT_2025_Q4", "ticker": "NVDA", "quarter": "Q4", "filing_date": "2025-12-01", "phase": "qa", "speaker_name": "Matt Ramsay", "speaker_role": "Analyst", "speaker_firm": "Cowen", "utterance_id": "u_0010", "utterance_type": "question", "text": "Yeah. Good afternoon. Thanks for taking my question. Your next generation Blackwell Ultra is set to launch in the second half of this year. In line with the team's\nannual product cadence. Jensen, can you help us understand the demand dynamics for Ultra given that you'll still be ramping the current generation Blackwell\nsolutions? How do your customers and the supply chain also manage the simultaneous ramps of these two products and is the team still on track to execute\nBlackwell Ultra in the second half of this year?", "char_count": 522, "word_count": 89, "token_count": 108, "exchange_id": "ex_004", "exchange_role": "question", "chunk_id": "NVDA_TRANSCRIPT_2025_Q4_u_0010", "company": "NVDA", "doc_type": "earnings_transcript", "fiscal_year": 2025, "period": "2025-Q4", "source_file": "data/earnings_calls_manual/NVDA/NVDA_FY2025_Q4.pdf", "parsed_at": "2025-11-09T21:07:08Z", "section_id": null, "section_title": null}
{"doc_id": "NVDA_TRANSCRIPT_2025_Q4", "ticker": "NVDA", "quarter": "Q4", "filing_date": "2025-12-01", "phase": "qa", "speaker_name": "Jensen Huang", "speaker_role": "CEO", "speaker_firm": null, "utterance_id": "u_0011", "utterance_type": "answer", "text": "Yes. Blackwell Ultra is second half. As you know, the first Blackwell was have we had a hiccup? That probably cost us a couple of months. We're fully recovered,\nof course. The team did an amazing job recovery. And all of our supply chain partners and just so many people helped us recover at the speed of light. And so\nnow we've successfully ramped production of Blackwell. But that doesn't stop the next train. The next train is you know, it's on an annual rhythm. And, Blackwell\nUltra with, new networking, new memories, and, of course, new processors and all of that is coming online. We've been working with all of our partners and\ncustomers laying this out. They have all of the necessary information. And we'll work with everybody to do the proper transition. This time between Blackwell,", "char_count": 794, "word_count": 139, "token_count": 179, "exchange_id": "ex_004", "exchange_role": "answer", "chunk_id": "NVDA_TRANSCRIPT_2025_Q4_u_0011", "company": "NVDA", "doc_type": "earnings_transcript", "fiscal_year": 2025, "period": "2025-Q4", "source_file": "data/earnings_calls_manual/NVDA/NVDA_FY2025_Q4.pdf", "parsed_at": "2025-11-09T21:07:08Z", "section_id": null, "section_title": null}
{"doc_id": "NVDA_TRANSCRIPT_2025_Q4", "ticker": "NVDA", "quarter": "Q4", "filing_date": "2025-12-01", "phase": "qa", "speaker_name": "Blackwell Ultra", "speaker_role": "unknown", "speaker_firm": null, "utterance_id": "u_0012", "utterance_type": "statement", "text": "NVLink 72 base system. So the chassis, the architecture of the system, the hardware, the power delivery, all of that had to change. This was quite a challenging\ntransition. But the next transition will slot right in. Grace Blackwell Ultra will slot right in. We've also already revealed and been working very closely with all of our\npartners on the click after that. And the click after that is called Vera Rubin. And, all of our partners are getting up to speed on the transition of that. And so\npreparing for that transition and, again, we're gonna provide a big, big, huge step up. And so come to GTC, and I'll hold on to you about Blackwell Ultra, Vera\nRubin, and then show you what's the one click after that. Really, really exciting new product, so come to GTC, please.\nChrista\nYour next question comes from the line of Timothy Arcuri with UBS. Please go ahead.", "char_count": 867, "word_count": 156, "token_count": 204, "exchange_id": null, "exchange_role": null, "chunk_id": "NVDA_TRANSCRIPT_2025_Q4_u_0012", "company": "NVDA", "doc_type": "earnings_transcript", "fiscal_year": 2025, "period": "2025-Q4", "source_file": "data/earnings_calls_manual/NVDA/NVDA_FY2025_Q4.pdf", "parsed_at": "2025-11-09T21:07:08Z", "section_id": null, "section_title": null}
{"doc_id": "NVDA_TRANSCRIPT_2025_Q4", "ticker": "NVDA", "quarter": "Q4", "filing_date": "2025-12-01", "phase": "qa", "speaker_name": "Timothy Arcuri", "speaker_role": "Analyst", "speaker_firm": "UBS", "utterance_id": "u_0013", "utterance_type": "question", "text": "Thanks a lot. Jensen, we hear a lot about custom ASICs. Can you kinda speak to the balance between custom ASIC and merchant GPU? We hear about some\nof these heterogeneous super clusters to use both GPU and ASIC. Is that something customers are planning on building or will these infrastructures remain fairly\ndistinct? Thanks.", "char_count": 326, "word_count": 55, "token_count": 66, "exchange_id": "ex_005", "exchange_role": "question", "chunk_id": "NVDA_TRANSCRIPT_2025_Q4_u_0013", "company": "NVDA", "doc_type": "earnings_transcript", "fiscal_year": 2025, "period": "2025-Q4", "source_file": "data/earnings_calls_manual/NVDA/NVDA_FY2025_Q4.pdf", "parsed_at": "2025-11-09T21:07:08Z", "section_id": null, "section_title": null}
{"doc_id": "NVDA_TRANSCRIPT_2025_Q4", "ticker": "NVDA", "quarter": "Q4", "filing_date": "2025-12-01", "phase": "qa", "speaker_name": "Jensen Huang", "speaker_role": "CEO", "speaker_firm": null, "utterance_id": "u_0014", "utterance_type": "answer", "text": "Well, we build very different things than ASICs. In some ways, completely different in some areas we intercept. We're different in several ways. One, NVIDIA\nCorporation's architecture is general. You know, whether you've optimized for autoregressive models or diffusion-based models or vision-based models or\nmultimodal models or text models. We're great in all of it. We're great in all of it because our software stack is so our architecture is responsible. Our software\nstack is ecosystem is so rich that we're the initial target of, you know, most exciting innovations and algorithms. And so by definition, we're much, much more\ngeneral than narrow. We're also really good from the end to end. From data processing, the curation of the training data, to the training of the data, of course, to\nreinforcement learning used in post-training. All the way to inference with test time scaling. So, you know, we're general. We're end to end. And we're\neverywhere. And because we're not in just one cloud, we're in every cloud, we could be on-prem. We could be in, you know, in a robot. Our architecture is much\nmore accessible. And a great target initial target for anybody who's starting up a new company. And so we're everywhere. And then the third thing I would say is\nthat our performance and our rhythm is so incredibly fast. Remember that these data centers are always fixed in size. They're fixed in size or they're fixed in\npower. And if our performance per watt is anywhere from 2x to 4x to 8x, which is not unusual. It translates directly to revenues. And so if you have a\n100-megawatt data center, if the performance or the throughput that 100-megawatt or that gigawatt data center is four times or eight times higher your revenues\nfor that gigawatt data center is eight times higher. And the reason that is so different than data centers of the past is because AI factories are directly monetizable\nthrough its tokens generated. And so the token throughput of our architecture being so incredibly fast is just incredibly valuable to all of the companies that are\nbuilding these things for revenue generation reasons. And capturing the fast ROIs. So I think the third reason is performance. And then the last thing that I would\nsay is the software stack is incredibly hard. Building an ASIC is no different than what we do. We have to build a new architecture. And the ecosystem that sits on\ntop of our architecture is ten times more complex today than it was two years ago. And that's fairly obvious because the amount of software this world building on\ntop of architecture is growing exponentially and AI is advancing very quickly. So bringing that whole ecosystem on top of multiple chips is hard. And so I would\nsay that those four reasons and then finally, I will say this. Just because the chip is designed doesn't mean it gets deployed. And you've seen this over and over\nagain. There are a lot of chips that get built. But when the time comes a business decision has to be made. And that business decision is about deploying a new\nengine, a new processor into a limited AI factory in size and power and find. And our technology is, you know, not only more advanced, more performant, it has\nmuch, much better software capability, and very importantly, our ability to deploy is lightning fast. And so these things are enough for the faint of heart as\neverybody knows now. And so there's a lot of different reasons why we do well. Why we win.\nChrista\nYour next question comes from the line of Ben Reitzes with Melius Research. Please go ahead.", "char_count": 3557, "word_count": 621, "token_count": 784, "exchange_id": "ex_005", "exchange_role": "answer", "chunk_id": "NVDA_TRANSCRIPT_2025_Q4_u_0014", "company": "NVDA", "doc_type": "earnings_transcript", "fiscal_year": 2025, "period": "2025-Q4", "source_file": "data/earnings_calls_manual/NVDA/NVDA_FY2025_Q4.pdf", "parsed_at": "2025-11-09T21:07:08Z", "section_id": null, "section_title": null}
{"doc_id": "NVDA_TRANSCRIPT_2025_Q4", "ticker": "NVDA", "quarter": "Q4", "filing_date": "2025-12-01", "phase": "qa", "speaker_name": "Ben Reitzes", "speaker_role": "unknown", "speaker_firm": null, "utterance_id": "u_0015", "utterance_type": "statement", "text": "Yeah. Hi. Ben Reitzes here. Hey. Thanks a lot for the question. Hey, Jensen. It's a geography-related question. You know, you did a great job explaining some of\nthe demand underlying, you know, factors here on the strength. But the US was up about $5 billion or so sequentially. And I think, you know, there is a concern\nabout whether the US can pick up the slack if there's regulations towards other geographies. And I was just wondering as we go throughout the year, you know, if\nthis kind of surge in the US continues and it's gonna be whether that's okay. And if that underlies your growth rate, how can you keep growing so fast with this mix\nshift towards the US? Your guidance looks like China is probably up sequentially. So just wondering if you could go through that dynamic and maybe Colette can\nweigh in. Thanks a lot.", "char_count": 829, "word_count": 151, "token_count": 191, "exchange_id": null, "exchange_role": null, "chunk_id": "NVDA_TRANSCRIPT_2025_Q4_u_0015", "company": "NVDA", "doc_type": "earnings_transcript", "fiscal_year": 2025, "period": "2025-Q4", "source_file": "data/earnings_calls_manual/NVDA/NVDA_FY2025_Q4.pdf", "parsed_at": "2025-11-09T21:07:08Z", "section_id": null, "section_title": null}
{"doc_id": "NVDA_TRANSCRIPT_2025_Q4", "ticker": "NVDA", "quarter": "Q4", "filing_date": "2025-12-01", "phase": "qa", "speaker_name": "Jensen Huang", "speaker_role": "CEO", "speaker_firm": null, "utterance_id": "u_0016", "utterance_type": "answer", "text": "China is approximately the same percentage as Q4. And as in as previous quarters. It's about half of what it was before the export control. But it's approximately\nthe same in percentage. With respect to geographies, the takeaway is that AI is software. It's modern software. It's incredible modern software. But it's modern\nsoftware. And AI has gone mainstream. AI is used in delivery services everywhere, shopping services everywhere. You know? You were to buy a quart of milk is\ndelivered to you. AI was involved. And so almost everything that a consumer service provides AI's at the core of it. Every student will use AI as a tutor. Health\ncare services use AI. Financial services use AI. No fintech company will not use AI. Every fintech company will. Climate tech company uses AI. Mineral Discovery\nnow uses AI. The number of every higher education, every university, uses AI. So I think it is fairly safe to say that AI has gone mainstream. And that it's being\nintegrated into every application. And our hope is that, of course, the technology continues to advance safely and advance in a helpful way to our society. And\nwith that, you know, we're I do believe that we're at the beginning of this new transition. And what I mean by that in the beginning, is remember behind us has\nbeen decades of data centers and decades of computers that have been built. And they've been built for a world of hand coding and general-purpose computing.\nAnd CPUs and so on and so forth. And going forward, I think it's fairly safe to say that that world is going to be almost all software will be infused with AI. All\nsoftware and all services will be based on ultimately based on machine learning, and the data flywheel is gonna part of improving software and services. And that\nthe future computers will be accelerated. The future computers will be based on AI. And we're really three years into that journey. And in modernizing computers\nthat have taken decades to build out. And so I'm fairly sure that we're in the beginning of this new era. And then lastly, no technology has ever had the opportunity\nto address a larger part of the world's GDP than AI. No software tool ever has. And so this is now a software tool that can address a much larger part of the\nworld's GDP, more than any time in history. And so the way we think about growth and the way we think about whether something is big or small. Has to be in the\ncontext of that. And when you take a step back and look at it from that perspective, we're really just in the beginnings.\nChrista\nYour next question comes from the line of Aaron Rakers with Wells Fargo. Please go ahead. Erin, your line is open. Your next question comes from Mark Lipacis\nwith Evercore ISI. Please go ahead.", "char_count": 2738, "word_count": 493, "token_count": 603, "exchange_id": null, "exchange_role": null, "chunk_id": "NVDA_TRANSCRIPT_2025_Q4_u_0016", "company": "NVDA", "doc_type": "earnings_transcript", "fiscal_year": 2025, "period": "2025-Q4", "source_file": "data/earnings_calls_manual/NVDA/NVDA_FY2025_Q4.pdf", "parsed_at": "2025-11-09T21:07:08Z", "section_id": null, "section_title": null}
{"doc_id": "NVDA_TRANSCRIPT_2025_Q4", "ticker": "NVDA", "quarter": "Q4", "filing_date": "2025-12-01", "phase": "qa", "speaker_name": "Marshall Pappas", "speaker_role": "unknown", "speaker_firm": null, "utterance_id": "u_0017", "utterance_type": "statement", "text": "Hi. This is Marshall Pappas. Thanks for taking the question. Question. I had a clarification and a question. Colette, for the clarification. Did you say that enterprise\nwithin the data center grew 2x year on year for the January quarter? And if so, does that would that make it the faster growing than the hyperscalers? And then,\nJensen, for you, the question, hyperscalers are the biggest purchasers of your solutions, but they buy equipment for both internal and external workloads,\nexternal workloads being cloud services that enterprises use. So the question is, can you give us a sense of how that hyperscale expense splits between that\nexternal workload and internal and as these new AI workflows and applications come up, would you expect enterprises to become a larger part of that\nconsumption mix? And does that impact how you develop your service your ecosystem? Thank you.", "char_count": 883, "word_count": 146, "token_count": 185, "exchange_id": null, "exchange_role": null, "chunk_id": "NVDA_TRANSCRIPT_2025_Q4_u_0017", "company": "NVDA", "doc_type": "earnings_transcript", "fiscal_year": 2025, "period": "2025-Q4", "source_file": "data/earnings_calls_manual/NVDA/NVDA_FY2025_Q4.pdf", "parsed_at": "2025-11-09T21:07:08Z", "section_id": null, "section_title": null}
{"doc_id": "NVDA_TRANSCRIPT_2025_Q4", "ticker": "NVDA", "quarter": "Q4", "filing_date": "2025-12-01", "phase": "qa", "speaker_name": "Colette Kress", "speaker_role": "CFO", "speaker_firm": null, "utterance_id": "u_0018", "utterance_type": "answer", "text": "Sure. Thanks for the question regarding our enterprise business. Yes. It grew 2x. Very similar to what we were seeing with our large CSPs. Keep in mind, these\nare both important areas to understand. Working with the CSPs can be working on large language models. Can be working on inference on their own work? But\nkeep in mind, that is also where the enterprises are surfacing. Your enterprises are both with your CSPs, as well as in terms of building on their own. They're both\ngrowing quite well.", "char_count": 497, "word_count": 88, "token_count": 113, "exchange_id": null, "exchange_role": null, "chunk_id": "NVDA_TRANSCRIPT_2025_Q4_u_0018", "company": "NVDA", "doc_type": "earnings_transcript", "fiscal_year": 2025, "period": "2025-Q4", "source_file": "data/earnings_calls_manual/NVDA/NVDA_FY2025_Q4.pdf", "parsed_at": "2025-11-09T21:07:08Z", "section_id": null, "section_title": null}
{"doc_id": "NVDA_TRANSCRIPT_2025_Q4", "ticker": "NVDA", "quarter": "Q4", "filing_date": "2025-12-01", "phase": "qa", "speaker_name": "Jensen Huang", "speaker_role": "CEO", "speaker_firm": null, "utterance_id": "u_0019", "utterance_type": "answer", "text": "The CSPs are about half of our business. And the CSPs have internal consumption, and external consumption, as you say. And we're using of course, used for\ninternal consumption. We work very closely with all of them to optimize workloads that are internal to them because they have a large infrastructure of NVIDIA\nCorporation gear that they could take advantage of. And the fact that we could be used for AI on the one hand, video processing on the other hand, data\nprocessing like Spark. We're fungible. And so the useful life. Our infrastructure is much better. If the useful life is much longer, then the TCO is also lower. And so\nthe second part is how do we see the growth of enterprise or not CSPs, if you will, going forward? And the answer is I believe, long term. It is by far larger. And\nthe reason for that is because if you look at the computer industry today, and what is not served by the computer industry is largely industrial. Let me give you an\nexample. When we say enterprise, and let's say let's use a car company as an example because they make both soft things and hard things. And so in the case\nof a car company, the employees would be what we call enterprise. And agentic AI and software planning systems and tools, and we have some really exciting\nthings to with you guys at GTC. Those agentic systems are for employees to make employees more productive. To design, to market, plan, to operate their\ncompany. That's agentic AIs. On the other hand, the cars that they manufacture also need AI. They need an AI system that trains the cars treats this entire giant\nfleet of cars, and you know, today, there's some billion cars on the road. Someday, there'd be a billion cars on the road, and every single one of those cars will be,\nyou know, robotic cars. And they'll all be collecting data, and we'll be improving them using an AI factory where they whereas they have a car factory today, in the\nfuture, they'll have a car factory and an AI factory. And then inside the car itself is a robotic system. And so as you can see, there are three computers involved.\nAnd there's the computer that helps the people. There's the computer that builds the AI for it. The machineries. It could be, of course. Could be a tractor. It could\nbe a lawnmower. It could be a human or a robot that's being developed today. It could be a building. It could be a warehouse. These physical systems require a\nnew type of AI we call physical AI. They can't just understand the meaning of words and languages but they have to understand the meaning of the world. Friction\nand inertia, object permanence, and cause and effect, and all of those types of things that are common sense to you and I. But you know, AI has to go learn those\nphysical effects. So we call that physical AI. That whole part of using agentic AI to revolutionize the way we work inside companies. That's just starting. This is\nnow the beginning of the agentic AI era. And you hear a lot of people talking about it and got some really great things going on. And then there's the physical AI\nafter that, and then there's robotic systems after that. And so these three computers are all brand new. And my sense is that long term, this will be by far a larger\nof a mold which kinda makes sense. You know, the world the world's GDP is represented by either heavy industries industrials. And companies that are providing\nfor those.\nChrista\nYour next question comes from the line of Aaron Rakers with Wells Fargo. Please go ahead.", "char_count": 3493, "word_count": 640, "token_count": 791, "exchange_id": null, "exchange_role": null, "chunk_id": "NVDA_TRANSCRIPT_2025_Q4_u_0019", "company": "NVDA", "doc_type": "earnings_transcript", "fiscal_year": 2025, "period": "2025-Q4", "source_file": "data/earnings_calls_manual/NVDA/NVDA_FY2025_Q4.pdf", "parsed_at": "2025-11-09T21:07:08Z", "section_id": null, "section_title": null}
{"doc_id": "NVDA_TRANSCRIPT_2025_Q4", "ticker": "NVDA", "quarter": "Q4", "filing_date": "2025-12-01", "phase": "qa", "speaker_name": "Aaron Rakers", "speaker_role": "Analyst", "speaker_firm": "Wells Fargo", "utterance_id": "u_0020", "utterance_type": "question", "text": "Yeah. Thanks for letting me back in. Jensen, I'm curious as we now approach the two-year anniversary of really the Hopper inflection that you saw in 2023 in Gen \nAI in general. We think about the roadmap you have in front of us, how do you think about the infrastructure that's been deployed from a replacement cycle\n\nperspective and whether, you know, if it's GV300 or if it's the Rubin cycle where we start to see maybe some refresh opportunity. I'm just curious to how you look\nat that.", "char_count": 489, "word_count": 89, "token_count": 113, "exchange_id": "ex_006", "exchange_role": "question", "chunk_id": "NVDA_TRANSCRIPT_2025_Q4_u_0020", "company": "NVDA", "doc_type": "earnings_transcript", "fiscal_year": 2025, "period": "2025-Q4", "source_file": "data/earnings_calls_manual/NVDA/NVDA_FY2025_Q4.pdf", "parsed_at": "2025-11-09T21:07:08Z", "section_id": null, "section_title": null}
{"doc_id": "NVDA_TRANSCRIPT_2025_Q4", "ticker": "NVDA", "quarter": "Q4", "filing_date": "2025-12-01", "phase": "qa", "speaker_name": "Jensen Huang", "speaker_role": "CEO", "speaker_firm": null, "utterance_id": "u_0021", "utterance_type": "answer", "text": "Yeah. I appreciate it. First of all, people are still using Voltas. And Pascals, and Amperes. And the reason for that is because they're always things that because\nCUDA is so programmable, you could use it right well, one of the major use cases right now is data processing and data curation. You find a circumstance that an\nAI model is not very good at? You present that circumstance to a vision language model, let's say. Let's say it's a car? You present that circumstance to a vision\nlanguage model, the vision language model actually looks at the circumstances. It's a this isn't this is what happened, and I wasn't very good at it. You then take\nthat response, this the prompt, and you go and prompt an AI model to go find in your whole link of data of other circumstances like that. Whatever that\ncircumstance was. And then you use an AI to do domain randomization and generate a whole bunch of other examples. And then from that, you can go train the\nmodel. And so you could use the Amperes to go and do data processing and data curation and machine learning-based search. And then you create the training\ndataset, which you then present to your Hopper systems for training. And so each one of these architectures are completely are you know, they're all CUDA\ncompatible, and so everything runs on everything. But if you have infrastructure in place, and you can put the less intensive workloads onto the installed base of\nthe past. All of our CPUs are very well employed.\nChrista\nWe have time for one more question, and that question comes from Atif Malik with Citi. Please go ahead.", "char_count": 1592, "word_count": 285, "token_count": 358, "exchange_id": "ex_006", "exchange_role": "answer", "chunk_id": "NVDA_TRANSCRIPT_2025_Q4_u_0021", "company": "NVDA", "doc_type": "earnings_transcript", "fiscal_year": 2025, "period": "2025-Q4", "source_file": "data/earnings_calls_manual/NVDA/NVDA_FY2025_Q4.pdf", "parsed_at": "2025-11-09T21:07:08Z", "section_id": null, "section_title": null}
{"doc_id": "NVDA_TRANSCRIPT_2025_Q4", "ticker": "NVDA", "quarter": "Q4", "filing_date": "2025-12-01", "phase": "qa", "speaker_name": "Atif Malik", "speaker_role": "unknown", "speaker_firm": null, "utterance_id": "u_0022", "utterance_type": "statement", "text": "Hi. Thank you for taking my question. I have a follow-up question on gross margins, Colette. I understand there are many moving parts that will yield and NVLink\n72 and Ethernet mix. And you kind of tiptoed the earlier question if April quarter is the bottom. But second half would have to ramp, like, 200 basis point per\nquarter to get to the mid-seventies range that you're giving, for the end of the fiscal year. And we still don't know much about tariffs impact to broader\nsemiconductor. So what kind of gives you the confidence in that trajectory in the back half of this year?", "char_count": 581, "word_count": 105, "token_count": 132, "exchange_id": null, "exchange_role": null, "chunk_id": "NVDA_TRANSCRIPT_2025_Q4_u_0022", "company": "NVDA", "doc_type": "earnings_transcript", "fiscal_year": 2025, "period": "2025-Q4", "source_file": "data/earnings_calls_manual/NVDA/NVDA_FY2025_Q4.pdf", "parsed_at": "2025-11-09T21:07:08Z", "section_id": null, "section_title": null}
{"doc_id": "NVDA_TRANSCRIPT_2025_Q4", "ticker": "NVDA", "quarter": "Q4", "filing_date": "2025-12-01", "phase": "qa", "speaker_name": "Colette Kress", "speaker_role": "CFO", "speaker_firm": null, "utterance_id": "u_0023", "utterance_type": "answer", "text": "Yeah. Thanks for the question. Our gross margins, they're quite complex. In terms of the material. And everything that we put together in a Blackwell system.\nTremendous amount of opportunity to look at a lot of different pieces of that. On how we can better improve our gross margins over time. Remember, we have\nmany different configurations as well. On Blackwell. That will be able to help us do that. So, together, working after we get some of these really strong ramping\ncompleted for our customers we can begin a lot of that work. If not, we're gonna probably start as soon as possible. If we can improve it in the short term, we will\nalso do that. Tariffs, at this point, it's a little bit of an unknown. It's an unknown until we understand further what the US government's plan is, its timing, it's where,\nand how much. So at this time, we are awaiting but again, we would, of course, always follow export control and or tariffs in that manner.\nChrista\nLadies and gentlemen, that does conclude our question and answer session. I'm sorry. Thank you.", "char_count": 1055, "word_count": 188, "token_count": 243, "exchange_id": null, "exchange_role": null, "chunk_id": "NVDA_TRANSCRIPT_2025_Q4_u_0023", "company": "NVDA", "doc_type": "earnings_transcript", "fiscal_year": 2025, "period": "2025-Q4", "source_file": "data/earnings_calls_manual/NVDA/NVDA_FY2025_Q4.pdf", "parsed_at": "2025-11-09T21:07:08Z", "section_id": null, "section_title": null}
{"doc_id": "NVDA_TRANSCRIPT_2025_Q4", "ticker": "NVDA", "quarter": "Q4", "filing_date": "2025-12-01", "phase": "qa", "speaker_name": "Jensen Huang", "speaker_role": "CEO", "speaker_firm": null, "utterance_id": "u_0024", "utterance_type": "answer", "text": "No. No. I'm gonna just wanna thank you. Up to, Jensen? And, like, the medium, a couple things. I just wanna thank you. Thank you, Colette. Demand for\nBlackwell is extraordinary. AI is evolving beyond perception. And generative AI into reasoning. With reasoning AI, we're observing another scaling law. Inference\ntime or test time scaling. The more computation the more the model thinks the smarter the answer. Models like OpenAI's Grok 3, DeepSeq R1, are reasoning\nmodels that apply inference time scale. Reasoning models can consume a hundred times more compute. Future reasoning models can consume much more\ncompute. DeepSeq R1 has ignited global enthusiasm. It's an excellent innovation. But even more importantly, it has open-sourced a world-class reasoning AI\nmodel. Nearly every AI developer is applying R1. Or chain of thought and reinforcement learning techniques like R1. To scale their model's performance. We now\nhave three scaling laws, as I mentioned earlier. Driving the demand for AI computing. The traditional scaling laws of AI remain intact. Foundation models are\nbeing enhanced with multimodality. And pretraining is still growing. But it's no longer enough. We have two additional scaling dimensions. Post-training scaling,\nwhere reinforcement learning fine-tuning, model distillation, require orders of magnitude more compute than pretraining alone. Inference time scaling and\nreasoning where a single query can demand a hundred times more compute. We designed Blackwell for this moment a single platform that can easily transition\nfrom pretraining, post-training, and test time scaling. Blackwell's MP4 transformer engine, and NVLink 72 scale-up fabric. And new software technologies let\nBlackwell process reasoning AI models 25 times faster than Hopper. Blackwell, in all of these configurations, is in full production. Each Grace Blackwell NVLink\n72 rack is an engineering marvel. One and a half million components produced across 350 manufacturing sites by nearly a hundred thousand factory operators.\nAI is advancing at light speed. We're at the beginning of reasoning AI and inference time scaling. But we're just at the start of the age of AI. Multimodal AIs.\nEnterprise AI, Sovereign AI. And physical AI are right around the corner. We will grow strongly in 2025. Going forward, data centers will dedicate most of CapEx\nto accelerated computing and AI. Data centers will increasingly become AI factories. And every company will have a either rented or self-operated. I wanna thank\nall of you for joining us today. Come join us at GTC in a couple of weeks gonna be talking about Blackwell Ultra, Rubin, and other new computing networking,\nreasoning AI, physical AI products. And a whole bunch more. Thank you.\nChrista\nThis concludes today's conference call. You may now disconnect.", "char_count": 2809, "word_count": 438, "token_count": 594, "exchange_id": null, "exchange_role": null, "chunk_id": "NVDA_TRANSCRIPT_2025_Q4_u_0024", "company": "NVDA", "doc_type": "earnings_transcript", "fiscal_year": 2025, "period": "2025-Q4", "source_file": "data/earnings_calls_manual/NVDA/NVDA_FY2025_Q4.pdf", "parsed_at": "2025-11-09T21:07:08Z", "section_id": null, "section_title": null}

{"doc_id": "NVDA_2025_Q3", "ticker": "NVDA", "year": 2025, "quarter": 3, "filing_date": "2025-09-01", "phase": "qa", "speaker_name": "Stewart Stecker", "speaker_role": "Unknown", "speaker_firm": null, "utterance_id": "u_0000", "utterance_type": "statement", "text": "Thank you. Good afternoon, everyone, and welcome to NVIDIA's conference call for the third quarter of fiscal 2025. With me today from NVIDIA are Jensen\nHuang, President and Chief Executive Officer; and Colette Kress, Executive Vice President and Chief Financial Officer. I'd like to remind you that our call is being\nwebcast live on NVIDIA's Investor Relations website. The webcast will be available for replay until the conference call to discuss our financial results for the fourth\nquarter of fiscal 2025. The content of today's call is NVIDIA's property. It can't be reproduced or transcribed without our prior written consent. During this call, we\nmay make forward-looking statements based on current expectations. These are subject to a number of significant risks and uncertainties and our actual results\nmay differ materially. For a discussion of factors that could affect our future financial results and business, please refer to the disclosure in today's earnings\nrelease, our most recent Forms 10-K and 10-Q, and the reports that we may file on Form 8-K with the Securities and Exchange Commission. All our statements\nare made as of today, November 20, 2024, based on information currently available to us. Except as required by law, we assume no obligation to update any\nsuch statements. During this call, we will discuss non-GAAP financial measures. You can find a reconciliation of these non-GAAP financial measures to GAAP\nfinancial measures in our CFO commentary, which is posted on our website. With that, let me turn the call over to Colette.", "char_count": 1561, "word_count": 251, "token_count": 324, "exchange_id": null, "exchange_role": null}
{"doc_id": "NVDA_2025_Q3", "ticker": "NVDA", "year": 2025, "quarter": 3, "filing_date": "2025-09-01", "phase": "qa", "speaker_name": "Colette Kress", "speaker_role": "CFO", "speaker_firm": null, "utterance_id": "u_0001", "utterance_type": "answer", "text": "Thank you, Stewart. Q3 was another record quarter. We continued to deliver incredible growth. Revenue of $35.1 billion was up 17% sequentially and up 94% \nyear-on-year and, well above our outlook of $32.5 billion. All market platforms posted strong sequential and year-over-year growth, fueled by the adoption of \nNVIDIA accelerated computing and AI. Starting with Data Center, another record was achieved in Data Center. Revenue of $30.8 billion, up 17% sequential and \nup 112% year-on-year. NVIDIA Hopper demand is exceptional and sequentially, NVIDIA H200 sales increased significantly to double-digit billions, the fastest \nproduct ramp in our company's history. The H200 delivers up to 2 times faster inference performance and up to 50% improved TCO. Cloud service providers \nwere approximately half of our data center sales with revenue increasing more than 2 times year-on-year. CSPs deployed NVIDIA H200 infrastructure and \nhigh-speed networking with installations scaling to tens of thousands of GPUs to grow their business and serve rapidly rising demand for AI training and inference \nworkloads. NVIDIA H200-powered cloud instances are now available from AWS, CoreWeave, and Microsoft Azure with Google Cloud and OCI coming soon. \nAlongside significant growth from our large CSPs, NVIDIA GPU regional cloud revenue jumped 2 times year-on-year as North America, EMEA, and Asia Pacific \nregions ramped NVIDIA cloud instances and sovereign cloud buildout. Consumer Internet revenue more than doubled year-on-year as companies scaled their \nNVIDIA Hopper infrastructure to support next-generation AI models, training, multimodal and agentic AI, deep learning recommender engines, and generative AI \ninference and content creation workloads. NVIDIA's Ampere and Hopper infrastructures are fueling inference revenue growth for customers. NVIDIA is the largest \ninference platform in the world. Our large installed base and rich software ecosystem encourage developers to optimize for NVIDIA and deliver continued \nperformance and TCL improvements. Rapid advancements in NVIDIA's software algorithms boosted Hopper inference throughput by an incredible 5 times in one \nyear and cut time to first token by 5 times. Our upcoming release of NVIDIA NIM will boost Hopper Inference performance by an additional 2.4 times. Continuous \nperformance optimizations are a hallmark of NVIDIA and drive increasingly economic returns for the entire NVIDIA installed base. Blackwell is in full production \nafter a successfully executed mass change. We shipped 13,000 GPU samples to customers in the third quarter, including one of the first Blackwell DGX \nengineering samples to OpenAI. Blackwell is a full stack, full infrastructure, AI data center scale system with customizable configurations needed to address a \ndiverse and growing AI market from x86 to ARM, training to inferencing GPUs, InfiniBand to Ethernet switches, and NVLINK and from liquid-cooled to air-cooled. \nEvery customer is racing to be the first to market. Blackwell is now in the hands of all of our major partners and they are working to bring up their Data Centers. \nWe are integrating Blackwell systems into the diverse Data Center configurations of our customers. Blackwell demand is staggering and we are racing to scale \nsupply to meet the incredible demand customers are placing on us. Customers are gearing up to deploy Blackwell at scale. Oracle announced the world's first \nZettascale AI Cloud computing clusters that can scale to over 131,000 Blackwell GPUs to help enterprises train and deploy some of the most demanding \nnext-generation AI models. Yesterday, Microsoft announced they will be the first CSP to offer in private preview Blackwell-based cloud instances powered by \nNVIDIA GB200, and Quantum InfiniBand. Last week, Blackwell made its debut on the most recent round of MLPerf Training results, sweeping the per GPU \nbenchmarks and delivering a 2.2 times leap in performance over Hopper. The results also demonstrate our relentless pursuit to drive down the cost of compute. \nJust 64 Blackwell GPUs are required to run the GPT-3 benchmark compared to 256 H100s or a 4 times reduction in cost. NVIDIA Blackwell architecture with \nNVLINK Switch enables up to 30 times faster inference performance and a new level of inference scaling throughput and response time that is excellent for \nrunning new reasoning inference applications like OpenAI's o1 model. With every new platform shift, a wave of start-ups is created. Hundreds of AI native \ncompanies are already delivering AI services with great success. Through Google, Meta, Microsoft, and OpenAI are the headliners and Anthropic, Perplexity, \nMistral, Adobe Firefly, Runway, Midjourney, Lightricks, Harvey, Codeium, Cursor, and Bridge are seeing great success, while thousands of AI-native startups are \nbuilding new services. The next wave of AI are Enterprise AI and Industrial AI. Enterprise AI is in full throttle. NVIDIA AI Enterprise, which includes NVIDIA NeMo \nand NIM microservices is an operating platform of agentic AI. Industry leaders are using NVIDIA AI to build Co-Pilots and agents. Working with NVIDIA, Cadence, \nCloudera, Cohesity, NetApp, Nutanix, Salesforce, SAP and ServiceNow are racing to accelerate development of these applications with the potential for billions of \nagents to be deployed in the coming years. Consulting leaders like Accenture and Deloitte are taking NVIDIA AI to the world's enterprises. Accenture launched a \nnew business group with 30,000 professionals trained on NVIDIA AI technology to help facilitate this global build-out. Additionally, Accenture with over 770,000 \nemployees is leveraging NVIDIA-powered Agentic AI applications internally, including in one case that cuts manual steps in marketing campaigns by 25% to 35%. \nNearly 1,000 companies are using NVIDIA NIM and the speed of its uptake is evident in NVIDIA AI Enterprise monetization. We expect NVIDIA AI Enterprise full \nyear revenue to increase over 2 times from last year and our pipeline continues to build. Overall, our software, service, and support revenue is annualizing at $1.5 \nbillion, and we expect to exit this year annualizing at over $2 billion. Industrial AI and robotics are accelerating. This is triggered by breakthroughs in physical AI, \nfoundation models that understand the physical world. Like NVIDIA NeMo for enterprise AI agents, we built NVIDIA Omniverse for developers to build, train, and \noperate industrial AI and robotics. Some of the largest industrial manufacturers in the world are adopting NVIDIA Omniverse to accelerate their businesses, \nautomate their workflows, and to achieve new levels of operating efficiency. Foxconn, the world's largest electronics manufacturer is using digital twins and \nindustrial AI built on NVIDIA Omniverse to speed the bring up of its Blackwells factories and drive new levels of efficiency. In its Mexico facility alone, Foxconn\n\nexpects to reduce -- a reduction of over 30% in annual kilowatt-hour usage. From a geographic perspective, our Data Center revenue in China grew sequentially\ndue to shipments of export-compliant copper products to industries. As a percentage of total Data Center revenue, it remains well below levels prior to the onset\nof export controls. We expect the market in China to remain very competitive going forward. We will continue to comply with export controls while serving our\ncustomers. Our sovereign AI initiatives continue to gather momentum as countries embrace NVIDIA accelerated computing for a new industrial revolution\npowered by AI. India's leading CSPs include Tata Communications and Yotta Data Services are building AI factories for tens of thousands of NVIDIA GPUs. By\nyear-end, they will have boosted NVIDIA GPU deployments in the country by nearly 10 times. Infosys, TSE, Wipro are adopting NVIDIA AI Enterprise and\nupskilling nearly 0.5 million developers and consultants to help clients build and run AI agents on our platform. In Japan, SoftBank is building the nation's most\npowerful AI supercomputer with NVIDIA DGX Blackwell and Quantum InfiniBand. SoftBank is also partnering with NVIDIA to transform the telecommunications\nnetwork into a distributed AI network with NVIDIA AI Aerial and AI-RAN platform that can process both 5G RAN on AI on CUDA. We are launching the same in\nthe US with T-Mobile. Leaders across Japan, including Fujitsu, NEC and NTT are adopting NVIDIA AI Enterprise and major consulting companies, including EY,\nStrategy, and Consulting will help bring NVIDIA AI technology to Japan's industries. Networking revenue increased 20% year-on-year. Areas of sequential\nrevenue growth include InfiniBand and Ethernet switches, SmartNICs, and BlueField DPUs. The networking revenue was sequentially down, networking demand\nis strong and growing and we intend -- anticipate sequential growth in Q4. CSPs and supercomputing centers are using and adopting the NVIDIA InfiniBand\nplatform to power new H200 clusters. NVIDIA Spectrum-X Ethernet for AI revenue increased over 3 times year-on-year and our pipeline continues to build with\nmultiple CSPs and consumer Internet companies planning large cluster deployments. Traditional Ethernet was not designed for AI. NVIDIA Spectrum-X uniquely\nleverages technology previously exclusive to InfiniBand to enable customers to achieve massive scale of their GPU compute. Utilizing Spectrum-X, xAI's\nColossus, 100,000 Hopper Supercomputer will experience zero application latency degradation and maintained 95% data throughput versus 60% for traditional\nEthernet. Now moving to gaming and AI PCs. Gaming revenue of $3.3 billion increased 14% sequentially and 15% year-on-year. Q3 was a great quarter for\ngaming with notebook, console, and desktop revenue, all growing sequentially and year-on-year. RTX end-demand was fueled by strong back-to-school sales as\nconsumers continue to choose GeForce RTX GPUs and devices to power gaming, creative, and AI applications. Channel inventory remains healthy and we are\ngearing up for the holiday season. We began shipping new GeForce RTX AI PCs with up to 321 AI tops from ASUS and MSI with Microsoft's Copilot+ capabilities\nanticipated in Q4. These machines harness the power of RTX ray tracing and AI technologies to supercharge gaming, photo and video editing, image generation,\nand coding. This past quarter, we celebrated the 25th anniversary of the GeForce 256, the world's first GPU. The transforming computing graphics to igniting the\nAI revolution, NVIDIA's GPUs have been the driving force behind some of the most consequential technologies of our time. Moving to ProViz. Revenue of $486\nmillion was up 7% sequentially and 17% year-on-year. NVIDIA RTX workstations continue to be the preferred choice to power professional graphics, design, and\nengineering-related workloads. Additionally, AI is emerging as a powerful demand driver, including autonomous vehicle simulation, generative AI model\nprototyping for productivity-related use cases, and generative AI, content creation in media and entertainment. Moving to Automotive. Revenue was a record\n$449 million, up 30% sequentially and up 72% year-on-year. Strong growth was driven by self-driving ramps of NVIDIA Orin and robust end-market demand for\nNAVs. Volvo Cars has rolling out its fully electric SUV built on NVIDIA Orin and DriveOS. Okay. Moving to the rest of the P&L. GAAP gross margin was 74.6%\nand non-GAAP gross margin was 75%, down sequentially, primarily driven by a mix-shift of the H100 systems to more complex and higher cost systems within\nData Center. Sequentially, GAAP operating expenses and non-GAAP operating expenses were up 9% due to higher compute, infrastructure, and engineering\ndevelopment costs for new product introductions. In Q3, we returned $11.2 billion to shareholders in the form of share repurchases and cash dividends. Well, let\nme turn to the outlook for the fourth quarter. Total revenue is expected to be $37.5 billion, plus or minus 2%, which incorporates continued demand for Hopper\narchitecture and the initial ramp of our Blackwell products, while demand is greatly exceed supply, we are on track to exceed our previous Blackwell revenue\nestimate of several billion dollars as our visibility into supply continues to increase. On gaming, although sell-through was strong in Q3, we expect fourth quarter\nrevenue to decline sequentially due to supply constraints. GAAP and non-GAAP gross margins are expected to be 73% and 73.5%, respectively, plus or minus\n50 basis points. Blackwell is a customizable AI infrastructure with several different types of NVIDIA-build chips, multiple networking options, and for air and\nliquid-cooled Data Centers. Our current focus is on ramping to strong demand, increasing system availability, and providing the optimal mix of configurations to\nour customer. As Blackwell ramps, we expect gross margins to moderate to the low-70s. When fully ramped, we expect Blackwell margins to be in the mid-70s.\nGAAP and non-GAAP operating expenses are expected to be approximately $4.8 billion and $3.4 billion, respectively. We are a data center scale AI\ninfrastructure company. Our investments include building data centers for development of our hardware and software stacks and to support new introductions.\nGAAP and non-GAAP other income and expenses are expected to be an income of approximately $400 million, excluding gains and losses from non-affiliated\ninvestments. GAAP and non-GAAP tax rates are expected to be 16.5% plus or minus 1%, excluding any discrete items. Further financial details are included in\nthe CFO commentary and other information available on our IR websites. In closing, let me highlight upcoming events for the financial community. We will be\nattending the UBS Global Technology and AI Conference on December 3rd, in Scottsdale. Please join us at CES in Las Vegas, where Jensen will deliver a\nkeynote on January 6th, and we will host a Q&A session for financial analysts the next day on January 7th. Our earnings call to discuss results for the fourth\nquarter of fiscal 2025 is scheduled for February 26th, 2025. We will now open the call for questions. Operator, can you poll for questions, please?\nOperator\n[Operator Instructions] Your first question comes from the line of C.J. Muse of Cantor Fitzgerald. Your line is open.\nC.J. Muse\nYes, good afternoon. Thank you for taking the question. I guess just a question for you on the debate around whether scaling for large language models have\nstalled. Obviously, we're very early here but would love to hear your thoughts on this front. How are you helping your customers as they work through these\nissues? And then obviously, part of the context here as we're discussing clusters that have yet to benefit from Blackwell. So is this driving even greater demand\nfor Blackwell? Thank you.", "char_count": 14860, "word_count": 2268, "token_count": 3097, "exchange_id": null, "exchange_role": null}
{"doc_id": "NVDA_2025_Q3", "ticker": "NVDA", "year": 2025, "quarter": 3, "filing_date": "2025-09-01", "phase": "qa", "speaker_name": "Jensen Huang", "speaker_role": "Unknown", "speaker_firm": null, "utterance_id": "u_0002", "utterance_type": "statement", "text": "A foundation model pre-training scaling is intact and it's continuing. As you know, this is an empirical law, not a fundamental physical law, but the evidence is that \nit continues to scale. What we're learning, however, is that it's not enough that we've now discovered two other ways to scale. One is post-training scaling. Of \ncourse, the first generation of post-training was reinforcement learning human feedback, but now we have reinforcement learning AI feedback and all forms of \nsynthetic data generated data that assists in post-training scaling. And one of the biggest events and one of the most exciting developments is Strawberry, \nChatGPT o1, OpenAI's o1, which does inference time scaling, what's called test time scaling. The longer it thinks, the better and higher-quality answer it produces \nand it considers approaches like chain of thought and multi-path planning and all kinds of techniques necessary to reflect and so on and so forth and it's \nintuitively, it's a little bit like us doing thinking in our head before we answer a question. And so we now have three ways of scaling and we're seeing all three \nways of scaling. And as a result of that, the demand for our infrastructure is really great. You see now that at the tail-end of the last generation of foundation \nmodels were at about 100,000 Hoppers. The next generation starts at 100,000 Blackwells. And so that kind of gives you a sense of where the industry is moving\n\nwith respect to pre-training scaling, post-training scaling, and then now very importantly inference time scaling. And so the demand is really great for all of those\nreasons. But remember, simultaneously, we're seeing inference really starting to scale out for our company. We are the largest inference platform in the world\ntoday because our installed base is so large and everything that was trained on Amperes and Hoppers inference incredibly on Amperes and Hoppers. And as we\nmove to Blackwells for training foundation models, it leads behind it a large installed base of extraordinary infrastructure for inference. And so we're seeing\ninference demand go up. We're seeing inference time scaling go up. We see the number of AI-native companies continue to grow. And of course, we're starting to\nsee enterprise adoption of agentic AI really is the latest rage. And so we're seeing a lot of demand coming from a lot of different places.\nOperator\nYour next question comes from the line of Toshiya Hari of Goldman Sachs. Your line is open.", "char_count": 2492, "word_count": 414, "token_count": 526, "exchange_id": null, "exchange_role": null}
{"doc_id": "NVDA_2025_Q3", "ticker": "NVDA", "year": 2025, "quarter": 3, "filing_date": "2025-09-01", "phase": "qa", "speaker_name": "Toshiya Hari", "speaker_role": "Analyst", "speaker_firm": "Goldman Sachs", "utterance_id": "u_0003", "utterance_type": "question", "text": "Hi, good afternoon. Thank you so much for taking the question. Jensen, you executed the mass change earlier this year. There were some reports over the\nweekend about some heating issues. On the back of this, we've had investors ask about your ability to execute to the roadmap you presented at GTC this year\nwith Ultra coming out next year and the transition to [Ruben] (ph) in 2026. Can you sort of speak to that? And some investors are questioning that. So if you can\nsort of speak to your ability to execute on time, that would be super helpful. And then a quick part B, on supply constraints, is it a multitude of componentry that's\ncausing this? Or is it specifically [HBM] (ph)? Is it supply constraints? Are the supply constraints getting better? Are they worsening? Any sort of color on that\nwould be super helpful as well. Thank you.", "char_count": 842, "word_count": 151, "token_count": 194, "exchange_id": "ex_001", "exchange_role": "question"}
{"doc_id": "NVDA_2025_Q3", "ticker": "NVDA", "year": 2025, "quarter": 3, "filing_date": "2025-09-01", "phase": "qa", "speaker_name": "Jensen Huang", "speaker_role": "Unknown", "speaker_firm": null, "utterance_id": "u_0004", "utterance_type": "statement", "text": "Yes, thanks. Thanks. So let's see, back to the first question. Blackwell production is in full steam. In fact, as Colette mentioned earlier, we will deliver this quarter\nmore Blackwells than we had previously estimated. And so the supply chain team is doing an incredible job working with our supply partners to increase\nBlackwell, and we're going to continue to work hard to increase Blackwell through next year. It is the case that demand exceeds our supply and that's expected as\nwe're in the beginnings of this generative AI revolution as we all know. And we're at the beginning of a new generation of foundation models that are able to do\nreasoning and able to do long thinking and of course, one of the really exciting areas is physical AI, AI that now understands the structure of the physical world.\nAnd so Blackwell demand is very strong. Our execution is on -- is going well. And there's obviously a lot of engineering that we're doing across the world. You see\nnow systems that are being stood up by Dell and CoreWeave, I think you saw systems from Oracle stood up. You have systems from Microsoft and they're about\nto preview their Grace Blackwell systems. You have systems that are at Google. And so all of these CSPs are racing to be first. The engineering that we do with\nthem is, as you know, rather complicated. And the reason for that is because although we build full stack and full infrastructure, we disaggregate all of the -- this\nAI supercomputer and we integrate it into all of the custom data centers in architectures around the world. That integration process is something we've done\nseveral generations now. We're very good at it, but still, there's still a lot of engineering that happens at this point. But as you see from all of the systems that are\nbeing stood up, Blackwell is in great shape. And as we mentioned earlier, the supply and what we're planning to ship this quarter is greater than our previous\nestimates. With respect to the supply chain, all right, there are seven different chips, seven custom chips that we built in order for us to deliver the Blackwell\nsystems. The Blackwell systems go in air-cooled or liquid-cooled, NVLink 8 or NVLink 72 or NVLink 8, NVLink 36, NVLink 72 we have x86 or Grace and the\nintegration of all of those systems into the world's data centers is nothing short of a miracle. And so the component supply chain necessary to ramp at the scale\nyou have to go back and take a look at how much Blackwell we shipped last quarter, which was zero. And in terms of how much Blackwell total systems will ship\nthis quarter, which is measured in billions, the ramp is incredible. And so almost every company in the world seems to be involved in our supply chain. And we've\ngot great partners, everybody from, of course, TSMC and Amphenol, the connector company, incredible company, Vertiv and SK Hynix and Micron Spill, Amkor\nand KYEC and there's Foxconn and the many the factories that they've built and Quanta and Wiwynn and gosh, Dell and HP and Super Micro, Lenovo and the\nnumber of companies is just really quite incredible, Quanta. And I'm sure I've missed partners that are involved in the ramping up of Blackwell, which I really\nappreciate. And so anyways, I think we're in great shape with respect to the Blackwell ramp at this point. And then lastly, your question about our execution of our\nroadmap. We're on an annual roadmap and we're expecting to continue to execute on our annual roadmap. And by doing so, we increase the performance, of\ncourse, of our platform, but it's also really important to realize that when we're able to increase performance and do so at X factors at a time, we're reducing the\ncost of training, we're reducing the cost of inferencing, we're reducing the cost of AI so that it could be much more accessible. But the other factor that's very\nimportant to note is that when there's a data center of some fixed size and a data center always is of some fixed size. It could be, of course, 10s of megawatts in\nthe past and now it's most data centers are now 100 megawatts to several 100 megawatts and we're planning on gigawatt data centers, it doesn't really matter\nhow large the data centers are, the power is limited. And when you're in the power-limited data center, the best -- the highest performance per watt translates\ndirectly into the highest revenues for our partners. And so on the one hand, our annual roadmap reduces cost, but on the other hand, because our perf per watt is\nso good compared to anything out there, we generate for our customers the greatest possible revenues. And so that annual rhythm is really important to us and\nwe have every intentions of continuing to do that. And everything is on track as far as I know.\nOperator\nYour next question comes from the line of Timothy Arcuri of UBS. Your line is open.", "char_count": 4832, "word_count": 859, "token_count": 1092, "exchange_id": null, "exchange_role": null}
{"doc_id": "NVDA_2025_Q3", "ticker": "NVDA", "year": 2025, "quarter": 3, "filing_date": "2025-09-01", "phase": "qa", "speaker_name": "Timothy Arcuri", "speaker_role": "Analyst", "speaker_firm": "UBS", "utterance_id": "u_0005", "utterance_type": "question", "text": "Thanks a lot. I'm wondering if you can talk about the trajectory of how Blackwell is going to ramp this year. I know, Jensen, you did just talk about Blackwell being\nbetter than I think you had said several billions of dollars in January. It sounds like you're going to do more than that. But I think in recent months also, you said\nthat Blackwell crosses over Hopper in the April quarter. So I guess I had two questions. First of all, is that still the right way to think about it that Blackwell will\ncrossover Hopper in April? And then Colette, you kind of talked about Blackwell bringing down gross margin to the low-70s as it ramps. So I guess if April is the\ncrossover, is that the worst of the pressure on gross margin? So you're going to be kind of in the low-70s as soon as April. I'm just wondering if you can sort of\nshape that for us. Thanks.", "char_count": 853, "word_count": 163, "token_count": 206, "exchange_id": "ex_002", "exchange_role": "question"}
{"doc_id": "NVDA_2025_Q3", "ticker": "NVDA", "year": 2025, "quarter": 3, "filing_date": "2025-09-01", "phase": "qa", "speaker_name": "Jensen Huang", "speaker_role": "Unknown", "speaker_firm": null, "utterance_id": "u_0006", "utterance_type": "statement", "text": "Colette, why don't you start?", "char_count": 29, "word_count": 5, "token_count": 9, "exchange_id": null, "exchange_role": null}
{"doc_id": "NVDA_2025_Q3", "ticker": "NVDA", "year": 2025, "quarter": 3, "filing_date": "2025-09-01", "phase": "qa", "speaker_name": "Colette Kress", "speaker_role": "CFO", "speaker_firm": null, "utterance_id": "u_0007", "utterance_type": "answer", "text": "Sure. Let me first start with your question, Tim. Thank you regarding our gross margins, and we discussed our gross margins as we are ramping Blackwell in the\nvery beginning and the many different configurations, the many different chips that we are bringing to market, we are going to focus on making sure we have the\nbest experience for our customers as they stand that up. We will start growing into our gross margins, but we do believe those will be in the low 70s in that first\npart of the ramp. So you're correct, as you look at the quarters following after that, we will start increasing our gross margins and we hope to get to the mid-70s\nquite quickly as part of that ramp.", "char_count": 682, "word_count": 126, "token_count": 150, "exchange_id": "ex_002", "exchange_role": "answer"}
{"doc_id": "NVDA_2025_Q3", "ticker": "NVDA", "year": 2025, "quarter": 3, "filing_date": "2025-09-01", "phase": "qa", "speaker_name": "Jensen Huang", "speaker_role": "Unknown", "speaker_firm": null, "utterance_id": "u_0008", "utterance_type": "statement", "text": "Hopper demand will continue through next year, surely the first several quarters of the next year. And meanwhile, we will ship more Blackwells next quarter than\nthis. And we'll ship more Blackwells the quarter after that than our first quarter. And so that kind of puts it in perspective. We are really at the beginnings of two\nfundamental shifts in computing that is really quite significant. The first is moving from coding that runs on CPUs to machine learning that creates neural networks\nthat runs on GPUs. And that fundamental shift from coding to machine learning is widespread at this point. There are no companies who are not going to do\nmachine learning. And so machine learning is also what enables generative AI. And so on the one hand, the first thing that's happening is a trillion dollars\u2019 worth\nof computing systems and data centers around the world is now being modernized for machine learning. On the other hand, secondarily, I guess, is that, that on\ntop of these systems are going to be -- we're going to be creating a new type of capability called AI. And when we say generative AI, we're essentially saying that\nthese data centers are really AI factories. They're generating something. Just like we generate electricity, we're now going to be generating AI. And if the number\nof customers is large, just as the number of consumers of electricity is large, these generators are going to be running 24/7. And today, many AI services are\nrunning 24/7, just like an AI factory. And so we're going to see this new type of system come online, and I call it an AI factory because that's really as close to\nwhat it is. It's unlike a data center of the past. And so these two fundamental trends are really just beginning. And so we expect this to happen this growth -- this\nmodernization and the creation of a new industry to go on for several years.\nOperator\nYour next question comes from the line of Vivek Arya of Bank of America Securities. Your line is open.", "char_count": 1975, "word_count": 350, "token_count": 427, "exchange_id": null, "exchange_role": null}
{"doc_id": "NVDA_2025_Q3", "ticker": "NVDA", "year": 2025, "quarter": 3, "filing_date": "2025-09-01", "phase": "qa", "speaker_name": "Vivek Arya", "speaker_role": "Analyst", "speaker_firm": "Bank of America", "utterance_id": "u_0009", "utterance_type": "question", "text": "Thanks for taking my question. Colette, just to clarify, do you think it's a fair assumption to think NVIDIA could recover to kind of mid-70s gross margin in the back\nhalf of calendar 2025? Just wanted to clarify that. And then, Jensen my main question historically, when we have seen hardware deployment cycles, they have\ninevitably included some digestion along the way. When do you think we get to that phase, or is it just too premature to discuss that because you're just the start\nof Blackwell? So how many quarters of shipments do you think is required to kind of satisfy this first wave? Can you continue to grow this into calendar 2026?\nJust how should we be prepared to see what we have seen historically, right, the periods of digestion along the way of a long-term kind of secular hardware\ndeployment?", "char_count": 813, "word_count": 143, "token_count": 177, "exchange_id": "ex_003", "exchange_role": "question"}
{"doc_id": "NVDA_2025_Q3", "ticker": "NVDA", "year": 2025, "quarter": 3, "filing_date": "2025-09-01", "phase": "qa", "speaker_name": "Colette Kress", "speaker_role": "CFO", "speaker_firm": null, "utterance_id": "u_0010", "utterance_type": "answer", "text": "Okay. Vivek, thank you for the question. Let me clarify your question regarding gross margins. Could we reach the mid-70s in the second half of next year? And\nyes, I think it is reasonable assumption or a goal for us to do, but we'll just have to see how that mix of ramp goes. But yes, it is definitely possible.", "char_count": 313, "word_count": 60, "token_count": 76, "exchange_id": "ex_003", "exchange_role": "answer"}
{"doc_id": "NVDA_2025_Q3", "ticker": "NVDA", "year": 2025, "quarter": 3, "filing_date": "2025-09-01", "phase": "qa", "speaker_name": "Jensen Huang", "speaker_role": "Unknown", "speaker_firm": null, "utterance_id": "u_0011", "utterance_type": "statement", "text": "The way to think through that, Vivek, is I believe that there will be no digestion until we modernize a trillion dollars with the data centers. Those -- if you look at the\nworld's data centers, the vast majority of it is built for a time when we wrote applications by hand and we ran them on CPUs. It's just not a sensible thing to do\nanymore. If you have -- if every company's CapEx, if they're ready to build a data center tomorrow, they ought to build it for a future of machine-learning and\ngenerative AI. Because they have plenty of old data centers. And so what's going to happen over the course of next X number of years, and let's assume that\nover the course of four years, the world's data centers could be modernized as we grow into IT. As you know, IT continues to grow about 20%, 30% a year, let's\nsay. And let's say by 2030, the world's data centers for computing is, call it a couple of trillion dollars. And we have to grow into that. We have to modernize the\ndata center from coding to machine learning. That's number one. The second part of it is generative AI, and we're now producing a new type of capability that\nworld has never known, a new market segment that the world has never had. If you look at OpenAI, it didn't replace anything. It's something that's completely\nbrand new. It's in a lot of ways as when the iPhone came, it was completely brand new. It wasn't really replacing anything. And so we're going to see more and\nmore companies like that. And they're going to create and generate out of their services, essentially intelligence. Some of it would be digital artist intelligence like\nRunway. Some of it would be basic intelligence, like OpenAI. Some of it would be legal intelligence like Harvey. Digital marketing intelligence like [Reuters] (ph),\nso on and so forth. And the number of these companies, these -- what are they call AI-native companies are just in hundreds and almost every platform shift there\nwas -- there were Internet companies as you recall, there were cloud-first companies. They were mobile-first companies and now they're AI natives. And so these\ncompanies are being created because people see that there's a platform shift and there's a brand new opportunity to do something completely new. And so my\nsense is that we're going to continue to build out to modernize IT, modernize computing, number one. And then number two, create these AI factories that are\ngoing to be for a new industry for the production of artificial intelligence.\nOperator\nYour next question comes from the line of Stacy Rasgon of Bernstein Research. Your line is open.", "char_count": 2601, "word_count": 462, "token_count": 580, "exchange_id": null, "exchange_role": null}
{"doc_id": "NVDA_2025_Q3", "ticker": "NVDA", "year": 2025, "quarter": 3, "filing_date": "2025-09-01", "phase": "qa", "speaker_name": "Stacy Rasgon", "speaker_role": "Analyst", "speaker_firm": "Bernstein", "utterance_id": "u_0012", "utterance_type": "question", "text": "Hi, guys. Thanks for taking my questions. Colette, I had a clarification and a question for you. The clarification, just when you say low-70s gross margins, is 73.5\ncount as low-70s, or do you have something else in mind? And for my question, you're guiding total revenues and so I mean, total Data Center revenues in the\nnext quarter must be up quote-unquote several billion dollars, but it sounds like Blackwell now should be up more than that. But you also said Hopper was still\nstrong. So like is Hopper down sequentially next quarter? And if it is like why? Is it because of the supply constraints? Is China has been pretty strong is China is\nkind of rolling off a bit into Q4. So any color you can give us on sort of the Blackwell ramp and the Blackwell versus Hopper behavior into Q4 would be really\nhelpful. Thank you.", "char_count": 826, "word_count": 151, "token_count": 198, "exchange_id": "ex_004", "exchange_role": "question"}
{"doc_id": "NVDA_2025_Q3", "ticker": "NVDA", "year": 2025, "quarter": 3, "filing_date": "2025-09-01", "phase": "qa", "speaker_name": "Colette Kress", "speaker_role": "CFO", "speaker_firm": null, "utterance_id": "u_0013", "utterance_type": "answer", "text": "So first starting on your first question there, Stacy, regarding our gross margin and defined low. Low, of course, is below the mid, and let's say we might be at\n71%, maybe about 72%, 72.5%, we're going to be in that range. We could be higher than that as well. We're just going to have to see how it comes through. We\ndo want to make sure that we are ramping and continuing that improvement, the improvement in terms of our yields, the improvement in terms of the product as\nwe go through the rest of the year. So we'll get up to the mid-70s by that point. The second statement was a question regarding our Hopper and what is our\nHopper doing. We have seen substantial growth for our H200, not only in terms of orders but the quickness in terms of those that are standing that up. It is an\namazing product and it's the fastest-growing and ramping that we've seen. We will continue to be selling Hopper in this quarter, in Q4 for sure, that is\nacross-the-board in terms of all of our different configurations and our configurations include what we may do in terms of China. But keep that in mind that folks\nare also at the same time looking to build out their Blackwell. So we've got a little bit of both happening in Q4. But yes, is it possible for Hopper to grow between\nQ3 and Q4, it's possible, but we'll just have to see.\nOperator\nYour next question comes from the line of Joseph Moore of Morgan Stanley. Your line is open.", "char_count": 1428, "word_count": 269, "token_count": 343, "exchange_id": "ex_004", "exchange_role": "answer"}
{"doc_id": "NVDA_2025_Q3", "ticker": "NVDA", "year": 2025, "quarter": 3, "filing_date": "2025-09-01", "phase": "qa", "speaker_name": "Joseph Moore", "speaker_role": "Analyst", "speaker_firm": "Morgan Stanley", "utterance_id": "u_0014", "utterance_type": "question", "text": "Great. Thank you. I wonder if you could talk a little bit about what you're seeing in the inference market. You've talked about Strawberry and some of the\nramifications of longer scaling inference projects. But you've also talked about the possibility that as some of these Hopper clusters age that you could use some\nof the Hopper latent chips for inference. So I guess, do you expect inference to outgrow training in the next kind of 12-month time frame, and just generally your\nthoughts there?", "char_count": 496, "word_count": 85, "token_count": 106, "exchange_id": "ex_005", "exchange_role": "question"}
{"doc_id": "NVDA_2025_Q3", "ticker": "NVDA", "year": 2025, "quarter": 3, "filing_date": "2025-09-01", "phase": "qa", "speaker_name": "Jensen Huang", "speaker_role": "Unknown", "speaker_firm": null, "utterance_id": "u_0015", "utterance_type": "statement", "text": "Our hopes and dreams is that someday, the world does a ton of inference. And that's when AI has really succeeded, right. It's when every single company is\ndoing inference inside their companies for the marketing department and forecasting department and supply chain group and their legal department and\nengineering, of course and coding, of course. And so we hope that every company is doing inference 24/7. And that there will be a whole bunch of AI native\nstartups, thousands of AI native startups that are generating tokens and generating AI and every aspect of your computer experience from using Outlook to\nPowerPointing or when you're sitting there with Excel, you're constantly generating tokens. And every time you read a PDF, open a PDF, it generated a whole\nbunch of tokens. One of my favorite applications is NotebookLM, this Google application that came out. I use the living daylights out of it just because it's fun. And\nI put every PDF, every archive paper into it just to listen to it as well as scanning through it. And so I think -- that's the goal is to train these models so that people\nuse it. And there's now a whole new era of AI if you will, a whole new genre of AI called physical AI, just those large language models understand the human\nlanguage and how we the thinking process, if you will. Physical AI understands the physical world and it understands the meaning of the structure and\nunderstands what's sensible and what's not and what could happen and what won't and not only does it understand but it can predict and roll out a short future.\nThat capability is incredibly valuable for industrial AI and robotics. And so that's fired up so many AI-native companies and robotics companies and physical AI\ncompanies that you're probably hearing about. And it's really the reason why we built Omniverse. Omniverse is so that we can enable these AIs to be created and\nlearn in Omniverse and learn from synthetic data generation and reinforcement learning physics feedback instead of human feedback is now physics feedback.\nTo have these capabilities, Omniverse was created so that we can enable physical AI. And so that the goal is to generate tokens. The goal is to inference and\nwe're starting to see that growth happening. So I'm super excited about that. Now let me just say one more thing. Inference is super hard. And the reason why\ninference is super hard is because you need the accuracy to be high on the one hand. You need the throughput to be high so that the cost could be as low as\npossible, but you also need the latency to be low. And computers that are high throughput as well as low latency is incredibly hard to build. And these applications\nhave long context lengths because they want to understand, they want to be able to inference within understanding the context of what's -- what they're being\nasked to do. And so the context length is growing larger and larger. On the other hand, the models are getting larger, they're multimodality. Just the number of\ndimensions that inference is innovating is incredible. And this innovation rate is what makes NVIDIA's architecture so great because our ecosystem is fantastic.\nEverybody knows that if they innovate on top of CUDA on top of NVIDIA's architecture, they can innovate more quickly and they know that everything should\nwork. And if something were to happen, it's probably likely their code and not ours. And so that ability to innovate in every single direction at the same time,\nhaving a large installed base so that whatever you create could land on a NVIDIA computer and be deployed broadly all around the world in every single data\ncenter all the way out to the edge into robotic systems, that capability is really quite phenomenal.\nOperator\nYour next question comes from the line of Aaron Rakers of Wells Fargo. Your line is open.", "char_count": 3838, "word_count": 668, "token_count": 789, "exchange_id": null, "exchange_role": null}
{"doc_id": "NVDA_2025_Q3", "ticker": "NVDA", "year": 2025, "quarter": 3, "filing_date": "2025-09-01", "phase": "qa", "speaker_name": "Aaron Rakers", "speaker_role": "Analyst", "speaker_firm": "Wells Fargo", "utterance_id": "u_0016", "utterance_type": "question", "text": "Yes, thanks for taking the question. I wanted to ask you as we kind of focus on the Blackwell cycle and think about the data center business. When I look at the\nresults this last quarter, Colette, you mentioned that obviously, the networking business was down about 15% sequentially, but then your comments were that you\nwere seeing very strong demand. You mentioned also that you had multiple cloud CSP design wins for these large-scale clusters. So I'm curious if you could\nunpack what's going on in the networking business and where maybe you've seen some constraints and just your confidence in the pace of Spectrum-X\nprogressing to that multiple billions of dollars that you previously had talked about. Thank you.", "char_count": 719, "word_count": 121, "token_count": 146, "exchange_id": "ex_006", "exchange_role": "question"}
{"doc_id": "NVDA_2025_Q3", "ticker": "NVDA", "year": 2025, "quarter": 3, "filing_date": "2025-09-01", "phase": "qa", "speaker_name": "Colette Kress", "speaker_role": "CFO", "speaker_firm": null, "utterance_id": "u_0017", "utterance_type": "answer", "text": "Let's first start with the networking. The growth year-over-year is tremendous and our focus since the beginning of our acquisition of Mellanox has really been\nabout building together the work that we do in terms of -- in the Data Center. The networking is such a critical part of that. Our ability to sell our networking with\nmany of our systems that we are doing in data center is continuing to grow and do quite well. So this quarter is just a slight dip down and we're going to be right\nback up in terms of growing. They're getting ready for Blackwell and more and more systems that will be using not only our existing networking but also the\nnetworking that is going to be incorporated in a lot of these large systems that we are providing them to.\nOperator\n\nYour next question comes from the line of Atif Malik of Citi. Your line is open.", "char_count": 844, "word_count": 156, "token_count": 180, "exchange_id": "ex_006", "exchange_role": "answer"}
{"doc_id": "NVDA_2025_Q3", "ticker": "NVDA", "year": 2025, "quarter": 3, "filing_date": "2025-09-01", "phase": "qa", "speaker_name": "Atif Malik", "speaker_role": "Unknown", "speaker_firm": null, "utterance_id": "u_0018", "utterance_type": "statement", "text": "Thank you for taking my question. I have two quick ones for Collette. Colette, on the last earnings call, you mentioned that sovereign demand is in low\ndouble-digit billions. Can you provide an update on that? And then can you explain the supply-constrained situation in gaming? Is that because you're shifting\nyour supply towards data center?", "char_count": 343, "word_count": 56, "token_count": 72, "exchange_id": null, "exchange_role": null}
{"doc_id": "NVDA_2025_Q3", "ticker": "NVDA", "year": 2025, "quarter": 3, "filing_date": "2025-09-01", "phase": "qa", "speaker_name": "Colette Kress", "speaker_role": "CFO", "speaker_firm": null, "utterance_id": "u_0019", "utterance_type": "answer", "text": "So first starting in terms of sovereign AI, such an important part of growth, something that is really surfaced with the onset of generative AI and building models in\nthe individual countries around the world. And we see a lot of them and we talked about a lot of them in the call today and the work that they are doing. So our\nsovereign AI and our pipeline going forward is still absolutely intact as those are working to build these foundational models in their own language, in their own\nculture, and working in terms of the enterprises within those countries. And I think you'll continue to see this be growth opportunities that you may see with our\nregional clouds that are being stored up and/or those that are focusing in terms of AI factories for many parts of the sovereign AI. This is areas where this is\ngrowing not only in terms of in Europe, but you're also seeing this in terms of growth in terms of -- in the Asia-Pac as well. Let me flip to your second question that\nyou asked regarding gaming. So our gaming right now from a supply, we're busy trying to make sure that we can ramp all of our different products. And in this\ncase, our gaming supply, given what we saw selling through was moving quite fast. Now the challenge that we have is how fast could we get that supply getting\nready into the market for this quarter. Not to worry, I think we'll be back on track with more suppliers we turn the corner into the new calendar year. We're just\ngoing to be tight for this quarter.\nOperator\nYour next question comes from the line of Ben Reitzes of Melius Research. Your line is open.", "char_count": 1599, "word_count": 297, "token_count": 345, "exchange_id": null, "exchange_role": null}
{"doc_id": "NVDA_2025_Q3", "ticker": "NVDA", "year": 2025, "quarter": 3, "filing_date": "2025-09-01", "phase": "qa", "speaker_name": "Ben Reitzes", "speaker_role": "Unknown", "speaker_firm": null, "utterance_id": "u_0020", "utterance_type": "statement", "text": "Yes. Hi. Thanks a lot for the question. I wanted to ask Colette and Jensen with regard to sequential growth. So very strong sequential growth this quarter and\nyou're guiding to about 7%. Do your comments on Blackwell imply that we reaccelerate from there as you get more supply? Just in the first half, it would seem\nthat there would be some catch-ups. So I was wondering how prescriptive you could be there. And then, Jensen, just overall, with the change in administration\nthat's going to take place here in the US and the China situation, have you gotten any sense, or any conversations about tariffs, or anything with regard to your\nChina business? Any sense of what may or may not go on? It's probably too early, but wondering if you had any thoughts there. Thanks so much.", "char_count": 778, "word_count": 139, "token_count": 173, "exchange_id": null, "exchange_role": null}
{"doc_id": "NVDA_2025_Q3", "ticker": "NVDA", "year": 2025, "quarter": 3, "filing_date": "2025-09-01", "phase": "qa", "speaker_name": "Jensen Huang", "speaker_role": "Unknown", "speaker_firm": null, "utterance_id": "u_0021", "utterance_type": "statement", "text": "We guide one quarter at a time.", "char_count": 31, "word_count": 7, "token_count": 8, "exchange_id": null, "exchange_role": null}
{"doc_id": "NVDA_2025_Q3", "ticker": "NVDA", "year": 2025, "quarter": 3, "filing_date": "2025-09-01", "phase": "qa", "speaker_name": "Colette Kress", "speaker_role": "CFO", "speaker_firm": null, "utterance_id": "u_0022", "utterance_type": "answer", "text": "We are working right now on the quarter that we're in and building what we need to ship in terms of Blackwell. We have every supplier on the planet working\nseamlessly with us to do that. And once we get to next quarter, we'll help you understand in terms of that ramp that we'll see to the next quarter and after that.", "char_count": 318, "word_count": 62, "token_count": 73, "exchange_id": null, "exchange_role": null}
{"doc_id": "NVDA_2025_Q3", "ticker": "NVDA", "year": 2025, "quarter": 3, "filing_date": "2025-09-01", "phase": "qa", "speaker_name": "Jensen Huang", "speaker_role": "Unknown", "speaker_firm": null, "utterance_id": "u_0023", "utterance_type": "statement", "text": "Whatever the new administration decides, we will of course support the administration. And that's our -- the highest mandate. And then after that, do the best we\ncan. And just as we always do. And so we have to simultaneously and we will comply with any regulation that comes along fully and support our customers to the\nbest of our abilities and compete in the marketplace. We'll do all of these three things simultaneously.\nOperator\nYour final question comes from the line of Pierre Ferragu of New Street Research. Your line is open.", "char_count": 535, "word_count": 93, "token_count": 109, "exchange_id": null, "exchange_role": null}
{"doc_id": "NVDA_2025_Q3", "ticker": "NVDA", "year": 2025, "quarter": 3, "filing_date": "2025-09-01", "phase": "qa", "speaker_name": "Pierre Ferragu", "speaker_role": "Unknown", "speaker_firm": null, "utterance_id": "u_0024", "utterance_type": "statement", "text": "Hey, thanks for taking my question. Jensen, you mentioned in your comments you have the pre-trainings, the actual language models and you have\nreinforcement learning that becomes more and more important in training and in inference as well. And then you have inference itself. And I was wondering if you\nhave a sense like a high-level typical sense of out of an overall AI ecosystem like maybe one of your clients or one of the large models that are out there. Today,\nhow much of the compute goes into each of these buckets? How much for the pre-training, how much for the reinforcement, and how much into inference today?\nDo you have any sense for how it's splitting and where the growth is the most important as well?", "char_count": 719, "word_count": 128, "token_count": 151, "exchange_id": null, "exchange_role": null}
{"doc_id": "NVDA_2025_Q3", "ticker": "NVDA", "year": 2025, "quarter": 3, "filing_date": "2025-09-01", "phase": "qa", "speaker_name": "Jensen Huang", "speaker_role": "Unknown", "speaker_firm": null, "utterance_id": "u_0025", "utterance_type": "statement", "text": "Well, today it's vastly in pre-training a foundation model because as you know, post-training, the new technologies are just coming online and whatever you could\ndo in pre-training and post-training, you would try to do so that the inference cost could be as low as possible for everyone. However, there are only so many\nthings that you could do priority. And so you'll always have to do on-the-spot thinking and in-context thinking and a reflection. And so I think that the fact that all\nthree are scaling is actually very sensible based on what we are. And in the area of foundation model, now we have multimodality foundation models and the\namount of petabytes of video that these foundation models are going to be trained on is incredible. And so my expectation is that for the foreseeable future, we're\ngoing to be scaling pre-training, post-training as well as inference time scaling and which is the reason why I think we're going to need more and more compute\nand we're going to have to drive as hard as we can to keep increasing the performance by X factors at a time so that we can continue to drive down the cost and\ncontinue to increase their revenues and get the AI revolution going. Thank you.\nOperator\n\nThank you. I'll now turn the call back over to Jensen Huang for closing remarks.", "char_count": 1298, "word_count": 230, "token_count": 273, "exchange_id": null, "exchange_role": null}
{"doc_id": "NVDA_2025_Q3", "ticker": "NVDA", "year": 2025, "quarter": 3, "filing_date": "2025-09-01", "phase": "qa", "speaker_name": "Jensen Huang", "speaker_role": "Unknown", "speaker_firm": null, "utterance_id": "u_0026", "utterance_type": "statement", "text": "Thank you. The tremendous growth in our business is being fueled by two fundamental trends that are driving global adoption of NVIDIA computing. First, the\ncomputing stack is undergoing a reinvention, a platform shift from coding to machine learning. From executing code on CPUs to processing neural networks on\nGPUs. The trillion-dollar installed base of traditional Data center infrastructure is being rebuilt for Software 2.0, which applies machine learning to produce AI.\nSecond, the age of AI is in full steam. Generative AI is not just a new software capability, but a new industry with AI factories manufacturing digital intelligence, a\nnew industrial revolution that can create a multi-trillion dollar AI industry. Demand for Hopper and anticipation for Blackwell, which is now in full production are\nincredible for several reasons. There are more foundation model makers now than there were a year ago. The computing scale of pre-training and post-training\ncontinues to grow exponentially. There are more AI-native start-ups than ever and the number of successful inference services is rising. And with the introduction\nof ChatGPT o1, OpenAI o1, a new scaling law called test time scaling has emerged. All of these consume a great deal of computing. AI is transforming every\nindustry, company, and country. Enterprises are adopting agentic AI to revolutionize workflows. Over time, AI coworkers will assist employees in performing their\njobs faster and better. Investments in industrial robotics are surging due to breakthroughs in physical AI. Driving new training infrastructure demand as\nresearchers train world foundation models on petabytes of video and Omniverse synthetically generated data. The age of robotics is coming. Countries across the\nworld recognize the fundamental AI trends we are seeing and have awakened to the importance of developing their national AI infrastructure. The age of AI is\nupon us and it's large and diverse. NVIDIA's expertise, scale, and ability to deliver full stack and full infrastructure let us serve the entire multi-trillion dollar AI and\nrobotics opportunities ahead. From every hyperscale cloud, enterprise private cloud to sovereign regional AI clouds, on-prem to industrial edge and robotics.\nThanks for joining us today and catch up next time.\nOperator\nThis concludes today's conference call. You may now disconnect.", "char_count": 2373, "word_count": 364, "token_count": 459, "exchange_id": null, "exchange_role": null}

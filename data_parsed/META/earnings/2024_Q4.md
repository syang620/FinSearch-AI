# META Q4 2024 Earnings Call

**Document ID**: META_2024_Q4
**Ticker**: META
**Year**: 2024
**Quarter**: Q4
**Utterances**: 26

---

---

## Q&A Session

### Exchange 001

**Eric Sheridan (Goldman Sachs)**: Thank you so much for taking the question. Maybe I can go back to your comments on open source. Can you help us understand how your views continue to
evolve with respect to the competitive dynamic around your approach with open source versus others in the industry? And how your approach to open source
could possibly bend the cost curve and improve return on capital for AI over the medium to long-term? Thanks so much.

**Mark Zuckerberg (CEO)**: Yes. I mean on open source, I think the best analogy for us is what we did with open compute, where we weren't first to building the system. So then by the time
that we got around to building it, it wasn't really a big advantage to have it be proprietary. So we shared it. And then a lot of the industry adopted what we were
doing, contributed innovations back to it. By standardizing it on it, that meant that a bunch of supply chain standardized on building it, which made prices more
efficient for everyone. I think what we see here is as Llama becomes more used, it's more likely, for example, that silicon providers and others -- other APIs and
developer platforms will optimize their work more for that and basically drive down the costs of using it and drive improvements that we can, in some cases, use
too. So I think that the strategy will continue to be effective, and yes, I mean, I continue to be optimistic about this. I think it's kind of -- I think it's working. I also
just think in light of some of the recent news, the new competitor DeepSeek from China, I think it also just puts -- it's one of the things that we're talking about is
there's going to be an open source standard globally. And I think for our kind of national advantage, it's important that it's an American standard. So we take that
seriously, and we want to build the AI system that people around the world are using and I think that if anything, some of the recent news has only strengthened
our conviction that this is the right thing for us to be focused on.
Operator
Your next question comes from the line of Mark Shmulik with Bernstein. Please go ahead.

### Exchange 002

**Mark Shmulik (Bernstein)**: Yes, thank you for taking my questions. Mark, I appreciate we may get an answer this year. But looking out, as you kind of track the progress of smart glasses,
Orion and so forth, do you view that as a better form factor to get the most out of the Meta AI assistance you highlighted in your opening remarks? Or is it more
complementary to kind of the in-app experience in the way you've seen people use it today? And then, Susan, the last few quarters, we've kind of seen pricing
growth is the dominant driver of ad revenue growth. Given the efforts you've highlighted around driving deeper, more commercial engagement and better
advertiser ROI, how do we just think about the contribution of the formula for ad revenue growth going forward? Thank you.

**Mark Zuckerberg (CEO)**: Yes. I mean, I can talk about glasses. I mean it's -- yes, I mean, I've said for a while that I think that glasses are the ideal form factor for an AI device, because
you can let an AI assistant on your glasses see what you see and hear what you hear, which gives it the context to be able to understand everything that's going
on in your life that you would want to talk to it about and get context on. So -- but look, I mean, I think the glasses are going to be a very important computing
platform in the future. When phones became the primary computing platform, it's not like computers went away. I think we'll have phones for some time. But there
are a lot of people in the world who have glasses. It's kind of hard for me to imagine that a decade or more from now, all the glasses aren't going to basically be
AI glasses, as well as a lot of people who don't wear glasses today, finding that to be a useful thing. So I'm incredibly optimistic about this. And like I shared last
year, I think one of the big surprises last year was I previously thought that glasses weren't going to become a major form factor until we got these -- the full kind
of holographic displays that we started showing in the prototype for Orion. But now I think it's pretty clear that AI is actually going to drive at least as much of the
value as the holographic AR is. So that's a cause to be excited. But look, the Ray-Ban Metas were hit. We still don't know what the long-term trajectory for this is
going to be. And I think we're going to learn a lot this year. So I think that this is a really important year for that.

### Exchange 003

**Justin Post (Bank of America)**: Great, thanks. Maybe one for Mark and one for Susan. Mark, you mentioned political changes in the U.S. and better positioning maybe for U.S. companies
abroad. But how do you think about it in the U.S. as far as usage and advertiser adoption, you got rid of fact checking. So do you think the content could change?
Could it appeal to more users? Will that impact advertising at all? And then Susan, on Meta AI, I know people are pretty excited about the use case, but also
thinking about the revenue case. How do you think about monetizing that? Could it be CPC ads? Or how are you thinking about that? Thank you.

**Mark Zuckerberg (CEO)**: The question was about fact checking and our content policies. I mean, look, I think we're trying to build the service that we think is the best for people. I believe
in free expression for quite a while. People don't want to see misinformation, but you need to build an effective system that gives people more context. And I think
what we found over time is that the community note system, I think, is just going to be more effective than the system that we had before. And I'm not afraid to
admit when someone does something that's better than us. I think it's sort of our job to go and just do best work and implement the best systems. So I think that
there's been a lot of people who have read this announcement is if we somehow don't care about adding context to things that are on our platform that are
misinformation, that's not right. I actually think that the community note system, like what X has had for a while is actually just more effective than what we were
doing before. And I think our product is going to get better because of it.

### Exchange 004

**Douglas Anmuth (JPMorgan)**: Thanks for taking the questions. One for Mark, one for Susan. Mark, just following up on open source as DeepSeek and other models potentially leverage Llama
or others to train faster and cheaper. How does this impact in your view? And what could have been for the trajectory of investment required over a multiyear
period? And then, Susan, just as we think about the $60 billion to $65 billion CapEx this year, does the composition change much from last year when you talked
about servers as the largest part followed by data centers and networking equipment. And how should we think about that mix between like training and inference
just following up on Jan's post this week? Thanks.

**Mark Zuckerberg (CEO)**: I can start on the DeepSeek question. I think there's a number of novel things that they did that I think we're still digesting. And there are a number of things that
they have advances that we will hope to implement in our systems. And that's part of the nature of how this works, whether it's a Chinese competitor or not. I kind
of expect that every new company that has an advance -- that has a launch is going to have some new advances that the rest of the field learns from. And that's
sort of how the technology industry goes. I don't know -- it's probably too early to really have a strong opinion on what this means for the trajectory around
infrastructure and CapEx and things like that. There are a bunch of trends that are happening here all at once. There's already sort of a debate around how much
of the compute infrastructure that we're using is going to go towards pretraining versus as you get more of these reasoning time models or reasoning models
where you get more of the intelligence by putting more of the compute into inference, whether just will mix shift how we use our compute infrastructure towards
that. That was already something that I think a lot of the other labs and ourselves were starting to think more about and already seemed pretty likely even before
this, that -- like of all the compute that we're using, that the largest pieces aren't necessarily going to go towards pre-training. But that doesn't mean that you need
less compute, because one of the new properties that's emerged is the ability to apply more compute at inference time in order to generate a higher level of
intelligence and a higher quality of service, which means that as a company that has a strong business model to support this, I think that's generally an advantage
that we're now going to be able to provide a higher quality of service than others, who don't necessarily have the business model to support it on a sustainable
basis. The other thing is just that when we're building things like Meta AI, but also how we're implementing AI into all the feeds and ad products and things like
that, we're just serving billions of people, which is different from, okay, you start to pretrain a model, and that model is sort of agnostic to how many people are
using it, like at some level, it's going to be expensive for us to serve all of these people, because we are serving a lot of people. And so I'm not sure what the kind
of net effect of all of this is. The field continues to move quickly. There's a lot to learn from releases from basically everyone who does something interesting, not
just the ones over the last month. We'll continue to kind of incorporate that into what we do as well as making novel contributions to the field ourselves And I
continue to think that investing very heavily in CapEx and infra is going to be a strategic advantage over time. It's possible that we'll learn otherwise at some point,
but I just think it's way too early to call that. And at this point, I would bet that the ability to build out that kind of infrastructure is going to be a major advantage for
both the quality of the service and being able to serve the scale that we want to.

### Exchange 005

**Ron Josey (Citigroup)**: Hey, thanks for taking the question. Mark, I want to get back to your comment on getting back to the OG Facebook, and I want to understand a little bit more on
the use cases and how that could expand? Video is clearly a benefit. Local marketplace groups have all been positive. So any insights on the OG Facebook? And
then back to Meta AI, given the adoption we're seeing on the 600-plus MAUs, just how does the user experience evolved to? What are people doing with Meta
AI? Thank you.

**Mark Zuckerberg (CEO)**: Okay. So for Facebook, a lot of people use Facebook every day, and it's an important part of their lives. And I think that there are a lot of opportunities to make it
way more culturally influential than it is today. And I think that, that's sort of a fun and interesting goal that will take our product development in some interesting
directions that we maybe have a focus on it as much over the last several years. So I don't know that I have anything much more specific on this other than that
this is going to be one of my focus areas for this year. I mean, I think it's an investment area and something I'm going to spend some time on it. It might mean that
in the near-term, we make some trade-offs to kind of focus on some product areas of what we're doing ahead of just kind of maximizing business results in the
near term on it. But overall, I'm really excited about doing some exciting stuff here. And I'm not going to get into many specifics now, but we'll get -- we'll follow up
on this over the next, I don't know, call it, a year as we start rolling it out and I think some of this will kind of get back to how Facebook was originally used back in
the day. So I think it will be fun.

### Exchange 006

**Ken Gawrelski (Wells Fargo)**: Thank you very much. Two for me, please. First, could you talk a little bit -- I know you talked a little bit on the capital intensity side and the recent developments,
and it's hard to see it's hard to tell yet where things are going? But maybe you could just talk a little bit more near term, '25, the CapEx budget you laid out or the
CapEx forecast. Could you talk a little bit about the constraints you're seeing or where you're seeing constraints, either internally resources planning or externally
and any one -- any parts of the ecosystem? And then on the second one, I'm curious, as you think about your needs for hiring and we just think about -- we know
you gave the OpEx guide for this year. But as we think about future needs for hiring, could you just give us a sense of how we should think about that? You
announced the performance-related reductions earlier this -- for early this year. Could you just talk about how we should be thinking about that '26, '27 and
beyond? Thank you.

**Susan Li (CFO)**: Sure. I'm happy to take both of those. So on your first question on just where do we see constraints in our ability to execute against our CapEx plans. Obviously,
we are staying on top of supply availability. That is certainly one of the factors that will influence our CapEx spend in 2025, but we don't really have any updates
to share on supply availability right now. We are planning to significantly ramp up deployment of GPUs in 2025, and we'll continue to engage with our vendors
and invest in our own silicon to meet those needs. When you asked how to think about capital intensity, we're not really -- as both Mark and I alluded to in our
prior comments, I think it is really too early to determine what long-run capital intensity is going to look like. There are so many different factors. The pace of
advancement in underlying models, how efficient can they be? What is the adoption and use case of our Gen AI products, what performance gains come from
next-generation hardware innovations, both our own and third-party and then ultimately, what monetization or other efficiency gains our AI investments unlock. So
again, I think we are sort of early in the journey here, and we don't have -- I would say we don't have kind of anything to share about long-run capital intensity yet.
Your second question was about thinking about hiring needs. So it's a good segue after infrastructure, employee compensation is the next largest driver of
expense growth in 2025. And here, growth in employee comp and headcount more broadly is primarily driven by those areas that I mentioned, infrastructure
monetization, generative AI, Reality Labs and regulation and compliance. And those generally are more technical organizations. That means that it is a higher
cost base relative to business functions where we are also expecting to keep headcount growth constrained. And I would say we are -- we're focused on running
the company efficiently. But at the same time, it is -- we feel like we're in a critical period in terms of making sure that we are investing to win, and we want to
make sure that we staff those priority areas in a way that really positions us to best do that.

### Exchange 007

**Ross Sandler (Barclays)**: Yes. One for Mark, on agents. So we all saw OpenAI's operator demo last week. So Mark, as the industry moves from chat to agentic behavior and more
commercial intent moves into these AI products? I guess how are you thinking about monetization potential for Meta AI? And then how might Llama 4 reasoning
help drive some of these new agentic experiences for Meta AI? Thank you.

**Mark Zuckerberg (CEO)**: Yes. So I guess a couple of things that I'd say on this. One is when you're thinking about agents and reasoning, a lot of this is about being able to perform
multistep tasks. So right now, the way that a lot of these systems work as you kind of say something and then it responds and it's almost chat like. But I think that
the direction that it's going is you're going to be able to give it an intent or a task and it's going to be able to go off and use sort of an arbitrary amount of compute
as much as you want to use on it to be able to do a task. Some of the tasks might be pretty simple for people go buy a specific thing. Some of them might be
really hard, like go write an app or optimize this code and like really make it as good as possible. And that type of thing, I think, is just going to start becoming
more and more prevalent over the next a year or two. So I think it's very exciting. It's sort of we'll feel in some ways like the current products are just getting
smarter and others, it will feel like sort of a new form factor, because it won't be as much like chat. But it's sort of another generation of these products. So I think
it's just in general, there's a lot to build and be excited about. I guess my note of caution or just my kind of periodic reminder on our product development process,
if you will, is we build these product. We try to scale them to reach usually 1 billion people or more. And it's at that point once they're at scale that we really start
focusing on monetization. So sometimes we'll experiment with monetization before, we're running some experiments with Threads now, for example. But we
typically don't really ramp these things up or see them as meaningfully contributing to the business until we reach quite a big scale. So the thing that I think is
going to be meaningful this year is the kind of getting of the AI product to scale. Last year was sort of the introduction and starting to get to be used. This year my
kind of expectation and hope is that we will be at a sufficient scale and have sufficient kind of flywheel of people using it and improvement from that, that this will
have a durable advantage. But that doesn't mean that it's going to be a major contributor to the business. This year the improvements of the business are going to
be taking the AI methods and applying them to advertising and recommendations and feeds and things like that. So the actual business opportunity for Meta AI
and AI studio and business agents and people interacting with these AIs remains outside of '25 for the most part. And I think that's an important thing for us to
communicate and for people to internalize as you're thinking about our prospects here. But nonetheless, we've run a process like this many times. We built a
product. We make it good. We scale it to be large. We build out the business around it. That's what we do. I'm very optimistic, but it's going to take some time.

# Local LLM Hosting Plan for FinSearch AI

## Executive Summary
Plan to upgrade FinSearch AI from the current Flan-T5-base model (250M params) to a more powerful locally-hosted LLM optimized for financial Q&A, while maintaining reasonable memory usage on Mac hardware.

## Current State Analysis

### Existing Setup
- **Model**: google/flan-t5-base (250M parameters)
- **Framework**: HuggingFace Transformers + PyTorch
- **Device**: CPU (configurable for CUDA)
- **Memory Usage**: ~1GB
- **Max Context**: 512 tokens
- **Performance**: Basic Q&A capabilities, limited financial reasoning

### Limitations
- Limited context window (512 tokens)
- Basic reasoning capabilities
- No financial domain expertise
- Struggles with complex analytical questions
- Cannot handle full financial documents

## Recommended Solution: Ollama + Qwen2.5-7B-Instruct

### Why This Combination?
- **Ollama**: Simple deployment, automatic quantization, excellent Mac support
- **Qwen2.5-7B**: Best-in-class for structured data, 128K context window, strong financial reasoning
- **Quantization**: Q4 format reduces memory from 14GB to 4-5GB with minimal quality loss

## Implementation Plan

### Phase 1: Setup Ollama (30 minutes)

#### Steps:
1. **Install Ollama**
   ```bash
   brew install ollama
   ```

2. **Start Ollama Service**
   ```bash
   ollama serve
   ```

3. **Download Models**
   ```bash
   # Primary model
   ollama pull qwen2.5:7b

   # Backup lightweight model
   ollama pull phi3.5
   ```

4. **Test Performance**
   ```bash
   ollama run qwen2.5:7b "Explain key metrics in a 10-K filing"
   ```

### Phase 2: Backend Integration (2-3 hours)

#### 1. Add Dependencies
```python
# requirements.txt
ollama-python==0.1.7
```

#### 2. Create Ollama Service
Create `backend/app/services/llm/ollama_service.py`:
```python
import ollama
from typing import Optional, Dict, Any
import logging

logger = logging.getLogger(__name__)

class OllamaLLMService:
    """Service for interacting with Ollama-hosted models"""

    def __init__(self, model_name: str = "qwen2.5:7b"):
        self.model_name = model_name
        self.client = ollama.Client()

    def generate(
        self,
        prompt: str,
        max_tokens: int = 512,
        temperature: float = 0.7,
        context: Optional[str] = None
    ) -> str:
        """Generate text using Ollama model"""

        # Build full prompt with context if provided
        full_prompt = prompt
        if context:
            full_prompt = f"Context: {context}\n\nQuestion: {prompt}\n\nAnswer:"

        response = self.client.generate(
            model=self.model_name,
            prompt=full_prompt,
            options={
                'temperature': temperature,
                'max_tokens': max_tokens
            }
        )

        return response['response']
```

#### 3. Update Configuration
Add to `backend/app/core/config.py`:
```python
# LLM Backend Selection
LLM_BACKEND: str = "ollama"  # "transformers" or "ollama"
OLLAMA_MODEL: str = "qwen2.5:7b"
OLLAMA_HOST: str = "http://localhost:11434"
```

#### 4. Modify Chat Route
Update `backend/app/api/routes/chat.py` to support backend selection:
```python
if settings.LLM_BACKEND == "ollama":
    from app.services.llm.ollama_service import OllamaLLMService
    llm_service = OllamaLLMService(settings.OLLAMA_MODEL)
else:
    from app.services.llm.llm_service import LLMService
    llm_service = LLMService()
```

### Phase 3: Testing & Optimization (1-2 hours)

#### Test Cases
1. **Basic Q&A**: Simple financial questions
2. **Document Analysis**: Full 10-K section analysis
3. **Multi-turn Conversation**: Context retention
4. **Numerical Reasoning**: Financial calculations
5. **Table Understanding**: Structured data interpretation

#### Performance Benchmarks
- Response time: < 2 seconds for typical queries
- Memory usage: 4-5GB steady state
- Token throughput: 20-30 tokens/second
- Context window: Test up to 8K tokens

#### Optimization Steps
1. **Prompt Engineering**
   - Create financial-specific prompt templates
   - Add role instructions ("You are a financial analyst...")
   - Include format specifications for structured output

2. **Context Management**
   - Implement sliding window for long documents
   - Prioritize recent context
   - Chunk documents intelligently

3. **Generation Parameters**
   ```python
   optimal_params = {
       'temperature': 0.3,  # Lower for factual accuracy
       'top_p': 0.9,
       'top_k': 40,
       'repeat_penalty': 1.1
   }
   ```

### Phase 4: Documentation (30 minutes)

#### Update Documentation
1. **README.md** - Add LLM section
2. **SETUP.md** - Installation instructions
3. **API.md** - Endpoint documentation
4. **.env.example** - New environment variables

## Alternative Approaches

### Option A: MLX Framework (Best Performance on Apple Silicon)

#### Pros:
- 30-50% faster on Mac
- Better memory efficiency
- Native Metal acceleration

#### Cons:
- Mac-only solution
- More complex setup
- Smaller model selection

#### Implementation:
```bash
pip install mlx-lm
```

```python
from mlx_lm import load, generate

model, tokenizer = load("mlx-community/Qwen2.5-7B-Instruct-4bit")
response = generate(model, tokenizer, prompt="Financial analysis:", max_tokens=512)
```

### Option B: Quantized Transformers (Minimal Changes)

#### Pros:
- Keep existing codebase
- Familiar HuggingFace ecosystem
- Cross-platform compatible

#### Cons:
- Slower than specialized solutions
- Higher memory usage
- Complex quantization setup

#### Implementation:
```python
from transformers import AutoModelForCausalLM, BitsAndBytesConfig

quantization_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.float16
)

model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen2.5-7B-Instruct",
    quantization_config=quantization_config
)
```

### Option C: Lightweight Models (Memory Constrained)

For systems with < 8GB RAM:

#### Phi-3.5 Mini (3.8B params)
- Memory: 2-3GB
- Performance: Good general reasoning
- Context: 4K tokens

#### Flan-T5-Large (780M params)
- Memory: 1.5GB
- Performance: Better than base
- Context: 512 tokens

## Model Comparison Matrix

| Model | Parameters | Memory (Q4) | Context | Financial Expertise | Speed | Quality |
|-------|------------|-------------|---------|-------------------|--------|----------|
| Flan-T5-Base (current) | 250M | 1GB | 512 | Low | Fast | Basic |
| Phi-3.5 Mini | 3.8B | 2-3GB | 4K | Medium | Fast | Good |
| Qwen2.5-7B | 7.3B | 4-5GB | 128K | High | Medium | Excellent |
| Mistral-7B | 7.3B | 4-5GB | 8K | Medium | Medium | Very Good |
| FinGPT-v3.2 | 7B | 4-5GB | 4K | Very High | Medium | Good |

## Hardware Requirements

### Minimum (8GB RAM)
- Model: Phi-3.5 Mini (Q4)
- Performance: Acceptable
- Limitations: Short context, basic reasoning

### Recommended (16GB RAM)
- Model: Qwen2.5-7B (Q4)
- Performance: Excellent
- Capabilities: Full financial analysis

### Optimal (32GB+ RAM)
- Model: Qwen2.5-7B (Q8) or multiple models
- Performance: Near-perfect quality
- Capabilities: Multi-model ensemble

## Expected Improvements

### Quality Metrics
- **Context Understanding**: +300% (512 â†’ 128K tokens)
- **Financial Reasoning**: +500% improvement
- **Mathematical Accuracy**: +400% improvement
- **Response Coherence**: +200% improvement

### Performance Metrics
- **Inference Speed**: 20-30 tokens/sec (Mac M1/M2)
- **Memory Usage**: 4-5GB steady state
- **Startup Time**: 5-10 seconds
- **Response Latency**: < 2 seconds

## Risk Mitigation

### Potential Issues & Solutions

1. **High Memory Usage**
   - Solution: Implement model unloading after idle time
   - Fallback: Switch to smaller model dynamically

2. **Slow Inference**
   - Solution: Implement response streaming
   - Cache common queries

3. **Model Availability**
   - Keep Flan-T5 as fallback
   - Implement graceful degradation

4. **Integration Complexity**
   - Start with Ollama (simplest)
   - Phase implementation gradually

## Success Criteria

### Functional Requirements
- [ ] Successfully integrate Ollama backend
- [ ] Maintain backward compatibility
- [ ] Support model switching
- [ ] Handle 8K+ token contexts

### Performance Requirements
- [ ] Response time < 3 seconds
- [ ] Memory usage < 6GB
- [ ] 95% uptime reliability
- [ ] Support 10 concurrent requests

### Quality Requirements
- [ ] Pass financial Q&A test suite
- [ ] Improve user satisfaction score
- [ ] Reduce hallucination rate
- [ ] Increase answer accuracy

## Timeline

### Week 1
- Day 1-2: Setup and testing Ollama
- Day 3-4: Backend integration
- Day 5: Initial testing

### Week 2
- Day 1-2: Optimization and tuning
- Day 3-4: Documentation
- Day 5: Deployment preparation

## Rollback Plan

If issues arise:
1. Keep existing LLMService intact
2. Use feature flag for backend selection
3. Monitor performance metrics
4. Quick switch via environment variable
5. No database migrations required

## Next Steps

1. **Immediate Action**: Install Ollama and test models locally
2. **Short-term**: Implement Phase 1 & 2
3. **Medium-term**: Optimize and tune for production
4. **Long-term**: Consider fine-tuning for financial domain

## Additional Resources

### Documentation
- [Ollama Documentation](https://ollama.ai/docs)
- [Qwen2.5 Model Card](https://huggingface.co/Qwen/Qwen2.5-7B-Instruct)
- [MLX Framework](https://github.com/ml-explore/mlx)
- [Quantization Guide](https://huggingface.co/docs/transformers/quantization)

### Model Resources
- [Ollama Model Library](https://ollama.ai/library)
- [HuggingFace Models](https://huggingface.co/models)
- [FinGPT Project](https://github.com/AI4Finance-Foundation/FinGPT)

### Community
- [LocalLLaMA Reddit](https://reddit.com/r/LocalLLaMA)
- [Ollama Discord](https://discord.gg/ollama)
- [HuggingFace Forums](https://discuss.huggingface.co)

---

## Summary

This plan provides a clear path to upgrade FinSearch AI with a powerful local LLM that:
- **Runs entirely offline** - No cloud dependencies or API costs
- **Handles financial analysis** - Specialized for structured data and numerical reasoning
- **Scales with hardware** - From 8GB to 32GB+ RAM configurations
- **Maintains compatibility** - No breaking changes to existing system
- **Improves dramatically** - 5-10x better reasoning and 250x larger context window

The recommended Ollama + Qwen2.5-7B solution offers the best balance of performance, ease of implementation, and financial domain capabilities for the FinSearch AI project.